[
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    org.apache.hadoop.hbase.master.HMaster master = this.cluster.getMaster();\n    org.apache.hadoop.hbase.HServerAddress address = master.getMasterAddress();\n    org.apache.hadoop.hbase.HTableDescriptor tableDesc = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.util.Bytes.toBytes(\"_MY_TABLE_\"));\n    org.apache.hadoop.hbase.HTableDescriptor metaTableDesc = meta.getTableDescriptor();\n    byte[] startKey0 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"f\");\n    byte[] endKey0 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"h\");\n    org.apache.hadoop.hbase.HRegionInfo regionInfo0 = new org.apache.hadoop.hbase.HRegionInfo(tableDesc, startKey0, endKey0);\n    org.apache.hadoop.hbase.HRegionInfo metaRegionInfo0 = new org.apache.hadoop.hbase.HRegionInfo(metaTableDesc, org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), regionInfo0.getRegionName());\n    org.apache.hadoop.hbase.master.MetaRegion meta0 = new org.apache.hadoop.hbase.master.MetaRegion(address, metaRegionInfo0);\n    byte[] startKey1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"j\");\n    byte[] endKey1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"m\");\n    org.apache.hadoop.hbase.HRegionInfo regionInfo1 = new org.apache.hadoop.hbase.HRegionInfo(tableDesc, startKey1, endKey1);\n    org.apache.hadoop.hbase.HRegionInfo metaRegionInfo1 = new org.apache.hadoop.hbase.HRegionInfo(metaTableDesc, regionInfo0.getRegionName(), regionInfo1.getRegionName());\n    org.apache.hadoop.hbase.master.MetaRegion meta1 = new org.apache.hadoop.hbase.master.MetaRegion(address, metaRegionInfo1);\n    org.apache.hadoop.hbase.HRegionInfo metaRegionInfo2 = new org.apache.hadoop.hbase.HRegionInfo(metaTableDesc, regionInfo1.getRegionName(), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"));\n    org.apache.hadoop.hbase.master.MetaRegion meta2 = new org.apache.hadoop.hbase.master.MetaRegion(address, metaRegionInfo2);\n    byte[] startKeyX = org.apache.hadoop.hbase.util.Bytes.toBytes(\"h\");\n    byte[] endKeyX = org.apache.hadoop.hbase.util.Bytes.toBytes(\"j\");\n    org.apache.hadoop.hbase.HRegionInfo regionInfoX = new org.apache.hadoop.hbase.HRegionInfo(tableDesc, startKeyX, endKeyX);\n    master.getRegionManager().offlineMetaRegionWithStartKey(startKey0);\n    master.getRegionManager().putMetaRegionOnline(meta0);\n    master.getRegionManager().putMetaRegionOnline(meta1);\n    master.getRegionManager().putMetaRegionOnline(meta2);\n    org.apache.hadoop.hbase.HBaseTestCase.assertEquals(metaRegionInfo1.getStartKey(), master.getRegionManager().getFirstMetaRegionForRegion(regionInfoX).getStartKey());\n    org.apache.hadoop.hbase.HBaseTestCase.assertEquals(metaRegionInfo1.getRegionName(), master.getRegionManager().getFirstMetaRegionForRegion(regionInfoX).getRegionName());\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    org.apache.hadoop.hbase.master.HMaster master = this.cluster.getMaster();\n    org.apache.hadoop.hbase.HServerAddress address = master.getMasterAddress();\n    org.apache.hadoop.hbase.HTableDescriptor tableDesc = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.util.Bytes.toBytes(\"_MY_TABLE_\"));\n    org.apache.hadoop.hbase.HTableDescriptor metaTableDesc = meta.getTableDescriptor();\n    byte[] startKey0 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"f\");\n    byte[] endKey0 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"h\");\n    org.apache.hadoop.hbase.HRegionInfo regionInfo0 = new org.apache.hadoop.hbase.HRegionInfo(tableDesc, startKey0, endKey0);\n    org.apache.hadoop.hbase.HRegionInfo metaRegionInfo0 = new org.apache.hadoop.hbase.HRegionInfo(metaTableDesc, org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), regionInfo0.getRegionName());\n    org.apache.hadoop.hbase.master.MetaRegion meta0 = new org.apache.hadoop.hbase.master.MetaRegion(address, metaRegionInfo0);\n    byte[] startKey1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"j\");\n    byte[] endKey1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"m\");\n    org.apache.hadoop.hbase.HRegionInfo regionInfo1 = new org.apache.hadoop.hbase.HRegionInfo(tableDesc, startKey1, endKey1);\n    org.apache.hadoop.hbase.HRegionInfo metaRegionInfo1 = new org.apache.hadoop.hbase.HRegionInfo(metaTableDesc, regionInfo0.getRegionName(), regionInfo1.getRegionName());\n    org.apache.hadoop.hbase.master.MetaRegion meta1 = new org.apache.hadoop.hbase.master.MetaRegion(address, metaRegionInfo1);\n    org.apache.hadoop.hbase.HRegionInfo metaRegionInfo2 = new org.apache.hadoop.hbase.HRegionInfo(metaTableDesc, regionInfo1.getRegionName(), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"));\n    org.apache.hadoop.hbase.master.MetaRegion meta2 = new org.apache.hadoop.hbase.master.MetaRegion(address, metaRegionInfo2);\n    byte[] startKeyX = org.apache.hadoop.hbase.util.Bytes.toBytes(\"h\");\n    byte[] endKeyX = org.apache.hadoop.hbase.util.Bytes.toBytes(\"j\");\n    org.apache.hadoop.hbase.HRegionInfo regionInfoX = new org.apache.hadoop.hbase.HRegionInfo(tableDesc, startKeyX, endKeyX);\n    master.getRegionManager().offlineMetaRegionWithStartKey(startKey0);\n    master.getRegionManager().putMetaRegionOnline(meta0);\n    master.getRegionManager().putMetaRegionOnline(meta1);\n    master.getRegionManager().putMetaRegionOnline(meta2);\n    org.apache.hadoop.hbase.HBaseTestCase.assertEquals(metaRegionInfo1.getStartKey(), master.getRegionManager().getFirstMetaRegionForRegion(regionInfoX).getStartKey());\n    org.apache.hadoop.hbase.HBaseTestCase.assertEquals(metaRegionInfo1.getRegionName(), master.getRegionManager().getFirstMetaRegionForRegion(regionInfoX).getRegionName());\n}",
            "ClassName": "TestRegionManager",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetFirstMetaRegionForRegionAfterMetaSplit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 28,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.HRegionInfo a = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(\"a\"), null, null);\n    org.apache.hadoop.hbase.HRegionInfo b = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(\"b\"), null, null);\n    junit.framework.Assert.assertTrue(a.compareTo(b) != 0);\n    org.apache.hadoop.hbase.HTableDescriptor t = new org.apache.hadoop.hbase.HTableDescriptor(\"t\");\n    byte[] midway = org.apache.hadoop.hbase.util.Bytes.toBytes(\"midway\");\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, null, midway);\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, midway, null);\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n    junit.framework.Assert.assertTrue(b.compareTo(a) > 0);\n    junit.framework.Assert.assertEquals(a, a);\n    junit.framework.Assert.assertTrue(a.compareTo(a) == 0);\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"a\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"d\"));\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"e\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"g\"));\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"dddd\"));\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"e\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"g\"));\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"dddd\"));\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"eeee\"));\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n}",
            "ClassName": "TestCompare",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testHRegionInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return ((org.apache.hadoop.hbase.HRegionInfo) (org.apache.hadoop.hbase.util.Writables.getWritable(bytes, new org.apache.hadoop.hbase.HRegionInfo())));\n}",
            "ClassName": "Writables",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getHRegionInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress>> result = new java.util.ArrayList<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress>>();\n    java.util.Set<org.apache.hadoop.hbase.master.MetaRegion> regions = this.regionManager.getMetaRegionsForTable(tableName);\n    byte[] firstRowInTable = org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.util.Bytes.toString(tableName) + \",,\");\n    for (org.apache.hadoop.hbase.master.MetaRegion m : regions) {\n        byte[] metaRegionName = m.getRegionName();\n        org.apache.hadoop.hbase.ipc.HRegionInterface srvr = this.connection.getHRegionConnection(m.getServer());\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(firstRowInTable);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        long scannerid = srvr.openScanner(metaRegionName, scan);\n        try {\n            while (true) {\n                org.apache.hadoop.hbase.client.Result data = srvr.next(scannerid);\n                if ((data == null) || (data.size() <= 0))\n                    break;\n\n                org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(data.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER));\n                if (org.apache.hadoop.hbase.util.Bytes.equals(info.getTableDesc().getName(), tableName)) {\n                    byte[] value = data.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n                    if (value != null) {\n                        org.apache.hadoop.hbase.HServerAddress server = new org.apache.hadoop.hbase.HServerAddress(org.apache.hadoop.hbase.util.Bytes.toString(value));\n                        result.add(new org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress>(info, server));\n                    }\n                } else {\n                    break;\n                }\n            } \n        } finally {\n            srvr.close(scannerid);\n        }\n    }\n    return result;\n}",
            "ClassName": "HMaster",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 34,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.HRegionInfo.parseRegionName(regionName)[0];\n    java.util.Set<org.apache.hadoop.hbase.master.MetaRegion> regions = regionManager.getMetaRegionsForTable(tableName);\n    for (org.apache.hadoop.hbase.master.MetaRegion m : regions) {\n        byte[] metaRegionName = m.getRegionName();\n        org.apache.hadoop.hbase.ipc.HRegionInterface srvr = connection.getHRegionConnection(m.getServer());\n        org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(regionName);\n        get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        org.apache.hadoop.hbase.client.Result data = srvr.get(metaRegionName, get);\n        if ((data == null) || (data.size() <= 0))\n            continue;\n\n        org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(data.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER));\n        byte[] value = data.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        if (value != null) {\n            org.apache.hadoop.hbase.HServerAddress server = new org.apache.hadoop.hbase.HServerAddress(org.apache.hadoop.hbase.util.Bytes.toString(value));\n            return new org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress>(info, server);\n        }\n    }\n    return null;\n}",
            "ClassName": "HMaster",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableRegionFromName",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestRegionManager",
        "Commit": "0491cb140a7563f7c4d51eb2aa43d1376f6ea2f3",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 17 May 2010 22:23:32 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testGetFirstMetaRegionForRegionAfterMetaSplit",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 28,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274135012"
    },
    {
        "Body": "{\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper zkw = new org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper(conf, org.apache.hadoop.hbase.EmptyWatcher.instance);\n    java.lang.String quorumServers = zkw.getQuorumServers();\n    int sessionTimeout = 5 * 1000;\n    org.apache.hadoop.hbase.client.HConnection connection = org.apache.hadoop.hbase.client.HConnectionManager.getConnection(conf);\n    org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper connectionZK = connection.getZooKeeperWrapper();\n    long sessionID = connectionZK.getSessionID();\n    byte[] password = connectionZK.getSessionPassword();\n    org.apache.zookeeper.ZooKeeper zk = new org.apache.zookeeper.ZooKeeper(quorumServers, sessionTimeout, org.apache.hadoop.hbase.EmptyWatcher.instance, sessionID, password);\n    zk.close();\n    java.lang.Thread.sleep(sessionTimeout * 3L);\n    java.lang.System.err.println(\"ZooKeeper should have timed out\");\n    connection.relocateRegion(org.apache.hadoop.hbase.HConstants.ROOT_TABLE_NAME, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper zkw = new org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper(conf, org.apache.hadoop.hbase.EmptyWatcher.instance);\n    java.lang.String quorumServers = zkw.getQuorumServers();\n    int sessionTimeout = 5 * 1000;\n    byte[] password = nodeZK.getSessionPassword();\n    long sessionID = nodeZK.getSessionID();\n    org.apache.zookeeper.ZooKeeper zk = new org.apache.zookeeper.ZooKeeper(quorumServers, sessionTimeout, org.apache.hadoop.hbase.EmptyWatcher.instance, sessionID, password);\n    zk.close();\n    final long sleep = sessionTimeout * 5L;\n    LOG.info(\"ZK Closed; sleeping=\" + sleep);\n    java.lang.Thread.sleep(sleep);\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "expireSession",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 13,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    return new org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper(conf, this);\n}",
            "ClassName": "RESTServlet",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "initZooKeeperWrapper",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    try {\n        zooKeeper.close();\n        org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.LOG.debug(\"Closed connection with ZooKeeper\");\n    } catch (java.lang.InterruptedException e) {\n        org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.LOG.warn(\"Failed to close connection with ZooKeeper\");\n    }\n}",
            "ClassName": "ZooKeeperWrapper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "close",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    this.zookeeper = zookeeper;\n}",
            "ClassName": "ZKMasterAddressWatcher",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setZookeeper",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return zooKeeper;\n}",
            "ClassName": "ZooKeeperWrapper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getZooKeeper",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestZooKeeper",
        "Commit": "0491cb140a7563f7c4d51eb2aa43d1376f6ea2f3",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 17 May 2010 22:23:32 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testClientSessionExpired",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 1,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 15,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 2,
        "ProjectName": "hbase",
        "Timestamp": "1274135012"
    },
    {
        "Body": "{\n    table = new org.apache.hadoop.hbase.client.HTable(conf, \"test\");\n    junit.framework.Assert.assertEquals(\"Test table should have 20 regions\", 20, table.getStartKeys().length);\n    assertRegionsAreBalanced();\n    LOG.debug(\"Adding 2nd region server.\");\n    cluster.startRegionServer();\n    assertRegionsAreBalanced();\n    LOG.debug(\"Adding 3rd region server.\");\n    cluster.startRegionServer();\n    assertRegionsAreBalanced();\n    LOG.debug(\"Killing the 3rd region server.\");\n    cluster.stopRegionServer(2, false);\n    cluster.waitOnRegionServer(2);\n    assertRegionsAreBalanced();\n    LOG.debug(\"Adding 3rd region server\");\n    cluster.startRegionServer();\n    LOG.debug(\"Adding 4th region server\");\n    cluster.startRegionServer();\n    assertRegionsAreBalanced();\n    for (int i = 0; i < 6; i++) {\n        LOG.debug((\"Adding \" + (i + 5)) + \"th region server\");\n        cluster.startRegionServer();\n    }\n    assertRegionsAreBalanced();\n}",
        "CUT_1": {
            "Body": "{\n    table = new org.apache.hadoop.hbase.client.HTable(conf, \"test\");\n    junit.framework.Assert.assertEquals(\"Test table should have 20 regions\", 20, table.getStartKeys().length);\n    assertRegionsAreBalanced();\n    LOG.debug(\"Adding 2nd region server.\");\n    cluster.startRegionServer();\n    assertRegionsAreBalanced();\n    LOG.debug(\"Adding 3rd region server.\");\n    cluster.startRegionServer();\n    assertRegionsAreBalanced();\n    LOG.debug(\"Killing the 3rd region server.\");\n    cluster.stopRegionServer(2, false);\n    cluster.waitOnRegionServer(2);\n    assertRegionsAreBalanced();\n    LOG.debug(\"Adding 3rd region server\");\n    cluster.startRegionServer();\n    LOG.debug(\"Adding 4th region server\");\n    cluster.startRegionServer();\n    assertRegionsAreBalanced();\n    for (int i = 0; i < 6; i++) {\n        LOG.debug((\"Adding \" + (i + 5)) + \"th region server\");\n        cluster.startRegionServer();\n    }\n    assertRegionsAreBalanced();\n}",
            "ClassName": "TestRegionRebalancing",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testRebalancing",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 25,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    this.cluster = cluster;\n}",
            "ClassName": "Client",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setCluster",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil.RegionServerThread> regionThreads = cluster.getRegionServerThreads();\n    int server = -1;\n    for (int i = 0; (i < regionThreads.size()) && (server == (-1)); i++) {\n        org.apache.hadoop.hbase.regionserver.HRegionServer s = regionThreads.get(i).getRegionServer();\n        java.util.Collection<org.apache.hadoop.hbase.regionserver.HRegion> regions = s.getOnlineRegions();\n        for (org.apache.hadoop.hbase.regionserver.HRegion r : regions) {\n            if (org.apache.hadoop.hbase.util.Bytes.equals(r.getTableDesc().getName(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME)) {\n                server = i;\n            }\n        }\n    }\n    if (server == (-1)) {\n        LOG.fatal(\"could not find region server serving meta region\");\n        junit.framework.Assert.fail();\n    }\n    if (abort) {\n        this.cluster.abortRegionServer(server);\n    } else {\n        this.cluster.stopRegionServer(server);\n    }\n    LOG.info((this.cluster.waitOnRegionServer(server) + \" has been \") + (abort ? \"aborted\" : \"shut down\"));\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "stopOrAbortMetaRegionServer",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return cluster;\n}",
            "ClassName": "Client",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getCluster",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return server;\n}",
            "ClassName": "MetaRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getServer",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestRegionRebalancing",
        "Commit": "0491cb140a7563f7c4d51eb2aa43d1376f6ea2f3",
        "CyclomaticComplexity": 1,
        "Date": "Mon, 17 May 2010 22:23:32 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testRebalancing",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 25,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274135012"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.TestFullLogReconstruction.TEST_UTIL.createTable(org.apache.hadoop.hbase.TestFullLogReconstruction.TABLE_NAME, org.apache.hadoop.hbase.TestFullLogReconstruction.FAMILY);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.TestFullLogReconstruction.TABLE_NAME);\n    org.apache.hadoop.hbase.TestFullLogReconstruction.TEST_UTIL.createMultiRegions(table, org.apache.hadoop.hbase.util.Bytes.toBytes(\"family\"));\n    int initialCount = org.apache.hadoop.hbase.TestFullLogReconstruction.TEST_UTIL.loadTable(table, org.apache.hadoop.hbase.TestFullLogReconstruction.FAMILY);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.ResultScanner results = table.getScanner(scan);\n    int count = 0;\n    for (org.apache.hadoop.hbase.client.Result res : results) {\n        count++;\n    }\n    results.close();\n    org.junit.Assert.assertEquals(initialCount, count);\n    for (int i = 0; i < 4; i++) {\n        org.apache.hadoop.hbase.TestFullLogReconstruction.TEST_UTIL.loadTable(table, org.apache.hadoop.hbase.TestFullLogReconstruction.FAMILY);\n    }\n    org.apache.hadoop.hbase.TestFullLogReconstruction.TEST_UTIL.expireRegionServerSession(0);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    results = table.getScanner(scan);\n    int newCount = 0;\n    for (org.apache.hadoop.hbase.client.Result res : results) {\n        newCount++;\n    }\n    org.junit.Assert.assertEquals(count, newCount);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.TestFullLogReconstruction.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFullLogReconstruction",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(getConfiguration(), tableName);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.ResultScanner resScan = table.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result res : resScan) {\n        org.apache.hadoop.hbase.client.Delete del = new org.apache.hadoop.hbase.client.Delete(res.getRow());\n        table.delete(del);\n    }\n    return table;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "truncateTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 10,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    byte[] startrow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"bbb\");\n    byte[] stoprow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"ccc\");\n    try {\n        this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n        org.apache.hadoop.hbase.HBaseTestCase.addContent(this.r, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"abc\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"abd\"));\n        scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n        int count = 0;\n        while (s.next(results)) {\n            count++;\n        } \n        s.close();\n        junit.framework.Assert.assertEquals(0, count);\n        scan = new org.apache.hadoop.hbase.client.Scan(startrow, stoprow);\n        scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        s = r.getScanner(scan);\n        count = 0;\n        org.apache.hadoop.hbase.KeyValue kv = null;\n        results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        for (boolean first = true; s.next(results);) {\n            kv = results.get(0);\n            if (first) {\n                junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.BYTES_COMPARATOR.compare(startrow, kv.getRow()) == 0);\n                first = false;\n            }\n            count++;\n        }\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.BYTES_COMPARATOR.compare(stoprow, kv.getRow()) > 0);\n        junit.framework.Assert.assertTrue(count > 10);\n        s.close();\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(this.cluster);\n    }\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testStopRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 39,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        rows++;\n    }\n    s.close();\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "countOfMetaRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        org.apache.hadoop.hbase.HRegionInfo hri = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(b);\n        byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(row);\n        p.add(org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily(), org.apache.hadoop.hbase.master.TestMasterTransitions.getTestQualifier(), row);\n        t.put(p);\n        rows++;\n    }\n    s.close();\n    org.junit.Assert.assertEquals(expected, rows);\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addToEachStartKey",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFullLogReconstruction",
        "Commit": "0491cb140a7563f7c4d51eb2aa43d1376f6ea2f3",
        "CyclomaticComplexity": 3,
        "Date": "Mon, 17 May 2010 22:23:32 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testReconstruction",
        "NumberOfAsserts": 2,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 25,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274135012"
    },
    {
        "Body": "{\n    new org.apache.hadoop.hbase.client.HTable(conf, \".META.\");\n    int port = cluster.getMaster().getInfoServer().getPort();\n    assertHasExpectedContent(new java.net.URL((\"http://localhost:\" + port) + \"/index.html\"), \"master\");\n    port = cluster.getRegionServerThreads().get(0).getRegionServer().getInfoServer().getPort();\n    assertHasExpectedContent(new java.net.URL((\"http://localhost:\" + port) + \"/index.html\"), \"regionserver\");\n}",
        "CUT_1": {
            "Body": "{\n    new org.apache.hadoop.hbase.client.HTable(conf, \".META.\");\n    int port = cluster.getMaster().getInfoServer().getPort();\n    assertHasExpectedContent(new java.net.URL((\"http://localhost:\" + port) + \"/index.html\"), \"master\");\n    port = cluster.getRegionServerThreads().get(0).getRegionServer().getInfoServer().getPort();\n    assertHasExpectedContent(new java.net.URL((\"http://localhost:\" + port) + \"/index.html\"), \"regionserver\");\n}",
            "ClassName": "TestInfoServers",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testInfoServersAreUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    try {\n        while (true) {\n            try {\n                hbaseCluster = new org.apache.hadoop.hbase.LocalHBaseCluster(conf, nRegionNodes, org.apache.hadoop.hbase.MiniHBaseCluster.MiniHBaseClusterMaster.class, org.apache.hadoop.hbase.MiniHBaseCluster.MiniHBaseClusterRegionServer.class);\n                hbaseCluster.startup();\n            } catch (java.net.BindException e) {\n                int port = conf.getInt(org.apache.hadoop.hbase.HConstants.MASTER_PORT, org.apache.hadoop.hbase.HConstants.DEFAULT_MASTER_PORT);\n                org.apache.hadoop.hbase.MiniHBaseCluster.LOG.info(\"Failed binding Master to port: \" + port, e);\n                port++;\n                conf.setInt(org.apache.hadoop.hbase.HConstants.MASTER_PORT, port);\n                continue;\n            }\n            break;\n        } \n    } catch (java.io.IOException e) {\n        shutdown();\n        throw e;\n    }\n}",
            "ClassName": "MiniHBaseCluster",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "init",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 20,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    conf.setInt(\"hbase.master.info.port\", 60011);\n    conf.setInt(\"hbase.regionserver.info.port\", 60031);\n}",
            "ClassName": "TestInfoServers",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "preHBaseClusterSetup",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    java.lang.String n = java.lang.Thread.currentThread().getName();\n    java.lang.Thread.UncaughtExceptionHandler handler = new java.lang.Thread.UncaughtExceptionHandler() {\n        public void uncaughtException(java.lang.Thread t, java.lang.Throwable e) {\n            abort();\n            org.apache.hadoop.hbase.regionserver.HRegionServer.LOG.fatal(\"Set stop flag in \" + t.getName(), e);\n        }\n    };\n    org.apache.hadoop.hbase.util.Threads.setDaemonThreadRunning(this.hlogRoller, n + \".logRoller\", handler);\n    org.apache.hadoop.hbase.util.Threads.setDaemonThreadRunning(this.cacheFlusher, n + \".cacheFlusher\", handler);\n    org.apache.hadoop.hbase.util.Threads.setDaemonThreadRunning(this.compactSplitThread, n + \".compactor\", handler);\n    org.apache.hadoop.hbase.util.Threads.setDaemonThreadRunning(this.workerThread, n + \".worker\", handler);\n    org.apache.hadoop.hbase.util.Threads.setDaemonThreadRunning(this.majorCompactionChecker, n + \".majorCompactionChecker\", handler);\n    this.leases.setName(n + \".leaseChecker\");\n    this.leases.start();\n    int port = this.conf.getInt(\"hbase.regionserver.info.port\", 60030);\n    if (port >= 0) {\n        java.lang.String addr = this.conf.get(\"hbase.regionserver.info.bindAddress\", \"0.0.0.0\");\n        boolean auto = this.conf.getBoolean(\"hbase.regionserver.info.port.auto\", false);\n        while (true) {\n            try {\n                this.infoServer = new org.apache.hadoop.hbase.util.InfoServer(\"regionserver\", addr, port, false);\n                this.infoServer.setAttribute(\"regionserver\", this);\n                this.infoServer.start();\n                break;\n            } catch (java.net.BindException e) {\n                if (!auto) {\n                    throw e;\n                }\n                org.apache.hadoop.hbase.regionserver.HRegionServer.LOG.info(\"Failed binding http info server to port: \" + port);\n                port++;\n                this.serverInfo.setInfoPort(port);\n            }\n        } \n    }\n    this.server.start();\n    org.apache.hadoop.hbase.regionserver.HRegionServer.LOG.info(\"HRegionServer started at: \" + this.serverInfo.getServerAddress().toString());\n}",
            "ClassName": "HRegionServer",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startServiceThreads",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 38,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 45
        },
        "CUT_5": {
            "Body": "{\n    java.net.URL url = org.apache.hadoop.hbase.util.InfoServer.class.getClassLoader().getResource(path);\n    if (url == null)\n        throw new java.io.IOException(\"webapps not found in CLASSPATH: \" + path);\n\n    return url.toString();\n}",
            "ClassName": "InfoServer",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getWebAppsPath",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestInfoServers",
        "Commit": "c83d862620d2895e28179161b456235fd28a3a2b",
        "CyclomaticComplexity": 0,
        "Date": "Tue, 18 May 2010 21:37:46 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testInfoServersAreUp",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 7,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274218666"
    },
    {
        "Body": "{\n    final int COL_COUNT = 10;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"tablename\");\n    final byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\");\n    this.conf.setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = null;\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(this.fs, dir, this.oldLogDir, this.conf, null);\n    try {\n        long timestamp = java.lang.System.currentTimeMillis();\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit cols = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        for (int i = 0; i < COL_COUNT; i++) {\n            cols.add(new org.apache.hadoop.hbase.KeyValue(row, org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\"), org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(i)), timestamp, new byte[]{ ((byte) (i + '0')) }));\n        }\n        org.apache.hadoop.hbase.HRegionInfo hri = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(tableName), org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.HConstants.EMPTY_END_ROW);\n        log.append(hri, tableName, cols, java.lang.System.currentTimeMillis());\n        long logSeqId = log.startCacheFlush();\n        log.completeCacheFlush(hri.getRegionName(), tableName, logSeqId, false);\n        log.close();\n        org.apache.hadoop.fs.Path filename = log.computeFilename(log.getFilenum());\n        log = null;\n        reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, filename, conf);\n        org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = reader.next();\n        junit.framework.Assert.assertEquals(COL_COUNT, entry.getEdit().size());\n        int idx = 0;\n        for (org.apache.hadoop.hbase.KeyValue val : entry.getEdit().getKeyValues()) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getRegionName(), entry.getKey().getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, entry.getKey().getTablename()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(row, val.getRow()));\n            junit.framework.Assert.assertEquals(((byte) (idx + '0')), val.getValue()[0]);\n            java.lang.System.out.println((entry.getKey() + \" \") + val);\n            idx++;\n        }\n        entry = reader.next();\n        junit.framework.Assert.assertEquals(1, entry.getEdit().size());\n        for (org.apache.hadoop.hbase.KeyValue val : entry.getEdit().getKeyValues()) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getRegionName(), entry.getKey().getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, entry.getKey().getTablename()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, val.getRow()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, val.getFamily()));\n            junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH, val.getValue()));\n            java.lang.System.out.println((entry.getKey() + \" \") + val);\n        }\n    } finally {\n        if (log != null) {\n            log.closeAndDelete();\n        }\n        if (reader != null) {\n            reader.close();\n        }\n    }\n}",
        "CUT_1": {
            "Body": "{\n    final int COL_COUNT = 10;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"tablename\");\n    final byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\");\n    this.conf.setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = null;\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(this.fs, dir, this.oldLogDir, this.conf, null);\n    try {\n        long timestamp = java.lang.System.currentTimeMillis();\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit cols = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        for (int i = 0; i < COL_COUNT; i++) {\n            cols.add(new org.apache.hadoop.hbase.KeyValue(row, org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\"), org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(i)), timestamp, new byte[]{ ((byte) (i + '0')) }));\n        }\n        org.apache.hadoop.hbase.HRegionInfo hri = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(tableName), org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.HConstants.EMPTY_END_ROW);\n        log.append(hri, tableName, cols, java.lang.System.currentTimeMillis());\n        long logSeqId = log.startCacheFlush();\n        log.completeCacheFlush(hri.getRegionName(), tableName, logSeqId, false);\n        log.close();\n        org.apache.hadoop.fs.Path filename = log.computeFilename(log.getFilenum());\n        log = null;\n        reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, filename, conf);\n        org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = reader.next();\n        junit.framework.Assert.assertEquals(COL_COUNT, entry.getEdit().size());\n        int idx = 0;\n        for (org.apache.hadoop.hbase.KeyValue val : entry.getEdit().getKeyValues()) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getRegionName(), entry.getKey().getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, entry.getKey().getTablename()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(row, val.getRow()));\n            junit.framework.Assert.assertEquals(((byte) (idx + '0')), val.getValue()[0]);\n            java.lang.System.out.println((entry.getKey() + \" \") + val);\n            idx++;\n        }\n        entry = reader.next();\n        junit.framework.Assert.assertEquals(1, entry.getEdit().size());\n        for (org.apache.hadoop.hbase.KeyValue val : entry.getEdit().getKeyValues()) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getRegionName(), entry.getKey().getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, entry.getKey().getTablename()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, val.getRow()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, val.getFamily()));\n            junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH, val.getValue()));\n            java.lang.System.out.println((entry.getKey() + \" \") + val);\n        }\n    } finally {\n        if (log != null) {\n            log.closeAndDelete();\n        }\n        if (reader != null) {\n            reader.close();\n        }\n    }\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testAppend",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 51,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    final int COL_COUNT = 10;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"tablename\");\n    final byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\");\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = null;\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(fs, dir, this.oldLogDir, this.conf, null);\n    try {\n        long timestamp = java.lang.System.currentTimeMillis();\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit cols = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        for (int i = 0; i < COL_COUNT; i++) {\n            cols.add(new org.apache.hadoop.hbase.KeyValue(row, org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\"), org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(i)), timestamp, new byte[]{ ((byte) (i + '0')) }));\n        }\n        org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(tableName), row, org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.util.Bytes.toString(row) + \"1\"), false);\n        final byte[] regionName = info.getRegionName();\n        log.append(info, tableName, cols, java.lang.System.currentTimeMillis());\n        long logSeqId = log.startCacheFlush();\n        log.completeCacheFlush(regionName, tableName, logSeqId, info.isMetaRegion());\n        log.close();\n        org.apache.hadoop.fs.Path filename = log.computeFilename(log.getFilenum());\n        log = null;\n        reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, filename, conf);\n        for (int i = 0; i < 1; i++) {\n            org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = reader.next(null);\n            if (entry == null)\n                break;\n\n            org.apache.hadoop.hbase.regionserver.wal.HLogKey key = entry.getKey();\n            org.apache.hadoop.hbase.regionserver.wal.WALEdit val = entry.getEdit();\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(regionName, key.getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, key.getTablename()));\n            org.apache.hadoop.hbase.KeyValue kv = val.getKeyValues().get(0);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(row, kv.getRow()));\n            junit.framework.Assert.assertEquals(((byte) (i + '0')), kv.getValue()[0]);\n            java.lang.System.out.println((key + \" \") + val);\n        }\n        org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = null;\n        while ((entry = reader.next(null)) != null) {\n            org.apache.hadoop.hbase.regionserver.wal.HLogKey key = entry.getKey();\n            org.apache.hadoop.hbase.regionserver.wal.WALEdit val = entry.getEdit();\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(regionName, key.getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, key.getTablename()));\n            org.apache.hadoop.hbase.KeyValue kv = val.getKeyValues().get(0);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, kv.getRow()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, kv.getFamily()));\n            junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH, val.getKeyValues().get(0).getValue()));\n            java.lang.System.out.println((key + \" \") + val);\n        } \n    } finally {\n        if (log != null) {\n            log.closeAndDelete();\n        }\n        if (reader != null) {\n            reader.close();\n        }\n    }\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 6,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testEditAdd",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 56,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    byte[] bytes = org.apache.hadoop.hbase.util.Bytes.toBytes(getName());\n    org.apache.hadoop.fs.Path p = new org.apache.hadoop.fs.Path(this.dir, getName() + \".fsdos\");\n    org.apache.hadoop.fs.FSDataOutputStream out = fs.create(p);\n    out.write(bytes);\n    out.sync();\n    org.apache.hadoop.fs.FSDataInputStream in = fs.open(p);\n    junit.framework.Assert.assertTrue(in.available() > 0);\n    byte[] buffer = new byte[1024];\n    int read = in.read(buffer);\n    junit.framework.Assert.assertEquals(bytes.length, read);\n    out.close();\n    in.close();\n    org.apache.hadoop.fs.Path subdir = new org.apache.hadoop.fs.Path(this.dir, \"hlogdir\");\n    org.apache.hadoop.hbase.regionserver.wal.HLog wal = new org.apache.hadoop.hbase.regionserver.wal.HLog(this.fs, subdir, this.oldLogDir, this.conf, null);\n    final int total = 20;\n    org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(bytes), null, null, false);\n    for (int i = 0; i < total; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit kvs = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        kvs.add(new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(i), bytes, bytes));\n        wal.append(info, bytes, kvs, java.lang.System.currentTimeMillis());\n    }\n    wal.sync();\n    org.apache.hadoop.fs.Path walPath = wal.computeFilename(wal.getFilenum());\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, walPath, conf);\n    int count = 0;\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = new org.apache.hadoop.hbase.regionserver.wal.HLog.Entry();\n    while ((entry = reader.next(entry)) != null)\n        count++;\n\n    junit.framework.Assert.assertEquals(total, count);\n    reader.close();\n    for (int i = 0; i < total; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit kvs = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        kvs.add(new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(i), bytes, bytes));\n        wal.append(info, bytes, kvs, java.lang.System.currentTimeMillis());\n    }\n    reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, walPath, conf);\n    count = 0;\n    while ((entry = reader.next(entry)) != null)\n        count++;\n\n    junit.framework.Assert.assertTrue(count >= total);\n    reader.close();\n    wal.sync();\n    reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, walPath, conf);\n    count = 0;\n    while ((entry = reader.next(entry)) != null)\n        count++;\n\n    junit.framework.Assert.assertEquals(total * 2, count);\n    final byte[] value = new byte[1025 * 1024];\n    for (int i = 0; i < total; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit kvs = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        kvs.add(new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(i), bytes, value));\n        wal.append(info, bytes, kvs, java.lang.System.currentTimeMillis());\n    }\n    wal.sync();\n    reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, walPath, conf);\n    count = 0;\n    while ((entry = reader.next(entry)) != null)\n        count++;\n\n    junit.framework.Assert.assertEquals(total * 3, count);\n    reader.close();\n    wal.close();\n    reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, walPath, conf);\n    count = 0;\n    while ((entry = reader.next(entry)) != null)\n        count++;\n\n    junit.framework.Assert.assertEquals(total * 3, count);\n    reader.close();\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 8,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "Broken_testSync",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 74,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    junit.framework.Assert.assertEquals(howmany, splits.size());\n    for (int i = 0; i < splits.size(); i++) {\n        org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(this.fs, splits.get(i), conf);\n        try {\n            int count = 0;\n            java.lang.String previousRegion = null;\n            long seqno = -1;\n            org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = new org.apache.hadoop.hbase.regionserver.wal.HLog.Entry();\n            while ((entry = reader.next(entry)) != null) {\n                org.apache.hadoop.hbase.regionserver.wal.HLogKey key = entry.getKey();\n                org.apache.hadoop.hbase.regionserver.wal.WALEdit kv = entry.getEdit();\n                java.lang.String region = org.apache.hadoop.hbase.util.Bytes.toString(key.getRegionName());\n                if (previousRegion != null) {\n                    junit.framework.Assert.assertEquals(previousRegion, region);\n                }\n                junit.framework.Assert.assertTrue(seqno < key.getLogSeqNum());\n                seqno = key.getLogSeqNum();\n                previousRegion = region;\n                count++;\n            } \n            junit.framework.Assert.assertEquals(howmany * howmany, count);\n        } finally {\n            reader.close();\n        }\n    }\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "verifySplits",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 27,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.KeyValue kv = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, null, java.lang.System.currentTimeMillis(), org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH);\n    org.apache.hadoop.hbase.regionserver.wal.WALEdit e = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n    e.add(kv);\n    return e;\n}",
            "ClassName": "HLog",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "completeCacheFlushLogEdit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHLog",
        "Commit": "96321df58284a75fcb0b38f989dc14c0bb3242b0",
        "CyclomaticComplexity": 5,
        "Date": "Tue, 18 May 2010 23:23:02 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testAppend",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 51,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274224982"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.master.TestMasterWithDisabling.TABLENAME);\n    htd.setMaxFileSize(1024);\n    htd.setMemStoreFlushSize(1024);\n    org.apache.hadoop.hbase.HColumnDescriptor hcd = new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.master.TestMasterWithDisabling.FAMILYNAME);\n    htd.addFamily(hcd);\n    org.apache.hadoop.hbase.master.TestMasterWithDisabling.TEST_UTIL.getHBaseAdmin().createTable(htd);\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterWithDisabling.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterWithDisabling.TABLENAME);\n    org.apache.hadoop.hbase.master.TestMasterWithDisabling.HBase2515Listener list = new org.apache.hadoop.hbase.master.TestMasterWithDisabling.HBase2515Listener(org.apache.hadoop.hbase.master.TestMasterWithDisabling.TEST_UTIL.getHBaseAdmin());\n    org.apache.hadoop.hbase.MiniHBaseCluster cluster = org.apache.hadoop.hbase.master.TestMasterWithDisabling.TEST_UTIL.getHBaseCluster();\n    org.apache.hadoop.hbase.master.HMaster m = cluster.getMaster();\n    m.getRegionServerOperationQueue().registerRegionServerOperationListener(list);\n    try {\n        org.apache.hadoop.hbase.master.TestMasterWithDisabling.TEST_UTIL.loadTable(t, org.apache.hadoop.hbase.master.TestMasterWithDisabling.FAMILYNAME);\n    } catch (java.io.IOException ex) {\n        org.apache.hadoop.hbase.master.TestMasterWithDisabling.LOG.info(\"Expected\", ex);\n    }\n    org.junit.Assert.assertEquals(0, cluster.getMaster().getClusterStatus().getRegionsInTransition().size());\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestMasterWithDisabling.TEST_UTIL.startMiniCluster(2);\n}",
            "ClassName": "TestMasterWithDisabling",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "beforeAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestMasterWithDisabling.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestMasterWithDisabling",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "afterAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        org.apache.hadoop.hbase.HRegionInfo hri = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(b);\n        byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(row);\n        p.add(org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily(), org.apache.hadoop.hbase.master.TestMasterTransitions.getTestQualifier(), row);\n        t.put(p);\n        rows++;\n    }\n    s.close();\n    org.junit.Assert.assertEquals(expected, rows);\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addToEachStartKey",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.Get g = new org.apache.hadoop.hbase.client.Get(row);\n    org.junit.Assert.assertTrue(t.get(g).size() > 0);\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "assertRegionIsBackOnline",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    org.apache.hadoop.hbase.master.HMaster master = this.cluster.getMaster();\n    org.apache.hadoop.hbase.HServerAddress address = master.getMasterAddress();\n    org.apache.hadoop.hbase.HTableDescriptor tableDesc = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.util.Bytes.toBytes(\"_MY_TABLE_\"));\n    org.apache.hadoop.hbase.HTableDescriptor metaTableDesc = meta.getTableDescriptor();\n    byte[] startKey0 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"f\");\n    byte[] endKey0 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"h\");\n    org.apache.hadoop.hbase.HRegionInfo regionInfo0 = new org.apache.hadoop.hbase.HRegionInfo(tableDesc, startKey0, endKey0);\n    org.apache.hadoop.hbase.HRegionInfo metaRegionInfo0 = new org.apache.hadoop.hbase.HRegionInfo(metaTableDesc, org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), regionInfo0.getRegionName());\n    org.apache.hadoop.hbase.master.MetaRegion meta0 = new org.apache.hadoop.hbase.master.MetaRegion(address, metaRegionInfo0);\n    byte[] startKey1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"j\");\n    byte[] endKey1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"m\");\n    org.apache.hadoop.hbase.HRegionInfo regionInfo1 = new org.apache.hadoop.hbase.HRegionInfo(tableDesc, startKey1, endKey1);\n    org.apache.hadoop.hbase.HRegionInfo metaRegionInfo1 = new org.apache.hadoop.hbase.HRegionInfo(metaTableDesc, regionInfo0.getRegionName(), regionInfo1.getRegionName());\n    org.apache.hadoop.hbase.master.MetaRegion meta1 = new org.apache.hadoop.hbase.master.MetaRegion(address, metaRegionInfo1);\n    org.apache.hadoop.hbase.HRegionInfo metaRegionInfo2 = new org.apache.hadoop.hbase.HRegionInfo(metaTableDesc, regionInfo1.getRegionName(), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"));\n    org.apache.hadoop.hbase.master.MetaRegion meta2 = new org.apache.hadoop.hbase.master.MetaRegion(address, metaRegionInfo2);\n    byte[] startKeyX = org.apache.hadoop.hbase.util.Bytes.toBytes(\"h\");\n    byte[] endKeyX = org.apache.hadoop.hbase.util.Bytes.toBytes(\"j\");\n    org.apache.hadoop.hbase.HRegionInfo regionInfoX = new org.apache.hadoop.hbase.HRegionInfo(tableDesc, startKeyX, endKeyX);\n    master.getRegionManager().offlineMetaRegionWithStartKey(startKey0);\n    master.getRegionManager().putMetaRegionOnline(meta0);\n    master.getRegionManager().putMetaRegionOnline(meta1);\n    master.getRegionManager().putMetaRegionOnline(meta2);\n    org.apache.hadoop.hbase.HBaseTestCase.assertEquals(metaRegionInfo1.getStartKey(), master.getRegionManager().getFirstMetaRegionForRegion(regionInfoX).getStartKey());\n    org.apache.hadoop.hbase.HBaseTestCase.assertEquals(metaRegionInfo1.getRegionName(), master.getRegionManager().getFirstMetaRegionForRegion(regionInfoX).getRegionName());\n}",
            "ClassName": "TestRegionManager",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetFirstMetaRegionForRegionAfterMetaSplit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 28,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestMasterWithDisabling",
        "Commit": "96321df58284a75fcb0b38f989dc14c0bb3242b0",
        "CyclomaticComplexity": 0,
        "Date": "Tue, 18 May 2010 23:23:02 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testDisableBetweenSplit",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 19,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274224982"
    },
    {
        "Body": "{\n    testScan(null, null, null);\n}",
        "CUT_1": {
            "Body": "{\n    return ((x == null) && (y == null)) || ((x != null) && x.equals(y));\n}",
            "ClassName": "Pair",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "equals",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "internalPreFlushcacheCommit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HalfHFileReader",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "midkey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return null;\n}",
            "ClassName": "ScanWildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return null;\n}",
            "ClassName": "WildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableInputFormatScan",
        "Commit": "96321df58284a75fcb0b38f989dc14c0bb3242b0",
        "CyclomaticComplexity": 0,
        "Date": "Tue, 18 May 2010 23:23:02 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testScanEmptyToEmpty",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274224982"
    },
    {
        "Body": "{\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(getName());\n    final byte[] rowName = tableName;\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(this.fs, this.dir, this.oldLogDir, this.conf, null);\n    final int howmany = 3;\n    org.apache.hadoop.hbase.HRegionInfo[] infos = new org.apache.hadoop.hbase.HRegionInfo[3];\n    for (int i = 0; i < howmany; i++) {\n        infos[i] = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(tableName), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\" + i), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\" + (i + 1)), false);\n    }\n    try {\n        for (int ii = 0; ii < howmany; ii++) {\n            for (int i = 0; i < howmany; i++) {\n                for (int j = 0; j < howmany; j++) {\n                    org.apache.hadoop.hbase.regionserver.wal.WALEdit edit = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n                    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\");\n                    byte[] qualifier = org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(j));\n                    byte[] column = org.apache.hadoop.hbase.util.Bytes.toBytes(\"column:\" + java.lang.Integer.toString(j));\n                    edit.add(new org.apache.hadoop.hbase.KeyValue(rowName, family, qualifier, java.lang.System.currentTimeMillis(), column));\n                    java.lang.System.out.println(((\"Region \" + i) + \": \") + edit);\n                    log.append(infos[i], tableName, edit, java.lang.System.currentTimeMillis());\n                }\n            }\n            log.hflush();\n            log.rollWriter();\n        }\n        java.util.List<org.apache.hadoop.fs.Path> splits = org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(this.testDir, this.dir, this.oldLogDir, this.fs, this.conf);\n        verifySplits(splits, howmany);\n        log = null;\n    } finally {\n        if (log != null) {\n            log.closeAndDelete();\n        }\n    }\n}",
        "CUT_1": {
            "Body": "{\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(getName());\n    final byte[] rowName = tableName;\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(this.fs, this.dir, this.oldLogDir, this.conf, null);\n    final int howmany = 3;\n    org.apache.hadoop.hbase.HRegionInfo[] infos = new org.apache.hadoop.hbase.HRegionInfo[3];\n    for (int i = 0; i < howmany; i++) {\n        infos[i] = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(tableName), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\" + i), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\" + (i + 1)), false);\n    }\n    try {\n        for (int ii = 0; ii < howmany; ii++) {\n            for (int i = 0; i < howmany; i++) {\n                for (int j = 0; j < howmany; j++) {\n                    org.apache.hadoop.hbase.regionserver.wal.WALEdit edit = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n                    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\");\n                    byte[] qualifier = org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(j));\n                    byte[] column = org.apache.hadoop.hbase.util.Bytes.toBytes(\"column:\" + java.lang.Integer.toString(j));\n                    edit.add(new org.apache.hadoop.hbase.KeyValue(rowName, family, qualifier, java.lang.System.currentTimeMillis(), column));\n                    java.lang.System.out.println(((\"Region \" + i) + \": \") + edit);\n                    log.append(infos[i], tableName, edit, java.lang.System.currentTimeMillis());\n                }\n            }\n            log.hflush();\n            log.rollWriter();\n        }\n        java.util.List<org.apache.hadoop.fs.Path> splits = org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(this.testDir, this.dir, this.oldLogDir, this.fs, this.conf);\n        verifySplits(splits, howmany);\n        log = null;\n    } finally {\n        if (log != null) {\n            log.closeAndDelete();\n        }\n    }\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSplit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 34,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    final int COL_COUNT = 10;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"tablename\");\n    final byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\");\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = null;\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(fs, dir, this.oldLogDir, this.conf, null);\n    try {\n        long timestamp = java.lang.System.currentTimeMillis();\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit cols = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        for (int i = 0; i < COL_COUNT; i++) {\n            cols.add(new org.apache.hadoop.hbase.KeyValue(row, org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\"), org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(i)), timestamp, new byte[]{ ((byte) (i + '0')) }));\n        }\n        org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(tableName), row, org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.util.Bytes.toString(row) + \"1\"), false);\n        final byte[] regionName = info.getRegionName();\n        log.append(info, tableName, cols, java.lang.System.currentTimeMillis());\n        long logSeqId = log.startCacheFlush();\n        log.completeCacheFlush(regionName, tableName, logSeqId, info.isMetaRegion());\n        log.close();\n        org.apache.hadoop.fs.Path filename = log.computeFilename(log.getFilenum());\n        log = null;\n        reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, filename, conf);\n        for (int i = 0; i < 1; i++) {\n            org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = reader.next(null);\n            if (entry == null)\n                break;\n\n            org.apache.hadoop.hbase.regionserver.wal.HLogKey key = entry.getKey();\n            org.apache.hadoop.hbase.regionserver.wal.WALEdit val = entry.getEdit();\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(regionName, key.getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, key.getTablename()));\n            org.apache.hadoop.hbase.KeyValue kv = val.getKeyValues().get(0);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(row, kv.getRow()));\n            junit.framework.Assert.assertEquals(((byte) (i + '0')), kv.getValue()[0]);\n            java.lang.System.out.println((key + \" \") + val);\n        }\n        org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = null;\n        while ((entry = reader.next(null)) != null) {\n            org.apache.hadoop.hbase.regionserver.wal.HLogKey key = entry.getKey();\n            org.apache.hadoop.hbase.regionserver.wal.WALEdit val = entry.getEdit();\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(regionName, key.getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, key.getTablename()));\n            org.apache.hadoop.hbase.KeyValue kv = val.getKeyValues().get(0);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, kv.getRow()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, kv.getFamily()));\n            junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH, val.getKeyValues().get(0).getValue()));\n            java.lang.System.out.println((key + \" \") + val);\n        } \n    } finally {\n        if (log != null) {\n            log.closeAndDelete();\n        }\n        if (reader != null) {\n            reader.close();\n        }\n    }\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 6,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testEditAdd",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 56,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    final int COL_COUNT = 10;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"tablename\");\n    final byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\");\n    this.conf.setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = null;\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(this.fs, dir, this.oldLogDir, this.conf, null);\n    try {\n        long timestamp = java.lang.System.currentTimeMillis();\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit cols = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        for (int i = 0; i < COL_COUNT; i++) {\n            cols.add(new org.apache.hadoop.hbase.KeyValue(row, org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\"), org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(i)), timestamp, new byte[]{ ((byte) (i + '0')) }));\n        }\n        org.apache.hadoop.hbase.HRegionInfo hri = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(tableName), org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.HConstants.EMPTY_END_ROW);\n        log.append(hri, tableName, cols, java.lang.System.currentTimeMillis());\n        long logSeqId = log.startCacheFlush();\n        log.completeCacheFlush(hri.getRegionName(), tableName, logSeqId, false);\n        log.close();\n        org.apache.hadoop.fs.Path filename = log.computeFilename(log.getFilenum());\n        log = null;\n        reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, filename, conf);\n        org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = reader.next();\n        junit.framework.Assert.assertEquals(COL_COUNT, entry.getEdit().size());\n        int idx = 0;\n        for (org.apache.hadoop.hbase.KeyValue val : entry.getEdit().getKeyValues()) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getRegionName(), entry.getKey().getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, entry.getKey().getTablename()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(row, val.getRow()));\n            junit.framework.Assert.assertEquals(((byte) (idx + '0')), val.getValue()[0]);\n            java.lang.System.out.println((entry.getKey() + \" \") + val);\n            idx++;\n        }\n        entry = reader.next();\n        junit.framework.Assert.assertEquals(1, entry.getEdit().size());\n        for (org.apache.hadoop.hbase.KeyValue val : entry.getEdit().getKeyValues()) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getRegionName(), entry.getKey().getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, entry.getKey().getTablename()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, val.getRow()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, val.getFamily()));\n            junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH, val.getValue()));\n            java.lang.System.out.println((entry.getKey() + \" \") + val);\n        }\n    } finally {\n        if (log != null) {\n            log.closeAndDelete();\n        }\n        if (reader != null) {\n            reader.close();\n        }\n    }\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testAppend",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 51,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.KeyValue kv = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, null, java.lang.System.currentTimeMillis(), org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH);\n    org.apache.hadoop.hbase.regionserver.wal.WALEdit e = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n    e.add(kv);\n    return e;\n}",
            "ClassName": "HLog",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "completeCacheFlushLogEdit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(row), org.apache.hadoop.hbase.util.Bytes.toBytes(family), org.apache.hadoop.hbase.util.Bytes.toBytes(qualifier), timestamp, type, org.apache.hadoop.hbase.util.Bytes.toBytes(value));\n}",
            "ClassName": "KeyValueTestUtil",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "create",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHLog",
        "Commit": "96321df58284a75fcb0b38f989dc14c0bb3242b0",
        "CyclomaticComplexity": 5,
        "Date": "Tue, 18 May 2010 23:23:02 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testSplit",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 34,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274224982"
    },
    {
        "Body": "{\n    final int COL_COUNT = 10;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"tablename\");\n    final byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\");\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = null;\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(fs, dir, this.oldLogDir, this.conf, null);\n    try {\n        long timestamp = java.lang.System.currentTimeMillis();\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit cols = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        for (int i = 0; i < COL_COUNT; i++) {\n            cols.add(new org.apache.hadoop.hbase.KeyValue(row, org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\"), org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(i)), timestamp, new byte[]{ ((byte) (i + '0')) }));\n        }\n        org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(tableName), row, org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.util.Bytes.toString(row) + \"1\"), false);\n        final byte[] regionName = info.getRegionName();\n        log.append(info, tableName, cols, java.lang.System.currentTimeMillis());\n        long logSeqId = log.startCacheFlush();\n        log.completeCacheFlush(regionName, tableName, logSeqId, info.isMetaRegion());\n        log.close();\n        org.apache.hadoop.fs.Path filename = log.computeFilename(log.getFilenum());\n        log = null;\n        reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, filename, conf);\n        for (int i = 0; i < 1; i++) {\n            org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = reader.next(null);\n            if (entry == null)\n                break;\n\n            org.apache.hadoop.hbase.regionserver.wal.HLogKey key = entry.getKey();\n            org.apache.hadoop.hbase.regionserver.wal.WALEdit val = entry.getEdit();\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(regionName, key.getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, key.getTablename()));\n            org.apache.hadoop.hbase.KeyValue kv = val.getKeyValues().get(0);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(row, kv.getRow()));\n            junit.framework.Assert.assertEquals(((byte) (i + '0')), kv.getValue()[0]);\n            java.lang.System.out.println((key + \" \") + val);\n        }\n        org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = null;\n        while ((entry = reader.next(null)) != null) {\n            org.apache.hadoop.hbase.regionserver.wal.HLogKey key = entry.getKey();\n            org.apache.hadoop.hbase.regionserver.wal.WALEdit val = entry.getEdit();\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(regionName, key.getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, key.getTablename()));\n            org.apache.hadoop.hbase.KeyValue kv = val.getKeyValues().get(0);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, kv.getRow()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, kv.getFamily()));\n            junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH, val.getKeyValues().get(0).getValue()));\n            java.lang.System.out.println((key + \" \") + val);\n        } \n    } finally {\n        if (log != null) {\n            log.closeAndDelete();\n        }\n        if (reader != null) {\n            reader.close();\n        }\n    }\n}",
        "CUT_1": {
            "Body": "{\n    final int COL_COUNT = 10;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"tablename\");\n    final byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\");\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = null;\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(fs, dir, this.oldLogDir, this.conf, null);\n    try {\n        long timestamp = java.lang.System.currentTimeMillis();\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit cols = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        for (int i = 0; i < COL_COUNT; i++) {\n            cols.add(new org.apache.hadoop.hbase.KeyValue(row, org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\"), org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(i)), timestamp, new byte[]{ ((byte) (i + '0')) }));\n        }\n        org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(tableName), row, org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.util.Bytes.toString(row) + \"1\"), false);\n        final byte[] regionName = info.getRegionName();\n        log.append(info, tableName, cols, java.lang.System.currentTimeMillis());\n        long logSeqId = log.startCacheFlush();\n        log.completeCacheFlush(regionName, tableName, logSeqId, info.isMetaRegion());\n        log.close();\n        org.apache.hadoop.fs.Path filename = log.computeFilename(log.getFilenum());\n        log = null;\n        reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, filename, conf);\n        for (int i = 0; i < 1; i++) {\n            org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = reader.next(null);\n            if (entry == null)\n                break;\n\n            org.apache.hadoop.hbase.regionserver.wal.HLogKey key = entry.getKey();\n            org.apache.hadoop.hbase.regionserver.wal.WALEdit val = entry.getEdit();\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(regionName, key.getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, key.getTablename()));\n            org.apache.hadoop.hbase.KeyValue kv = val.getKeyValues().get(0);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(row, kv.getRow()));\n            junit.framework.Assert.assertEquals(((byte) (i + '0')), kv.getValue()[0]);\n            java.lang.System.out.println((key + \" \") + val);\n        }\n        org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = null;\n        while ((entry = reader.next(null)) != null) {\n            org.apache.hadoop.hbase.regionserver.wal.HLogKey key = entry.getKey();\n            org.apache.hadoop.hbase.regionserver.wal.WALEdit val = entry.getEdit();\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(regionName, key.getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, key.getTablename()));\n            org.apache.hadoop.hbase.KeyValue kv = val.getKeyValues().get(0);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, kv.getRow()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, kv.getFamily()));\n            junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH, val.getKeyValues().get(0).getValue()));\n            java.lang.System.out.println((key + \" \") + val);\n        } \n    } finally {\n        if (log != null) {\n            log.closeAndDelete();\n        }\n        if (reader != null) {\n            reader.close();\n        }\n    }\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 6,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testEditAdd",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 56,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    final int COL_COUNT = 10;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"tablename\");\n    final byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\");\n    this.conf.setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = null;\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(this.fs, dir, this.oldLogDir, this.conf, null);\n    try {\n        long timestamp = java.lang.System.currentTimeMillis();\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit cols = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        for (int i = 0; i < COL_COUNT; i++) {\n            cols.add(new org.apache.hadoop.hbase.KeyValue(row, org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\"), org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(i)), timestamp, new byte[]{ ((byte) (i + '0')) }));\n        }\n        org.apache.hadoop.hbase.HRegionInfo hri = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(tableName), org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.HConstants.EMPTY_END_ROW);\n        log.append(hri, tableName, cols, java.lang.System.currentTimeMillis());\n        long logSeqId = log.startCacheFlush();\n        log.completeCacheFlush(hri.getRegionName(), tableName, logSeqId, false);\n        log.close();\n        org.apache.hadoop.fs.Path filename = log.computeFilename(log.getFilenum());\n        log = null;\n        reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, filename, conf);\n        org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = reader.next();\n        junit.framework.Assert.assertEquals(COL_COUNT, entry.getEdit().size());\n        int idx = 0;\n        for (org.apache.hadoop.hbase.KeyValue val : entry.getEdit().getKeyValues()) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getRegionName(), entry.getKey().getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, entry.getKey().getTablename()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(row, val.getRow()));\n            junit.framework.Assert.assertEquals(((byte) (idx + '0')), val.getValue()[0]);\n            java.lang.System.out.println((entry.getKey() + \" \") + val);\n            idx++;\n        }\n        entry = reader.next();\n        junit.framework.Assert.assertEquals(1, entry.getEdit().size());\n        for (org.apache.hadoop.hbase.KeyValue val : entry.getEdit().getKeyValues()) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getRegionName(), entry.getKey().getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, entry.getKey().getTablename()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, val.getRow()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, val.getFamily()));\n            junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH, val.getValue()));\n            java.lang.System.out.println((entry.getKey() + \" \") + val);\n        }\n    } finally {\n        if (log != null) {\n            log.closeAndDelete();\n        }\n        if (reader != null) {\n            reader.close();\n        }\n    }\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testAppend",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 51,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    byte[] bytes = org.apache.hadoop.hbase.util.Bytes.toBytes(getName());\n    org.apache.hadoop.fs.Path p = new org.apache.hadoop.fs.Path(this.dir, getName() + \".fsdos\");\n    org.apache.hadoop.fs.FSDataOutputStream out = fs.create(p);\n    out.write(bytes);\n    out.sync();\n    org.apache.hadoop.fs.FSDataInputStream in = fs.open(p);\n    junit.framework.Assert.assertTrue(in.available() > 0);\n    byte[] buffer = new byte[1024];\n    int read = in.read(buffer);\n    junit.framework.Assert.assertEquals(bytes.length, read);\n    out.close();\n    in.close();\n    org.apache.hadoop.fs.Path subdir = new org.apache.hadoop.fs.Path(this.dir, \"hlogdir\");\n    org.apache.hadoop.hbase.regionserver.wal.HLog wal = new org.apache.hadoop.hbase.regionserver.wal.HLog(this.fs, subdir, this.oldLogDir, this.conf, null);\n    final int total = 20;\n    org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(bytes), null, null, false);\n    for (int i = 0; i < total; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit kvs = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        kvs.add(new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(i), bytes, bytes));\n        wal.append(info, bytes, kvs, java.lang.System.currentTimeMillis());\n    }\n    wal.sync();\n    org.apache.hadoop.fs.Path walPath = wal.computeFilename(wal.getFilenum());\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, walPath, conf);\n    int count = 0;\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = new org.apache.hadoop.hbase.regionserver.wal.HLog.Entry();\n    while ((entry = reader.next(entry)) != null)\n        count++;\n\n    junit.framework.Assert.assertEquals(total, count);\n    reader.close();\n    for (int i = 0; i < total; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit kvs = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        kvs.add(new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(i), bytes, bytes));\n        wal.append(info, bytes, kvs, java.lang.System.currentTimeMillis());\n    }\n    reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, walPath, conf);\n    count = 0;\n    while ((entry = reader.next(entry)) != null)\n        count++;\n\n    junit.framework.Assert.assertTrue(count >= total);\n    reader.close();\n    wal.sync();\n    reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, walPath, conf);\n    count = 0;\n    while ((entry = reader.next(entry)) != null)\n        count++;\n\n    junit.framework.Assert.assertEquals(total * 2, count);\n    final byte[] value = new byte[1025 * 1024];\n    for (int i = 0; i < total; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit kvs = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        kvs.add(new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(i), bytes, value));\n        wal.append(info, bytes, kvs, java.lang.System.currentTimeMillis());\n    }\n    wal.sync();\n    reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, walPath, conf);\n    count = 0;\n    while ((entry = reader.next(entry)) != null)\n        count++;\n\n    junit.framework.Assert.assertEquals(total * 3, count);\n    reader.close();\n    wal.close();\n    reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, walPath, conf);\n    count = 0;\n    while ((entry = reader.next(entry)) != null)\n        count++;\n\n    junit.framework.Assert.assertEquals(total * 3, count);\n    reader.close();\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 8,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "Broken_testSync",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 74,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.KeyValue kv = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, null, java.lang.System.currentTimeMillis(), org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH);\n    org.apache.hadoop.hbase.regionserver.wal.WALEdit e = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n    e.add(kv);\n    return e;\n}",
            "ClassName": "HLog",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "completeCacheFlushLogEdit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    junit.framework.Assert.assertEquals(howmany, splits.size());\n    for (int i = 0; i < splits.size(); i++) {\n        org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(this.fs, splits.get(i), conf);\n        try {\n            int count = 0;\n            java.lang.String previousRegion = null;\n            long seqno = -1;\n            org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = new org.apache.hadoop.hbase.regionserver.wal.HLog.Entry();\n            while ((entry = reader.next(entry)) != null) {\n                org.apache.hadoop.hbase.regionserver.wal.HLogKey key = entry.getKey();\n                org.apache.hadoop.hbase.regionserver.wal.WALEdit kv = entry.getEdit();\n                java.lang.String region = org.apache.hadoop.hbase.util.Bytes.toString(key.getRegionName());\n                if (previousRegion != null) {\n                    junit.framework.Assert.assertEquals(previousRegion, region);\n                }\n                junit.framework.Assert.assertTrue(seqno < key.getLogSeqNum());\n                seqno = key.getLogSeqNum();\n                previousRegion = region;\n                count++;\n            } \n            junit.framework.Assert.assertEquals(howmany * howmany, count);\n        } finally {\n            reader.close();\n        }\n    }\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "verifySplits",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 27,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHLog",
        "Commit": "96321df58284a75fcb0b38f989dc14c0bb3242b0",
        "CyclomaticComplexity": 6,
        "Date": "Tue, 18 May 2010 23:23:02 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testEditAdd",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 56,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274224982"
    },
    {
        "Body": "{\n    java.util.Map<byte[], java.lang.Long> regionsToSeqids = new java.util.HashMap<byte[], java.lang.Long>();\n    for (int i = 0; i < 10; i++) {\n        java.lang.Long l = java.lang.Long.valueOf(i);\n        regionsToSeqids.put(l.toString().getBytes(), l);\n    }\n    byte[][] regions = org.apache.hadoop.hbase.regionserver.wal.HLog.findMemstoresWithEditsOlderThan(1, regionsToSeqids);\n    junit.framework.Assert.assertEquals(1, regions.length);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(regions[0], \"0\".getBytes()));\n    regions = org.apache.hadoop.hbase.regionserver.wal.HLog.findMemstoresWithEditsOlderThan(3, regionsToSeqids);\n    int count = 3;\n    junit.framework.Assert.assertEquals(count, regions.length);\n    for (int i = 0; i < count; i++) {\n        junit.framework.Assert.assertTrue((org.apache.hadoop.hbase.util.Bytes.equals(regions[i], \"0\".getBytes()) || org.apache.hadoop.hbase.util.Bytes.equals(regions[i], \"1\".getBytes())) || org.apache.hadoop.hbase.util.Bytes.equals(regions[i], \"2\".getBytes()));\n    }\n}",
        "CUT_1": {
            "Body": "{\n    java.util.Map<byte[], java.lang.Long> regionsToSeqids = new java.util.HashMap<byte[], java.lang.Long>();\n    for (int i = 0; i < 10; i++) {\n        java.lang.Long l = java.lang.Long.valueOf(i);\n        regionsToSeqids.put(l.toString().getBytes(), l);\n    }\n    byte[][] regions = org.apache.hadoop.hbase.regionserver.wal.HLog.findMemstoresWithEditsOlderThan(1, regionsToSeqids);\n    junit.framework.Assert.assertEquals(1, regions.length);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(regions[0], \"0\".getBytes()));\n    regions = org.apache.hadoop.hbase.regionserver.wal.HLog.findMemstoresWithEditsOlderThan(3, regionsToSeqids);\n    int count = 3;\n    junit.framework.Assert.assertEquals(count, regions.length);\n    for (int i = 0; i < count; i++) {\n        junit.framework.Assert.assertTrue((org.apache.hadoop.hbase.util.Bytes.equals(regions[i], \"0\".getBytes()) || org.apache.hadoop.hbase.util.Bytes.equals(regions[i], \"1\".getBytes())) || org.apache.hadoop.hbase.util.Bytes.equals(regions[i], \"2\".getBytes()));\n    }\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testFindMemstoresWithEditsOlderThan",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    java.util.List<byte[]> regions = null;\n    for (java.util.Map.Entry<byte[], java.lang.Long> e : regionsToSeqids.entrySet()) {\n        if (e.getValue().longValue() < oldestWALseqid) {\n            if (regions == null)\n                regions = new java.util.ArrayList<byte[]>();\n\n            regions.add(e.getKey());\n        }\n    }\n    return regions == null ? null : regions.toArray(new byte[][]{ org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY });\n}",
            "ClassName": "HLog",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "findMemstoresWithEditsOlderThan",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    this.regions = regions;\n}",
            "ClassName": "TableInfoModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    this.regions = regions;\n}",
            "ClassName": "StorageClusterStatusModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return regions;\n}",
            "ClassName": "StorageClusterStatusModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHLog",
        "Commit": "96321df58284a75fcb0b38f989dc14c0bb3242b0",
        "CyclomaticComplexity": 2,
        "Date": "Tue, 18 May 2010 23:23:02 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testFindMemstoresWithEditsOlderThan",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 16,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274224982"
    },
    {
        "Body": "{\n    createStoreFile(r);\n    for (int i = 0; i < org.apache.hadoop.hbase.regionserver.TestCompaction.COMPACTION_THRESHOLD; i++) {\n        createStoreFile(r);\n    }\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(new org.apache.hadoop.hbase.client.Scan());\n    do {\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        boolean result = s.next(results);\n        r.delete(new org.apache.hadoop.hbase.client.Delete(results.get(0).getRow()), null, false);\n        if (!result)\n            break;\n\n    } while (true );\n    r.flushcache();\n    r.compactStores(true);\n    s = r.getScanner(new org.apache.hadoop.hbase.client.Scan());\n    int counter = 0;\n    do {\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        boolean result = s.next(results);\n        if (!result)\n            break;\n\n        counter++;\n    } while (true );\n    junit.framework.Assert.assertEquals(0, counter);\n}",
        "CUT_1": {
            "Body": "{\n    createStoreFile(r);\n    for (int i = 0; i < org.apache.hadoop.hbase.regionserver.TestCompaction.COMPACTION_THRESHOLD; i++) {\n        createStoreFile(r);\n    }\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(new org.apache.hadoop.hbase.client.Scan());\n    do {\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        boolean result = s.next(results);\n        r.delete(new org.apache.hadoop.hbase.client.Delete(results.get(0).getRow()), null, false);\n        if (!result)\n            break;\n\n    } while (true );\n    r.flushcache();\n    r.compactStores(true);\n    s = r.getScanner(new org.apache.hadoop.hbase.client.Scan());\n    int counter = 0;\n    do {\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        boolean result = s.next(results);\n        if (!result)\n            break;\n\n        counter++;\n    } while (true );\n    junit.framework.Assert.assertEquals(0, counter);\n}",
            "ClassName": "TestCompaction",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testMajorCompactingToNoOutput",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 28,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n    try {\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        boolean hasNext = true;\n        do {\n            hasNext = s.next(results);\n            org.apache.hadoop.hbase.HRegionInfo info = null;\n            for (org.apache.hadoop.hbase.KeyValue kv : results) {\n                info = org.apache.hadoop.hbase.util.Writables.getHRegionInfoOrNull(kv.getValue());\n                if (info == null) {\n                    org.apache.hadoop.hbase.util.MetaUtils.LOG.warn(((\"Region info is null for row \" + org.apache.hadoop.hbase.util.Bytes.toString(kv.getRow())) + \" in table \") + r.getTableDesc().getNameAsString());\n                }\n                continue;\n            }\n            if (!listener.processRow(info)) {\n                break;\n            }\n            results.clear();\n        } while (hasNext );\n    } finally {\n        s.close();\n    }\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "scanMetaRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 26,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ org.apache.hadoop.hbase.KeyValueTestUtil.create(\"R1\", \"cf\", \"a\", 2, org.apache.hadoop.hbase.KeyValue.Type.Put, \"dont-care\"), org.apache.hadoop.hbase.KeyValueTestUtil.create(\"R1\", \"cf\", \"a\", 1, org.apache.hadoop.hbase.KeyValue.Type.Put, \"dont-care\"), org.apache.hadoop.hbase.KeyValueTestUtil.create(\"R2\", \"cf\", \"a\", 1, org.apache.hadoop.hbase.KeyValue.Type.Put, \"dont-care\") };\n    java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner> scanners = org.apache.hadoop.hbase.regionserver.KeyValueScanFixture.scanFixture(kvs);\n    org.apache.hadoop.hbase.client.Scan scanSpec = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"R1\"));\n    org.apache.hadoop.hbase.regionserver.StoreScanner scan = new org.apache.hadoop.hbase.regionserver.StoreScanner(scanSpec, CF, java.lang.Long.MAX_VALUE, org.apache.hadoop.hbase.KeyValue.COMPARATOR, getCols(\"a\"), scanners);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    scan.next(results);\n    junit.framework.Assert.assertEquals(1, results.size());\n    junit.framework.Assert.assertEquals(kvs[0], results.get(0));\n    results.clear();\n    scan.next(results);\n    junit.framework.Assert.assertEquals(1, results.size());\n    junit.framework.Assert.assertEquals(kvs[2], results.get(0));\n    results.clear();\n    scan.next(results);\n    junit.framework.Assert.assertEquals(0, results.size());\n}",
            "ClassName": "TestStoreScanner",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testWontNextToNext",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 17,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n    boolean hasMore = true;\n    while (hasMore) {\n        hasMore = s.next(results);\n        for (org.apache.hadoop.hbase.KeyValue kv : results) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.compareTo(kv.getRow(), stopRow) <= 0);\n        }\n        results.clear();\n    } \n    s.close();\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "rowInclusiveStopFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 14,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ org.apache.hadoop.hbase.KeyValueTestUtil.create(\"R1\", \"cf\", \"a\", 1, org.apache.hadoop.hbase.KeyValue.Type.Put, \"dont-care\"), org.apache.hadoop.hbase.KeyValueTestUtil.create(\"R1\", \"cf\", \"a\", 1, org.apache.hadoop.hbase.KeyValue.Type.Delete, \"dont-care\"), org.apache.hadoop.hbase.KeyValueTestUtil.create(\"R2\", \"cf\", \"a\", 20, org.apache.hadoop.hbase.KeyValue.Type.Put, \"dont-care\") };\n    java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner> scanners = org.apache.hadoop.hbase.regionserver.KeyValueScanFixture.scanFixture(kvs);\n    org.apache.hadoop.hbase.client.Scan scanSpec = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"R1\"));\n    org.apache.hadoop.hbase.regionserver.StoreScanner scan = new org.apache.hadoop.hbase.regionserver.StoreScanner(scanSpec, CF, java.lang.Long.MAX_VALUE, org.apache.hadoop.hbase.KeyValue.COMPARATOR, getCols(\"a\"), scanners);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    junit.framework.Assert.assertEquals(true, scan.next(results));\n    junit.framework.Assert.assertEquals(0, results.size());\n    junit.framework.Assert.assertEquals(true, scan.next(results));\n    junit.framework.Assert.assertEquals(1, results.size());\n    junit.framework.Assert.assertEquals(kvs[2], results.get(0));\n    junit.framework.Assert.assertEquals(false, scan.next(results));\n}",
            "ClassName": "TestStoreScanner",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDeletedRowThenGoodRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 13,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestCompaction",
        "Commit": "528b2c2d32a9d2883eee0b21c71cd650fffe9cdc",
        "CyclomaticComplexity": 5,
        "Date": "Wed, 19 May 2010 18:28:05 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testMajorCompactingToNoOutput",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 28,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274293685"
    },
    {
        "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] fam = org.apache.hadoop.hbase.util.Bytes.toBytes(\"info\");\n    byte[][] families = new byte[][]{ fam };\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, families);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"table_name\");\n    byte[] serverinfo = org.apache.hadoop.hbase.util.Bytes.toBytes(\"serverinfo\");\n    byte[] splitA = org.apache.hadoop.hbase.util.Bytes.toBytes(\"splitA\");\n    byte[] splitB = org.apache.hadoop.hbase.util.Bytes.toBytes(\"splitB\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(fam, splitA, org.apache.hadoop.hbase.util.Bytes.toBytes(\"reference_A\"));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(fam, splitB, org.apache.hadoop.hbase.util.Bytes.toBytes(\"reference_B\"));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(fam, serverinfo, org.apache.hadoop.hbase.util.Bytes.toBytes(\"ip_address\"));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(row);\n    delete.deleteColumns(fam, splitA);\n    region.delete(delete, null, true);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row).addColumn(fam, serverinfo);\n    org.apache.hadoop.hbase.client.Result result = region.get(get, null);\n    junit.framework.Assert.assertEquals(1, result.size());\n    get = new org.apache.hadoop.hbase.client.Get(row).addColumn(fam, splitA);\n    result = region.get(get, null);\n    junit.framework.Assert.assertEquals(0, result.size());\n    get = new org.apache.hadoop.hbase.client.Get(row).addColumn(fam, splitB);\n    result = region.get(get, null);\n    junit.framework.Assert.assertEquals(1, result.size());\n    put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(fam, splitA, org.apache.hadoop.hbase.util.Bytes.toBytes(\"reference_A\"));\n    region.put(put);\n    get = new org.apache.hadoop.hbase.client.Get(row);\n    result = region.get(get, null);\n    junit.framework.Assert.assertEquals(3, result.size());\n    delete = new org.apache.hadoop.hbase.client.Delete(row);\n    region.delete(delete, null, false);\n    junit.framework.Assert.assertEquals(0, region.get(get, null).size());\n    try {\n        java.lang.Thread.sleep(10);\n    } catch (java.lang.InterruptedException e) {\n        e.printStackTrace();\n    }\n    region.put(new org.apache.hadoop.hbase.client.Put(row).add(fam, splitA, org.apache.hadoop.hbase.util.Bytes.toBytes(\"reference_A\")));\n    result = region.get(get, null);\n    junit.framework.Assert.assertEquals(1, result.size());\n}",
        "CUT_1": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] fam = org.apache.hadoop.hbase.util.Bytes.toBytes(\"info\");\n    byte[][] families = new byte[][]{ fam };\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, families);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"table_name\");\n    byte[] serverinfo = org.apache.hadoop.hbase.util.Bytes.toBytes(\"serverinfo\");\n    byte[] splitA = org.apache.hadoop.hbase.util.Bytes.toBytes(\"splitA\");\n    byte[] splitB = org.apache.hadoop.hbase.util.Bytes.toBytes(\"splitB\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(fam, splitA, org.apache.hadoop.hbase.util.Bytes.toBytes(\"reference_A\"));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(fam, splitB, org.apache.hadoop.hbase.util.Bytes.toBytes(\"reference_B\"));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(fam, serverinfo, org.apache.hadoop.hbase.util.Bytes.toBytes(\"ip_address\"));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(row);\n    delete.deleteColumns(fam, splitA);\n    region.delete(delete, null, true);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row).addColumn(fam, serverinfo);\n    org.apache.hadoop.hbase.client.Result result = region.get(get, null);\n    junit.framework.Assert.assertEquals(1, result.size());\n    get = new org.apache.hadoop.hbase.client.Get(row).addColumn(fam, splitA);\n    result = region.get(get, null);\n    junit.framework.Assert.assertEquals(0, result.size());\n    get = new org.apache.hadoop.hbase.client.Get(row).addColumn(fam, splitB);\n    result = region.get(get, null);\n    junit.framework.Assert.assertEquals(1, result.size());\n    put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(fam, splitA, org.apache.hadoop.hbase.util.Bytes.toBytes(\"reference_A\"));\n    region.put(put);\n    get = new org.apache.hadoop.hbase.client.Get(row);\n    result = region.get(get, null);\n    junit.framework.Assert.assertEquals(3, result.size());\n    delete = new org.apache.hadoop.hbase.client.Delete(row);\n    region.delete(delete, null, false);\n    junit.framework.Assert.assertEquals(0, region.get(get, null).size());\n    try {\n        java.lang.Thread.sleep(10);\n    } catch (java.lang.InterruptedException e) {\n        e.printStackTrace();\n    }\n    region.put(new org.apache.hadoop.hbase.client.Put(row).add(fam, splitA, org.apache.hadoop.hbase.util.Bytes.toBytes(\"reference_A\")));\n    result = region.get(get, null);\n    junit.framework.Assert.assertEquals(1, result.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDelete_mixed",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 49,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] fam = org.apache.hadoop.hbase.util.Bytes.toBytes(\"info\");\n    byte[][] families = new byte[][]{ fam };\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, families);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"table_name\");\n    byte[] serverinfo = org.apache.hadoop.hbase.util.Bytes.toBytes(\"serverinfo\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(fam, serverinfo, org.apache.hadoop.hbase.HConstants.LATEST_TIMESTAMP - 5, org.apache.hadoop.hbase.util.Bytes.toBytes(\"value\"));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(row);\n    region.delete(delete, null, true);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row).addColumn(fam, serverinfo);\n    org.apache.hadoop.hbase.client.Result result = region.get(get, null);\n    junit.framework.Assert.assertEquals(1, result.size());\n    delete = new org.apache.hadoop.hbase.client.Delete(row, org.apache.hadoop.hbase.HConstants.LATEST_TIMESTAMP - 3, null);\n    region.delete(delete, null, true);\n    get = new org.apache.hadoop.hbase.client.Get(row).addColumn(fam, serverinfo);\n    result = region.get(get, null);\n    junit.framework.Assert.assertEquals(0, result.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDeleteRowWithFutureTs",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"emptytable\");\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\");\n    byte[] fam = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\");\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, fam);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addFamily(fam);\n    org.apache.hadoop.hbase.client.Result r = region.get(get, null);\n    junit.framework.Assert.assertTrue(r.isEmpty());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGet_Empty",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 11,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"test_table\");\n    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testFamily\");\n    initHRegion(tableName, getName(), family);\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row111\");\n    byte[] row2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row222\");\n    byte[] row3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row333\");\n    byte[] row4 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row444\");\n    byte[] row5 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row555\");\n    byte[] col1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub111\");\n    byte[] col2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub222\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(10L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row2);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(15L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row3);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(20L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row4);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(30L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row5);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(40L));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row3, row4);\n    scan.setMaxVersions();\n    scan.addColumn(family, col1);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    junit.framework.Assert.assertEquals(false, s.next(results));\n    junit.framework.Assert.assertEquals(0, results.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_StopRow1542",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 34,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    int count = 0;\n    for (char c = 'a'; c <= 'c'; c++) {\n        byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"ab\" + c);\n        int i;\n        for (i = 0; i < 2500; i++) {\n            byte[] b = org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.String.format(\"%10d\", i));\n            org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n            put.add(family, b, b);\n            region.put(put);\n            count++;\n        }\n    }\n    {\n        byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"abf\");\n        int i;\n        for (i = 0; i < 100000; i++) {\n            byte[] b = org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.String.format(\"%10d\", i));\n            org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n            put.add(family, b, b);\n            region.put(put);\n            count++;\n        }\n    }\n    return count;\n}",
            "ClassName": "TestWideScanner",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addWideContent",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 26,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHRegion",
        "Commit": "528b2c2d32a9d2883eee0b21c71cd650fffe9cdc",
        "CyclomaticComplexity": 0,
        "Date": "Wed, 19 May 2010 18:28:05 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testDelete_mixed",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 1,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 49,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 2,
        "ProjectName": "hbase",
        "Timestamp": "1274293685"
    },
    {
        "Body": "{\n    junit.framework.Assert.assertFalse(remoteAdmin.isTableAvailable(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.TABLE_1));\n    remoteAdmin.createTable(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.DESC_1);\n    junit.framework.Assert.assertTrue(remoteAdmin.isTableAvailable(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.TABLE_1));\n}",
        "CUT_1": {
            "Body": "{\n    junit.framework.Assert.assertFalse(remoteAdmin.isTableAvailable(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.TABLE_1));\n    remoteAdmin.createTable(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.DESC_1);\n    junit.framework.Assert.assertTrue(remoteAdmin.isTableAvailable(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.TABLE_1));\n}",
            "ClassName": "TestRemoteAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testCreateTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    junit.framework.Assert.assertTrue(remoteAdmin.isTableAvailable(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.TABLE_2));\n    remoteAdmin.deleteTable(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.TABLE_2);\n    junit.framework.Assert.assertFalse(remoteAdmin.isTableAvailable(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.TABLE_2));\n}",
            "ClassName": "TestRemoteAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDeleteTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    super.setUp();\n    localAdmin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    remoteAdmin = new org.apache.hadoop.hbase.rest.client.RemoteAdmin(new org.apache.hadoop.hbase.rest.client.Client(new org.apache.hadoop.hbase.rest.client.Cluster().add(\"localhost\", testServletPort)), conf);\n    if (localAdmin.tableExists(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.TABLE_1)) {\n        localAdmin.disableTable(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.TABLE_1);\n        localAdmin.deleteTable(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.TABLE_1);\n    }\n    if (!localAdmin.tableExists(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.TABLE_2)) {\n        localAdmin.createTable(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.DESC_2);\n    }\n}",
            "ClassName": "TestRemoteAdmin",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor tableDescriptor = getTableDescriptor();\n    org.apache.hadoop.hbase.rest.client.RemoteAdmin admin = new org.apache.hadoop.hbase.rest.client.RemoteAdmin(new org.apache.hadoop.hbase.rest.client.Client(org.apache.hadoop.hbase.rest.PerformanceEvaluation.cluster), conf, org.apache.hadoop.hbase.rest.PerformanceEvaluation.accessToken);\n    if (!admin.isTableAvailable(tableDescriptor.getName())) {\n        admin.createTable(tableDescriptor);\n        return true;\n    }\n    return false;\n}",
            "ClassName": "PerformanceEvaluation",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "checkTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return isTableAvailable(org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n}",
            "ClassName": "RemoteAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "isTableAvailable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestRemoteAdmin",
        "Commit": "528b2c2d32a9d2883eee0b21c71cd650fffe9cdc",
        "CyclomaticComplexity": 0,
        "Date": "Wed, 19 May 2010 18:28:05 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testCreateTable",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 5,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274293685"
    },
    {
        "Body": "{\n    try {\n        this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n        org.apache.hadoop.hbase.HBaseTestCase.addContent(this.r, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        try {\n            s.next(results);\n            s.close();\n            s.next(results);\n            junit.framework.Assert.fail(\"We don't want anything more, we should be failing\");\n        } catch (org.apache.hadoop.hbase.UnknownScannerException ex) {\n            return;\n        }\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(this.cluster);\n    }\n}",
        "CUT_1": {
            "Body": "{\n    try {\n        this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n        org.apache.hadoop.hbase.HBaseTestCase.addContent(this.r, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        try {\n            s.next(results);\n            s.close();\n            s.next(results);\n            junit.framework.Assert.fail(\"We don't want anything more, we should be failing\");\n        } catch (org.apache.hadoop.hbase.UnknownScannerException ex) {\n            return;\n        }\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(this.cluster);\n    }\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testRaceBetweenClientAndTimeout",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    byte[] startrow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"bbb\");\n    byte[] stoprow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"ccc\");\n    try {\n        this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n        org.apache.hadoop.hbase.HBaseTestCase.addContent(this.r, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"abc\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"abd\"));\n        scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n        int count = 0;\n        while (s.next(results)) {\n            count++;\n        } \n        s.close();\n        junit.framework.Assert.assertEquals(0, count);\n        scan = new org.apache.hadoop.hbase.client.Scan(startrow, stoprow);\n        scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        s = r.getScanner(scan);\n        count = 0;\n        org.apache.hadoop.hbase.KeyValue kv = null;\n        results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        for (boolean first = true; s.next(results);) {\n            kv = results.get(0);\n            if (first) {\n                junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.BYTES_COMPARATOR.compare(startrow, kv.getRow()) == 0);\n                first = false;\n            }\n            count++;\n        }\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.BYTES_COMPARATOR.compare(stoprow, kv.getRow()) > 0);\n        junit.framework.Assert.assertTrue(count > 10);\n        s.close();\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(this.cluster);\n    }\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testStopRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 39,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    try {\n        this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestWideScanner.REGION_INFO.getTableDesc(), null, null);\n        int inserted = addWideContent(this.r, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        scan.setBatch(org.apache.hadoop.hbase.regionserver.TestWideScanner.BATCH);\n        org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n        int total = 0;\n        int i = 0;\n        boolean more;\n        do {\n            more = s.next(results);\n            i++;\n            LOG.info(((\"iteration #\" + i) + \", results.size=\") + results.size());\n            junit.framework.Assert.assertTrue(results.size() <= org.apache.hadoop.hbase.regionserver.TestWideScanner.BATCH);\n            total += results.size();\n            if (results.size() > 0) {\n                byte[] row = results.get(0).getRow();\n                for (org.apache.hadoop.hbase.KeyValue kv : results) {\n                    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(row, kv.getRow()));\n                }\n            }\n            results.clear();\n        } while (more );\n        LOG.info(((\"inserted \" + inserted) + \", scanned \") + total);\n        junit.framework.Assert.assertTrue(total == inserted);\n        s.close();\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(this.cluster);\n    }\n}",
            "ClassName": "TestWideScanner",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testWideScanBatching",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 35,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n    try {\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        boolean hasNext = true;\n        do {\n            hasNext = s.next(results);\n            org.apache.hadoop.hbase.HRegionInfo info = null;\n            for (org.apache.hadoop.hbase.KeyValue kv : results) {\n                info = org.apache.hadoop.hbase.util.Writables.getHRegionInfoOrNull(kv.getValue());\n                if (info == null) {\n                    org.apache.hadoop.hbase.util.MetaUtils.LOG.warn(((\"Region info is null for row \" + org.apache.hadoop.hbase.util.Bytes.toString(kv.getRow())) + \" in table \") + r.getTableDesc().getNameAsString());\n                }\n                continue;\n            }\n            if (!listener.processRow(info)) {\n                break;\n            }\n            results.clear();\n        } while (hasNext );\n    } finally {\n        s.close();\n    }\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "scanMetaRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 26,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    try {\n        this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n        org.apache.hadoop.hbase.HBaseTestCase.addContent(this.r, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        byte[] prefix = org.apache.hadoop.hbase.util.Bytes.toBytes(\"ab\");\n        org.apache.hadoop.hbase.filter.Filter newFilter = new org.apache.hadoop.hbase.filter.PrefixFilter(prefix);\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.setFilter(newFilter);\n        rowPrefixFilter(scan);\n        byte[] stopRow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"bbc\");\n        newFilter = new org.apache.hadoop.hbase.filter.WhileMatchFilter(new org.apache.hadoop.hbase.filter.InclusiveStopFilter(stopRow));\n        scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.setFilter(newFilter);\n        rowInclusiveStopFilter(scan, stopRow);\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(this.cluster);\n    }\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testFilters",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 20,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestScanner",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testRaceBetweenClientAndTimeout",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 21,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] startrow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"bbb\");\n    byte[] stoprow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"ccc\");\n    try {\n        this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n        org.apache.hadoop.hbase.HBaseTestCase.addContent(this.r, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"abc\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"abd\"));\n        scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n        int count = 0;\n        while (s.next(results)) {\n            count++;\n        } \n        s.close();\n        junit.framework.Assert.assertEquals(0, count);\n        scan = new org.apache.hadoop.hbase.client.Scan(startrow, stoprow);\n        scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        s = r.getScanner(scan);\n        count = 0;\n        org.apache.hadoop.hbase.KeyValue kv = null;\n        results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        for (boolean first = true; s.next(results);) {\n            kv = results.get(0);\n            if (first) {\n                junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.BYTES_COMPARATOR.compare(startrow, kv.getRow()) == 0);\n                first = false;\n            }\n            count++;\n        }\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.BYTES_COMPARATOR.compare(stoprow, kv.getRow()) > 0);\n        junit.framework.Assert.assertTrue(count > 10);\n        s.close();\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(this.cluster);\n    }\n}",
        "CUT_1": {
            "Body": "{\n    byte[] startrow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"bbb\");\n    byte[] stoprow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"ccc\");\n    try {\n        this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n        org.apache.hadoop.hbase.HBaseTestCase.addContent(this.r, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"abc\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"abd\"));\n        scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n        int count = 0;\n        while (s.next(results)) {\n            count++;\n        } \n        s.close();\n        junit.framework.Assert.assertEquals(0, count);\n        scan = new org.apache.hadoop.hbase.client.Scan(startrow, stoprow);\n        scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        s = r.getScanner(scan);\n        count = 0;\n        org.apache.hadoop.hbase.KeyValue kv = null;\n        results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        for (boolean first = true; s.next(results);) {\n            kv = results.get(0);\n            if (first) {\n                junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.BYTES_COMPARATOR.compare(startrow, kv.getRow()) == 0);\n                first = false;\n            }\n            count++;\n        }\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.BYTES_COMPARATOR.compare(stoprow, kv.getRow()) > 0);\n        junit.framework.Assert.assertTrue(count > 10);\n        s.close();\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(this.cluster);\n    }\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testStopRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 39,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    try {\n        this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n        org.apache.hadoop.hbase.HBaseTestCase.addContent(this.r, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        try {\n            s.next(results);\n            s.close();\n            s.next(results);\n            junit.framework.Assert.fail(\"We don't want anything more, we should be failing\");\n        } catch (org.apache.hadoop.hbase.UnknownScannerException ex) {\n            return;\n        }\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(this.cluster);\n    }\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testRaceBetweenClientAndTimeout",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n    try {\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        boolean hasNext = true;\n        do {\n            hasNext = s.next(results);\n            org.apache.hadoop.hbase.HRegionInfo info = null;\n            for (org.apache.hadoop.hbase.KeyValue kv : results) {\n                info = org.apache.hadoop.hbase.util.Writables.getHRegionInfoOrNull(kv.getValue());\n                if (info == null) {\n                    org.apache.hadoop.hbase.util.MetaUtils.LOG.warn(((\"Region info is null for row \" + org.apache.hadoop.hbase.util.Bytes.toString(kv.getRow())) + \" in table \") + r.getTableDesc().getNameAsString());\n                }\n                continue;\n            }\n            if (!listener.processRow(info)) {\n                break;\n            }\n            results.clear();\n        } while (hasNext );\n    } finally {\n        s.close();\n    }\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "scanMetaRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 26,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    final int COUNT = 5;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(getName());\n    for (int i = 0; i < COUNT; i++) {\n        byte[] regionName = org.apache.hadoop.hbase.HRegionInfo.createRegionName(tableName, org.apache.hadoop.hbase.util.Bytes.toBytes(i == 0 ? \"\" : java.lang.Integer.toString(i)), java.lang.Long.toString(java.lang.System.currentTimeMillis()));\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(regionName);\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER, org.apache.hadoop.hbase.util.Bytes.toBytes(\"localhost:1234\"));\n        t.put(put);\n    }\n    long sleepTime = conf.getLong(\"hbase.master.meta.thread.rescanfrequency\", 10000);\n    int tries = conf.getInt(\"hbase.client.retries.number\", 5);\n    int count = 0;\n    do {\n        tries -= 1;\n        try {\n            java.lang.Thread.sleep(sleepTime);\n        } catch (java.lang.InterruptedException e) {\n        }\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITA_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITB_QUALIFIER);\n        org.apache.hadoop.hbase.client.ResultScanner scanner = t.getScanner(scan);\n        try {\n            count = 0;\n            org.apache.hadoop.hbase.client.Result r;\n            while ((r = scanner.next()) != null) {\n                if (!r.isEmpty()) {\n                    count += 1;\n                }\n            } \n        } finally {\n            scanner.close();\n        }\n    } while ((count != 0) && (tries >= 0) );\n    junit.framework.Assert.assertTrue(tries >= 0);\n    junit.framework.Assert.assertEquals(0, count);\n}",
            "ClassName": "TestEmptyMetaInfo",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testEmptyMetaInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 41,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_5": {
            "Body": "{\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n    boolean hasMore = true;\n    while (hasMore) {\n        hasMore = s.next(results);\n        for (org.apache.hadoop.hbase.KeyValue kv : results) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.compareTo(kv.getRow(), stopRow) <= 0);\n        }\n        results.clear();\n    } \n    s.close();\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "rowInclusiveStopFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 14,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestScanner",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 3,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testStopRow",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 39,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    final byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\");\n    final byte[] qualifier = org.apache.hadoop.hbase.util.Bytes.toBytes(\"qualifier\");\n    final byte[] value = org.apache.hadoop.hbase.util.Bytes.toBytes(\"value\");\n    final byte[] table = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testDisableAndEnableTable\");\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.createTable(table, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, qualifier, value);\n    ht.put(put);\n    this.admin.disableTable(table);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, qualifier);\n    boolean ok = false;\n    try {\n        ht.get(get);\n    } catch (org.apache.hadoop.hbase.client.RetriesExhaustedException e) {\n        ok = true;\n    }\n    org.junit.Assert.assertEquals(true, ok);\n    this.admin.enableTable(table);\n    try {\n        ht.get(get);\n    } catch (org.apache.hadoop.hbase.client.RetriesExhaustedException e) {\n        ok = false;\n    }\n    org.junit.Assert.assertEquals(true, ok);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(c, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.Result res = t.get(get);\n    org.apache.hadoop.hbase.KeyValue[] kvs = res.raw();\n    if (kvs.length <= 0) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    byte[] value = kvs[0].getValue();\n    if (value == null) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(value);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    info.setOffline(onlineOffline);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(info));\n    t.put(put);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(row);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER);\n    t.delete(delete);\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "changeOnlineStatus",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    this.server = cluster.getRegionServerThreads().get(0).getRegionServer();\n    this.log = server.getLog();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    for (int i = 1; i <= 256; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + java.lang.String.format(\"%1$04d\", i)));\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, value);\n        table.put(put);\n        if ((i % 32) == 0) {\n            try {\n                java.lang.Thread.sleep(2000);\n            } catch (java.lang.InterruptedException e) {\n            }\n        }\n    }\n}",
            "ClassName": "TestLogRolling",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startAndWriteData",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    if (org.apache.hadoop.hbase.util.MetaUtils.LOG.isDebugEnabled()) {\n        org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(hri.getRegionName());\n        get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        org.apache.hadoop.hbase.client.Result res = r.get(get, null);\n        org.apache.hadoop.hbase.KeyValue[] kvs = res.raw();\n        if (kvs.length <= 0) {\n            return;\n        }\n        byte[] value = kvs[0].getValue();\n        if (value == null) {\n            return;\n        }\n        org.apache.hadoop.hbase.HRegionInfo h = org.apache.hadoop.hbase.util.Writables.getHRegionInfoOrNull(value);\n        org.apache.hadoop.hbase.util.MetaUtils.LOG.debug(((((((((\"Old \" + org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY)) + \":\") + org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER)) + \" for \") + hri.toString()) + \" in \") + r.toString()) + \" is: \") + h.toString());\n    }\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(hri.getRegionName());\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(hri));\n    r.put(put);\n    if (org.apache.hadoop.hbase.util.MetaUtils.LOG.isDebugEnabled()) {\n        org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(hri.getRegionName());\n        get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        org.apache.hadoop.hbase.client.Result res = r.get(get, null);\n        org.apache.hadoop.hbase.KeyValue[] kvs = res.raw();\n        if (kvs.length <= 0) {\n            return;\n        }\n        byte[] value = kvs[0].getValue();\n        if (value == null) {\n            return;\n        }\n        org.apache.hadoop.hbase.HRegionInfo h = org.apache.hadoop.hbase.util.Writables.getHRegionInfoOrNull(value);\n        org.apache.hadoop.hbase.util.MetaUtils.LOG.debug(((((((((\"New \" + org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY)) + \":\") + org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER)) + \" for \") + hri.toString()) + \" in \") + r.toString()) + \" is: \") + h.toString());\n    }\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 6,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "updateMETARegionInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 35,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testDisableAndEnableTable",
        "NumberOfAsserts": 2,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 27,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    try {\n        this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n        org.apache.hadoop.hbase.HBaseTestCase.addContent(this.r, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        byte[] prefix = org.apache.hadoop.hbase.util.Bytes.toBytes(\"ab\");\n        org.apache.hadoop.hbase.filter.Filter newFilter = new org.apache.hadoop.hbase.filter.PrefixFilter(prefix);\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.setFilter(newFilter);\n        rowPrefixFilter(scan);\n        byte[] stopRow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"bbc\");\n        newFilter = new org.apache.hadoop.hbase.filter.WhileMatchFilter(new org.apache.hadoop.hbase.filter.InclusiveStopFilter(stopRow));\n        scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.setFilter(newFilter);\n        rowInclusiveStopFilter(scan, stopRow);\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(this.cluster);\n    }\n}",
        "CUT_1": {
            "Body": "{\n    try {\n        this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n        org.apache.hadoop.hbase.HBaseTestCase.addContent(this.r, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        byte[] prefix = org.apache.hadoop.hbase.util.Bytes.toBytes(\"ab\");\n        org.apache.hadoop.hbase.filter.Filter newFilter = new org.apache.hadoop.hbase.filter.PrefixFilter(prefix);\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.setFilter(newFilter);\n        rowPrefixFilter(scan);\n        byte[] stopRow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"bbc\");\n        newFilter = new org.apache.hadoop.hbase.filter.WhileMatchFilter(new org.apache.hadoop.hbase.filter.InclusiveStopFilter(stopRow));\n        scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.setFilter(newFilter);\n        rowInclusiveStopFilter(scan, stopRow);\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(this.cluster);\n    }\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testFilters",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 20,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    byte[] startrow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"bbb\");\n    byte[] stoprow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"ccc\");\n    try {\n        this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n        org.apache.hadoop.hbase.HBaseTestCase.addContent(this.r, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"abc\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"abd\"));\n        scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n        int count = 0;\n        while (s.next(results)) {\n            count++;\n        } \n        s.close();\n        junit.framework.Assert.assertEquals(0, count);\n        scan = new org.apache.hadoop.hbase.client.Scan(startrow, stoprow);\n        scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        s = r.getScanner(scan);\n        count = 0;\n        org.apache.hadoop.hbase.KeyValue kv = null;\n        results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        for (boolean first = true; s.next(results);) {\n            kv = results.get(0);\n            if (first) {\n                junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.BYTES_COMPARATOR.compare(startrow, kv.getRow()) == 0);\n                first = false;\n            }\n            count++;\n        }\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.BYTES_COMPARATOR.compare(stoprow, kv.getRow()) > 0);\n        junit.framework.Assert.assertTrue(count > 10);\n        s.close();\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(this.cluster);\n    }\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testStopRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 39,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.filter.Filter filter = new org.apache.hadoop.hbase.filter.DependentColumnFilter(org.apache.hadoop.hbase.filter.TestDependentColumnFilter.FAMILIES[0], org.apache.hadoop.hbase.filter.TestDependentColumnFilter.QUALIFIER);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.setFilter(filter);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    verifyScan(scan, 2, 8);\n    filter = new org.apache.hadoop.hbase.filter.DependentColumnFilter(org.apache.hadoop.hbase.filter.TestDependentColumnFilter.FAMILIES[0], org.apache.hadoop.hbase.filter.TestDependentColumnFilter.QUALIFIER, true);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.setFilter(filter);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    verifyScan(scan, 2, 3);\n    filter = new org.apache.hadoop.hbase.filter.DependentColumnFilter(org.apache.hadoop.hbase.filter.TestDependentColumnFilter.FAMILIES[0], org.apache.hadoop.hbase.filter.TestDependentColumnFilter.QUALIFIER, false, org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.filter.TestDependentColumnFilter.MATCH_VAL));\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.setFilter(filter);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    verifyScan(scan, 2, 3);\n    filter = new org.apache.hadoop.hbase.filter.DependentColumnFilter(org.apache.hadoop.hbase.filter.TestDependentColumnFilter.FAMILIES[0], org.apache.hadoop.hbase.filter.TestDependentColumnFilter.QUALIFIER, true, org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.filter.TestDependentColumnFilter.MATCH_VAL));\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.setFilter(filter);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    verifyScan(scan, 1, 1);\n}",
            "ClassName": "TestDependentColumnFilter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScans",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    try {\n        this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n        org.apache.hadoop.hbase.HBaseTestCase.addContent(this.r, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        try {\n            s.next(results);\n            s.close();\n            s.next(results);\n            junit.framework.Assert.fail(\"We don't want anything more, we should be failing\");\n        } catch (org.apache.hadoop.hbase.UnknownScannerException ex) {\n            return;\n        }\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(this.cluster);\n    }\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testRaceBetweenClientAndTimeout",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    if (this.scan == null)\n        this.scan = new org.apache.hadoop.hbase.client.Scan();\n\n    return scan;\n}",
            "ClassName": "TableInputFormatBase",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getScan",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestScanner",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testFilters",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 20,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    junit.framework.Assert.assertTrue(\"This test requires HLog file replication.\", fs.getDefaultReplication() > 1);\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    this.server = cluster.getRegionServer(0);\n    this.log = server.getLog();\n    junit.framework.Assert.assertTrue(\"Need HDFS-826 for this test\", log.canGetCurReplicas());\n    junit.framework.Assert.assertTrue(\"Need append support for this test\", org.apache.hadoop.hbase.regionserver.wal.HLog.isAppend(conf));\n    dfsCluster.startDataNodes(conf, 1, true, null, null);\n    dfsCluster.waitActive();\n    junit.framework.Assert.assertTrue(dfsCluster.getDataNodes().size() >= (fs.getDefaultReplication() + 1));\n    java.lang.String tableName = getName();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    table.setAutoFlush(true);\n    long curTime = java.lang.System.currentTimeMillis();\n    long oldFilenum = log.getFilenum();\n    junit.framework.Assert.assertTrue(\"Log should have a timestamp older than now\", (curTime > oldFilenum) && (oldFilenum != (-1)));\n    writeData(table, 1);\n    junit.framework.Assert.assertTrue(\"The log shouldn't have rolled yet\", oldFilenum == log.getFilenum());\n    java.io.OutputStream stm = log.getOutputStream();\n    java.lang.reflect.Method getPipeline = null;\n    for (java.lang.reflect.Method m : stm.getClass().getDeclaredMethods()) {\n        if (m.getName().endsWith(\"getPipeline\")) {\n            getPipeline = m;\n            getPipeline.setAccessible(true);\n            break;\n        }\n    }\n    junit.framework.Assert.assertTrue(\"Need DFSOutputStream.getPipeline() for this test\", getPipeline != null);\n    java.lang.Object repl = getPipeline.invoke(stm, new java.lang.Object[]{  });\n    org.apache.hadoop.hdfs.protocol.DatanodeInfo[] pipeline = ((org.apache.hadoop.hdfs.protocol.DatanodeInfo[]) (repl));\n    junit.framework.Assert.assertTrue(pipeline.length == fs.getDefaultReplication());\n    org.apache.hadoop.hdfs.MiniDFSCluster.DataNodeProperties dnprop = dfsCluster.stopDataNode(pipeline[0].getName());\n    junit.framework.Assert.assertTrue(dnprop != null);\n    writeData(table, 2);\n    long newFilenum = log.getFilenum();\n    junit.framework.Assert.assertTrue(\"Missing datanode should've triggered a log roll\", (newFilenum > oldFilenum) && (newFilenum > curTime));\n    writeData(table, 3);\n    junit.framework.Assert.assertTrue(\"The log should not roll again.\", log.getFilenum() == newFilenum);\n    junit.framework.Assert.assertTrue(\"New log file should have the default replication\", log.getLogReplication() == fs.getDefaultReplication());\n}",
        "CUT_1": {
            "Body": "{\n    junit.framework.Assert.assertTrue(\"This test requires HLog file replication.\", fs.getDefaultReplication() > 1);\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    this.server = cluster.getRegionServer(0);\n    this.log = server.getLog();\n    junit.framework.Assert.assertTrue(\"Need HDFS-826 for this test\", log.canGetCurReplicas());\n    junit.framework.Assert.assertTrue(\"Need append support for this test\", org.apache.hadoop.hbase.regionserver.wal.HLog.isAppend(conf));\n    dfsCluster.startDataNodes(conf, 1, true, null, null);\n    dfsCluster.waitActive();\n    junit.framework.Assert.assertTrue(dfsCluster.getDataNodes().size() >= (fs.getDefaultReplication() + 1));\n    java.lang.String tableName = getName();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    table.setAutoFlush(true);\n    long curTime = java.lang.System.currentTimeMillis();\n    long oldFilenum = log.getFilenum();\n    junit.framework.Assert.assertTrue(\"Log should have a timestamp older than now\", (curTime > oldFilenum) && (oldFilenum != (-1)));\n    writeData(table, 1);\n    junit.framework.Assert.assertTrue(\"The log shouldn't have rolled yet\", oldFilenum == log.getFilenum());\n    java.io.OutputStream stm = log.getOutputStream();\n    java.lang.reflect.Method getPipeline = null;\n    for (java.lang.reflect.Method m : stm.getClass().getDeclaredMethods()) {\n        if (m.getName().endsWith(\"getPipeline\")) {\n            getPipeline = m;\n            getPipeline.setAccessible(true);\n            break;\n        }\n    }\n    junit.framework.Assert.assertTrue(\"Need DFSOutputStream.getPipeline() for this test\", getPipeline != null);\n    java.lang.Object repl = getPipeline.invoke(stm, new java.lang.Object[]{  });\n    org.apache.hadoop.hdfs.protocol.DatanodeInfo[] pipeline = ((org.apache.hadoop.hdfs.protocol.DatanodeInfo[]) (repl));\n    junit.framework.Assert.assertTrue(pipeline.length == fs.getDefaultReplication());\n    org.apache.hadoop.hdfs.MiniDFSCluster.DataNodeProperties dnprop = dfsCluster.stopDataNode(pipeline[0].getName());\n    junit.framework.Assert.assertTrue(dnprop != null);\n    writeData(table, 2);\n    long newFilenum = log.getFilenum();\n    junit.framework.Assert.assertTrue(\"Missing datanode should've triggered a log roll\", (newFilenum > oldFilenum) && (newFilenum > curTime));\n    writeData(table, 3);\n    junit.framework.Assert.assertTrue(\"The log should not roll again.\", log.getFilenum() == newFilenum);\n    junit.framework.Assert.assertTrue(\"New log file should have the default replication\", log.getLogReplication() == fs.getDefaultReplication());\n}",
            "ClassName": "TestLogRolling",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testLogRollOnDatanodeDeath",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 44,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    final int COL_COUNT = 10;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"tablename\");\n    final byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\");\n    this.conf.setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = null;\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(this.fs, dir, this.oldLogDir, this.conf, null);\n    try {\n        long timestamp = java.lang.System.currentTimeMillis();\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit cols = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        for (int i = 0; i < COL_COUNT; i++) {\n            cols.add(new org.apache.hadoop.hbase.KeyValue(row, org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\"), org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(i)), timestamp, new byte[]{ ((byte) (i + '0')) }));\n        }\n        org.apache.hadoop.hbase.HRegionInfo hri = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(tableName), org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.HConstants.EMPTY_END_ROW);\n        log.append(hri, tableName, cols, java.lang.System.currentTimeMillis());\n        long logSeqId = log.startCacheFlush();\n        log.completeCacheFlush(hri.getRegionName(), tableName, logSeqId, false);\n        log.close();\n        org.apache.hadoop.fs.Path filename = log.computeFilename(log.getFilenum());\n        log = null;\n        reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, filename, conf);\n        org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = reader.next();\n        junit.framework.Assert.assertEquals(COL_COUNT, entry.getEdit().size());\n        int idx = 0;\n        for (org.apache.hadoop.hbase.KeyValue val : entry.getEdit().getKeyValues()) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getRegionName(), entry.getKey().getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, entry.getKey().getTablename()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(row, val.getRow()));\n            junit.framework.Assert.assertEquals(((byte) (idx + '0')), val.getValue()[0]);\n            java.lang.System.out.println((entry.getKey() + \" \") + val);\n            idx++;\n        }\n        entry = reader.next();\n        junit.framework.Assert.assertEquals(1, entry.getEdit().size());\n        for (org.apache.hadoop.hbase.KeyValue val : entry.getEdit().getKeyValues()) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getRegionName(), entry.getKey().getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, entry.getKey().getTablename()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, val.getRow()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, val.getFamily()));\n            junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH, val.getValue()));\n            java.lang.System.out.println((entry.getKey() + \" \") + val);\n        }\n    } finally {\n        if (log != null) {\n            log.closeAndDelete();\n        }\n        if (reader != null) {\n            reader.close();\n        }\n    }\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testAppend",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 51,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    final int COL_COUNT = 10;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"tablename\");\n    final byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\");\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = null;\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(fs, dir, this.oldLogDir, this.conf, null);\n    try {\n        long timestamp = java.lang.System.currentTimeMillis();\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit cols = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        for (int i = 0; i < COL_COUNT; i++) {\n            cols.add(new org.apache.hadoop.hbase.KeyValue(row, org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\"), org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(i)), timestamp, new byte[]{ ((byte) (i + '0')) }));\n        }\n        org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(tableName), row, org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.util.Bytes.toString(row) + \"1\"), false);\n        final byte[] regionName = info.getRegionName();\n        log.append(info, tableName, cols, java.lang.System.currentTimeMillis());\n        long logSeqId = log.startCacheFlush();\n        log.completeCacheFlush(regionName, tableName, logSeqId, info.isMetaRegion());\n        log.close();\n        org.apache.hadoop.fs.Path filename = log.computeFilename(log.getFilenum());\n        log = null;\n        reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, filename, conf);\n        for (int i = 0; i < 1; i++) {\n            org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = reader.next(null);\n            if (entry == null)\n                break;\n\n            org.apache.hadoop.hbase.regionserver.wal.HLogKey key = entry.getKey();\n            org.apache.hadoop.hbase.regionserver.wal.WALEdit val = entry.getEdit();\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(regionName, key.getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, key.getTablename()));\n            org.apache.hadoop.hbase.KeyValue kv = val.getKeyValues().get(0);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(row, kv.getRow()));\n            junit.framework.Assert.assertEquals(((byte) (i + '0')), kv.getValue()[0]);\n            java.lang.System.out.println((key + \" \") + val);\n        }\n        org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = null;\n        while ((entry = reader.next(null)) != null) {\n            org.apache.hadoop.hbase.regionserver.wal.HLogKey key = entry.getKey();\n            org.apache.hadoop.hbase.regionserver.wal.WALEdit val = entry.getEdit();\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(regionName, key.getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, key.getTablename()));\n            org.apache.hadoop.hbase.KeyValue kv = val.getKeyValues().get(0);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, kv.getRow()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, kv.getFamily()));\n            junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH, val.getKeyValues().get(0).getValue()));\n            java.lang.System.out.println((key + \" \") + val);\n        } \n    } finally {\n        if (log != null) {\n            log.closeAndDelete();\n        }\n        if (reader != null) {\n            reader.close();\n        }\n    }\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 6,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testEditAdd",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 56,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    java.lang.Runnable runnable = new java.lang.Runnable() {\n        public void run() {\n            try {\n                org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n                org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n                scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n                org.apache.hadoop.hbase.client.ResultScanner s = t.getScanner(scan);\n                s.close();\n            } catch (java.io.IOException e) {\n                LOG.fatal(\"could not re-open meta table because\", e);\n                junit.framework.Assert.fail();\n            }\n            org.apache.hadoop.hbase.client.ResultScanner scanner = null;\n            try {\n                org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n                scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n                scanner = table.getScanner(scan);\n                LOG.info(\"Obtained scanner \" + scanner);\n                for (org.apache.hadoop.hbase.client.Result r : scanner) {\n                    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(r.getRow(), row));\n                    junit.framework.Assert.assertEquals(1, r.size());\n                    byte[] bytes = r.value();\n                    junit.framework.Assert.assertNotNull(bytes);\n                    junit.framework.Assert.assertTrue(tableName.equals(org.apache.hadoop.hbase.util.Bytes.toString(bytes)));\n                }\n                LOG.info(\"Success!\");\n            } catch (java.lang.Exception e) {\n                e.printStackTrace();\n                junit.framework.Assert.fail();\n            } finally {\n                if (scanner != null) {\n                    LOG.info(\"Closing scanner \" + scanner);\n                    scanner.close();\n                }\n            }\n        }\n    };\n    return new java.lang.Thread(runnable);\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startVerificationThread",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 40,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_5": {
            "Body": "{\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    this.server = cluster.getRegionServerThreads().get(0).getRegionServer();\n    this.log = server.getLog();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    for (int i = 1; i <= 256; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + java.lang.String.format(\"%1$04d\", i)));\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, value);\n        table.put(put);\n        if ((i % 32) == 0) {\n            try {\n                java.lang.Thread.sleep(2000);\n            } catch (java.lang.InterruptedException e) {\n            }\n        }\n    }\n}",
            "ClassName": "TestLogRolling",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startAndWriteData",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "ClassName": "TestLogRolling",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 2,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testLogRollOnDatanodeDeath",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 44,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    this.tableName = getName();\n    try {\n        startAndWriteData();\n        org.apache.hadoop.hbase.regionserver.wal.TestLogRolling.LOG.info((\"after writing there are \" + log.getNumLogFiles()) + \" log files\");\n        java.util.List<org.apache.hadoop.hbase.regionserver.HRegion> regions = new java.util.ArrayList<org.apache.hadoop.hbase.regionserver.HRegion>(server.getOnlineRegions());\n        for (org.apache.hadoop.hbase.regionserver.HRegion r : regions) {\n            r.flushcache();\n        }\n        log.rollWriter();\n        int count = log.getNumLogFiles();\n        org.apache.hadoop.hbase.regionserver.wal.TestLogRolling.LOG.info((\"after flushing all regions and rolling logs there are \" + log.getNumLogFiles()) + \" log files\");\n        junit.framework.Assert.assertTrue(\"actual count: \" + count, count <= 2);\n    } catch (java.lang.Exception e) {\n        org.apache.hadoop.hbase.regionserver.wal.TestLogRolling.LOG.fatal(\"unexpected exception\", e);\n        throw e;\n    }\n}",
        "CUT_1": {
            "Body": "{\n    this.tableName = getName();\n    try {\n        startAndWriteData();\n        org.apache.hadoop.hbase.regionserver.wal.TestLogRolling.LOG.info((\"after writing there are \" + log.getNumLogFiles()) + \" log files\");\n        java.util.List<org.apache.hadoop.hbase.regionserver.HRegion> regions = new java.util.ArrayList<org.apache.hadoop.hbase.regionserver.HRegion>(server.getOnlineRegions());\n        for (org.apache.hadoop.hbase.regionserver.HRegion r : regions) {\n            r.flushcache();\n        }\n        log.rollWriter();\n        int count = log.getNumLogFiles();\n        org.apache.hadoop.hbase.regionserver.wal.TestLogRolling.LOG.info((\"after flushing all regions and rolling logs there are \" + log.getNumLogFiles()) + \" log files\");\n        junit.framework.Assert.assertTrue(\"actual count: \" + count, count <= 2);\n    } catch (java.lang.Exception e) {\n        org.apache.hadoop.hbase.regionserver.wal.TestLogRolling.LOG.fatal(\"unexpected exception\", e);\n        throw e;\n    }\n}",
            "ClassName": "TestLogRolling",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testLogRolling",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 18,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    if (org.apache.hadoop.hbase.regionserver.HRegion.LOG.isDebugEnabled()) {\n        org.apache.hadoop.hbase.regionserver.HRegion.LOG.debug(\"Opening region: \" + info);\n    }\n    if (info == null) {\n        throw new java.lang.NullPointerException(\"Passed region info is null\");\n    }\n    org.apache.hadoop.hbase.regionserver.HRegion r = org.apache.hadoop.hbase.regionserver.HRegion.newHRegion(org.apache.hadoop.hbase.HTableDescriptor.getTableDir(rootDir, info.getTableDesc().getName()), log, org.apache.hadoop.fs.FileSystem.get(conf), conf, info, null);\n    r.initialize(null, null);\n    if (log != null) {\n        log.setSequenceNumber(r.getMinSequenceId());\n    }\n    return r;\n}",
            "ClassName": "HRegion",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "openHRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 14,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    if (!a.getRegionInfo().getTableDesc().getNameAsString().equals(b.getRegionInfo().getTableDesc().getNameAsString())) {\n        throw new java.io.IOException(\"Regions do not belong to the same table\");\n    }\n    org.apache.hadoop.fs.FileSystem fs = a.getFilesystem();\n    a.flushcache();\n    b.flushcache();\n    a.compactStores(true);\n    if (org.apache.hadoop.hbase.regionserver.HRegion.LOG.isDebugEnabled()) {\n        org.apache.hadoop.hbase.regionserver.HRegion.LOG.debug(\"Files for region: \" + a);\n        org.apache.hadoop.hbase.regionserver.HRegion.listPaths(fs, a.getRegionDir());\n    }\n    b.compactStores(true);\n    if (org.apache.hadoop.hbase.regionserver.HRegion.LOG.isDebugEnabled()) {\n        org.apache.hadoop.hbase.regionserver.HRegion.LOG.debug(\"Files for region: \" + b);\n        org.apache.hadoop.hbase.regionserver.HRegion.listPaths(fs, b.getRegionDir());\n    }\n    org.apache.hadoop.conf.Configuration conf = a.getConf();\n    org.apache.hadoop.hbase.HTableDescriptor tabledesc = a.getTableDesc();\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = a.getLog();\n    org.apache.hadoop.fs.Path basedir = a.getBaseDir();\n    final byte[] startKey = (a.comparator.matchingRows(a.getStartKey(), 0, a.getStartKey().length, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY, 0, org.apache.hadoop.hbase.regionserver.HRegion.EMPTY_BYTE_ARRAY.length) || b.comparator.matchingRows(b.getStartKey(), 0, b.getStartKey().length, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY, 0, org.apache.hadoop.hbase.regionserver.HRegion.EMPTY_BYTE_ARRAY.length)) ? org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY : a.comparator.compareRows(a.getStartKey(), 0, a.getStartKey().length, b.getStartKey(), 0, b.getStartKey().length) <= 0 ? a.getStartKey() : b.getStartKey();\n    final byte[] endKey = (a.comparator.matchingRows(a.getEndKey(), 0, a.getEndKey().length, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY, 0, org.apache.hadoop.hbase.regionserver.HRegion.EMPTY_BYTE_ARRAY.length) || a.comparator.matchingRows(b.getEndKey(), 0, b.getEndKey().length, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY, 0, org.apache.hadoop.hbase.regionserver.HRegion.EMPTY_BYTE_ARRAY.length)) ? org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY : a.comparator.compareRows(a.getEndKey(), 0, a.getEndKey().length, b.getEndKey(), 0, b.getEndKey().length) <= 0 ? b.getEndKey() : a.getEndKey();\n    org.apache.hadoop.hbase.HRegionInfo newRegionInfo = new org.apache.hadoop.hbase.HRegionInfo(tabledesc, startKey, endKey);\n    org.apache.hadoop.hbase.regionserver.HRegion.LOG.info(\"Creating new region \" + newRegionInfo.toString());\n    int encodedName = newRegionInfo.getEncodedName();\n    org.apache.hadoop.fs.Path newRegionDir = org.apache.hadoop.hbase.regionserver.HRegion.getRegionDir(a.getBaseDir(), encodedName);\n    if (fs.exists(newRegionDir)) {\n        throw new java.io.IOException(\"Cannot merge; target file collision at \" + newRegionDir);\n    }\n    fs.mkdirs(newRegionDir);\n    org.apache.hadoop.hbase.regionserver.HRegion.LOG.info((((((((((\"starting merge of regions: \" + a) + \" and \") + b) + \" into new region \") + newRegionInfo.toString()) + \" with start key <\") + org.apache.hadoop.hbase.util.Bytes.toString(startKey)) + \"> and end key <\") + org.apache.hadoop.hbase.util.Bytes.toString(endKey)) + \">\");\n    java.util.Map<byte[], java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>> byFamily = new java.util.TreeMap<byte[], java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>>(org.apache.hadoop.hbase.util.Bytes.BYTES_COMPARATOR);\n    byFamily = org.apache.hadoop.hbase.regionserver.HRegion.filesByFamily(byFamily, a.close());\n    byFamily = org.apache.hadoop.hbase.regionserver.HRegion.filesByFamily(byFamily, b.close());\n    for (java.util.Map.Entry<byte[], java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile>> es : byFamily.entrySet()) {\n        byte[] colFamily = es.getKey();\n        org.apache.hadoop.hbase.regionserver.HRegion.makeColumnFamilyDirs(fs, basedir, newRegionInfo, colFamily);\n        java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile> srcFiles = es.getValue();\n        if (srcFiles.size() == 2) {\n            long seqA = srcFiles.get(0).getMaxSequenceId();\n            long seqB = srcFiles.get(1).getMaxSequenceId();\n            if (seqA == seqB) {\n                throw new java.io.IOException(\"Files have same sequenceid: \" + seqA);\n            }\n        }\n        for (org.apache.hadoop.hbase.regionserver.StoreFile hsf : srcFiles) {\n            org.apache.hadoop.hbase.regionserver.StoreFile.rename(fs, hsf.getPath(), org.apache.hadoop.hbase.regionserver.StoreFile.getUniqueFile(fs, org.apache.hadoop.hbase.regionserver.Store.getStoreHomedir(basedir, newRegionInfo.getEncodedName(), colFamily)));\n        }\n    }\n    if (org.apache.hadoop.hbase.regionserver.HRegion.LOG.isDebugEnabled()) {\n        org.apache.hadoop.hbase.regionserver.HRegion.LOG.debug(\"Files for new region\");\n        org.apache.hadoop.hbase.regionserver.HRegion.listPaths(fs, newRegionDir);\n    }\n    org.apache.hadoop.hbase.regionserver.HRegion dstRegion = org.apache.hadoop.hbase.regionserver.HRegion.newHRegion(basedir, log, fs, conf, newRegionInfo, null);\n    dstRegion.initialize(null, null);\n    dstRegion.compactStores();\n    if (org.apache.hadoop.hbase.regionserver.HRegion.LOG.isDebugEnabled()) {\n        org.apache.hadoop.hbase.regionserver.HRegion.LOG.debug(\"Files for new region\");\n        org.apache.hadoop.hbase.regionserver.HRegion.listPaths(fs, dstRegion.getRegionDir());\n    }\n    org.apache.hadoop.hbase.regionserver.HRegion.deleteRegion(fs, a.getRegionDir());\n    org.apache.hadoop.hbase.regionserver.HRegion.deleteRegion(fs, b.getRegionDir());\n    org.apache.hadoop.hbase.regionserver.HRegion.LOG.info(\"merge completed. New region is \" + dstRegion);\n    return dstRegion;\n}",
            "ClassName": "HRegion",
            "CyclomaticComplexity": 10,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "merge",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 66,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    final int COL_COUNT = 10;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"tablename\");\n    final byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\");\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = null;\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(fs, dir, this.oldLogDir, this.conf, null);\n    try {\n        long timestamp = java.lang.System.currentTimeMillis();\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit cols = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        for (int i = 0; i < COL_COUNT; i++) {\n            cols.add(new org.apache.hadoop.hbase.KeyValue(row, org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\"), org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(i)), timestamp, new byte[]{ ((byte) (i + '0')) }));\n        }\n        org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(tableName), row, org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.util.Bytes.toString(row) + \"1\"), false);\n        final byte[] regionName = info.getRegionName();\n        log.append(info, tableName, cols, java.lang.System.currentTimeMillis());\n        long logSeqId = log.startCacheFlush();\n        log.completeCacheFlush(regionName, tableName, logSeqId, info.isMetaRegion());\n        log.close();\n        org.apache.hadoop.fs.Path filename = log.computeFilename(log.getFilenum());\n        log = null;\n        reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, filename, conf);\n        for (int i = 0; i < 1; i++) {\n            org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = reader.next(null);\n            if (entry == null)\n                break;\n\n            org.apache.hadoop.hbase.regionserver.wal.HLogKey key = entry.getKey();\n            org.apache.hadoop.hbase.regionserver.wal.WALEdit val = entry.getEdit();\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(regionName, key.getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, key.getTablename()));\n            org.apache.hadoop.hbase.KeyValue kv = val.getKeyValues().get(0);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(row, kv.getRow()));\n            junit.framework.Assert.assertEquals(((byte) (i + '0')), kv.getValue()[0]);\n            java.lang.System.out.println((key + \" \") + val);\n        }\n        org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = null;\n        while ((entry = reader.next(null)) != null) {\n            org.apache.hadoop.hbase.regionserver.wal.HLogKey key = entry.getKey();\n            org.apache.hadoop.hbase.regionserver.wal.WALEdit val = entry.getEdit();\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(regionName, key.getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, key.getTablename()));\n            org.apache.hadoop.hbase.KeyValue kv = val.getKeyValues().get(0);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, kv.getRow()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, kv.getFamily()));\n            junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH, val.getKeyValues().get(0).getValue()));\n            java.lang.System.out.println((key + \" \") + val);\n        } \n    } finally {\n        if (log != null) {\n            log.closeAndDelete();\n        }\n        if (reader != null) {\n            reader.close();\n        }\n    }\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 6,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testEditAdd",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 56,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    LOG.info(\"Taking out counting scan\");\n    org.apache.hadoop.hbase.HBaseTestCase.ScannerIncommon s = hri.getScanner(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.regionserver.TestScanner.EXPLICIT_COLS, org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.HConstants.LATEST_TIMESTAMP);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> values = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    int count = 0;\n    boolean justFlushed = false;\n    while (s.next(values)) {\n        if (justFlushed) {\n            LOG.info(\"after next() just after next flush\");\n            justFlushed = false;\n        }\n        count++;\n        if (flushIndex == count) {\n            LOG.info(\"Starting flush at flush index \" + flushIndex);\n            java.lang.Thread t = new java.lang.Thread() {\n                public void run() {\n                    try {\n                        hri.flushcache();\n                        LOG.info(\"Finishing flush\");\n                    } catch (java.io.IOException e) {\n                        LOG.info(\"Failed flush cache\");\n                    }\n                }\n            };\n            if (concurrent) {\n                t.start();\n            } else {\n                t.run();\n            }\n            LOG.info(\"Continuing on after kicking off background flush\");\n            justFlushed = true;\n        }\n    } \n    s.close();\n    LOG.info((\"Found \" + count) + \" items\");\n    return count;\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "count",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 37,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 8
        },
        "ClassName": "TestLogRolling",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 1,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testLogRolling",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 18,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n    org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon hri = new org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon(r);\n    try {\n        LOG.info(\"Added: \" + org.apache.hadoop.hbase.HBaseTestCase.addContent(hri, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY), org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER)));\n        int count = count(hri, -1, false);\n        junit.framework.Assert.assertEquals(count, count(hri, 100, true));\n    } catch (java.lang.Exception e) {\n        LOG.error(\"Failed\", e);\n        throw e;\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(cluster);\n    }\n}",
        "CUT_1": {
            "Body": "{\n    this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n    org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon hri = new org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon(r);\n    try {\n        LOG.info(\"Added: \" + org.apache.hadoop.hbase.HBaseTestCase.addContent(hri, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY), org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER)));\n        int count = count(hri, -1, false);\n        junit.framework.Assert.assertEquals(count, count(hri, 100, true));\n    } catch (java.lang.Exception e) {\n        LOG.error(\"Failed\", e);\n        throw e;\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(cluster);\n    }\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanAndRealConcurrentFlush",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n    org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon hri = new org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon(r);\n    try {\n        LOG.info(\"Added: \" + org.apache.hadoop.hbase.HBaseTestCase.addContent(hri, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY), org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER)));\n        int count = count(hri, -1, false);\n        junit.framework.Assert.assertEquals(count, count(hri, 100, false));\n    } catch (java.lang.Exception e) {\n        LOG.error(\"Failed\", e);\n        throw e;\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(cluster);\n    }\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanAndSyncFlush",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    byte[] startrow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"bbb\");\n    byte[] stoprow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"ccc\");\n    try {\n        this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n        org.apache.hadoop.hbase.HBaseTestCase.addContent(this.r, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"abc\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"abd\"));\n        scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n        int count = 0;\n        while (s.next(results)) {\n            count++;\n        } \n        s.close();\n        junit.framework.Assert.assertEquals(0, count);\n        scan = new org.apache.hadoop.hbase.client.Scan(startrow, stoprow);\n        scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        s = r.getScanner(scan);\n        count = 0;\n        org.apache.hadoop.hbase.KeyValue kv = null;\n        results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        for (boolean first = true; s.next(results);) {\n            kv = results.get(0);\n            if (first) {\n                junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.BYTES_COMPARATOR.compare(startrow, kv.getRow()) == 0);\n                first = false;\n            }\n            count++;\n        }\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.BYTES_COMPARATOR.compare(stoprow, kv.getRow()) > 0);\n        junit.framework.Assert.assertTrue(count > 10);\n        s.close();\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(this.cluster);\n    }\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testStopRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 39,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    if (org.apache.hadoop.hbase.util.MetaUtils.LOG.isDebugEnabled()) {\n        org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(hri.getRegionName());\n        get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        org.apache.hadoop.hbase.client.Result res = r.get(get, null);\n        org.apache.hadoop.hbase.KeyValue[] kvs = res.raw();\n        if (kvs.length <= 0) {\n            return;\n        }\n        byte[] value = kvs[0].getValue();\n        if (value == null) {\n            return;\n        }\n        org.apache.hadoop.hbase.HRegionInfo h = org.apache.hadoop.hbase.util.Writables.getHRegionInfoOrNull(value);\n        org.apache.hadoop.hbase.util.MetaUtils.LOG.debug(((((((((\"Old \" + org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY)) + \":\") + org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER)) + \" for \") + hri.toString()) + \" in \") + r.toString()) + \" is: \") + h.toString());\n    }\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(hri.getRegionName());\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(hri));\n    r.put(put);\n    if (org.apache.hadoop.hbase.util.MetaUtils.LOG.isDebugEnabled()) {\n        org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(hri.getRegionName());\n        get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        org.apache.hadoop.hbase.client.Result res = r.get(get, null);\n        org.apache.hadoop.hbase.KeyValue[] kvs = res.raw();\n        if (kvs.length <= 0) {\n            return;\n        }\n        byte[] value = kvs[0].getValue();\n        if (value == null) {\n            return;\n        }\n        org.apache.hadoop.hbase.HRegionInfo h = org.apache.hadoop.hbase.util.Writables.getHRegionInfoOrNull(value);\n        org.apache.hadoop.hbase.util.MetaUtils.LOG.debug(((((((((\"New \" + org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY)) + \":\") + org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER)) + \" for \") + hri.toString()) + \" in \") + r.toString()) + \" is: \") + h.toString());\n    }\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 6,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "updateMETARegionInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 35,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    try {\n        this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n        org.apache.hadoop.hbase.HBaseTestCase.addContent(this.r, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        try {\n            s.next(results);\n            s.close();\n            s.next(results);\n            junit.framework.Assert.fail(\"We don't want anything more, we should be failing\");\n        } catch (org.apache.hadoop.hbase.UnknownScannerException ex) {\n            return;\n        }\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(this.cluster);\n    }\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testRaceBetweenClientAndTimeout",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestScanner",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testScanAndRealConcurrentFlush",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 16,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n    org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon hri = new org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon(r);\n    try {\n        LOG.info(\"Added: \" + org.apache.hadoop.hbase.HBaseTestCase.addContent(hri, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY), org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER)));\n        int count = count(hri, -1, false);\n        junit.framework.Assert.assertEquals(count, count(hri, 100, false));\n    } catch (java.lang.Exception e) {\n        LOG.error(\"Failed\", e);\n        throw e;\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(cluster);\n    }\n}",
        "CUT_1": {
            "Body": "{\n    this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n    org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon hri = new org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon(r);\n    try {\n        LOG.info(\"Added: \" + org.apache.hadoop.hbase.HBaseTestCase.addContent(hri, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY), org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER)));\n        int count = count(hri, -1, false);\n        junit.framework.Assert.assertEquals(count, count(hri, 100, false));\n    } catch (java.lang.Exception e) {\n        LOG.error(\"Failed\", e);\n        throw e;\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(cluster);\n    }\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanAndSyncFlush",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n    org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon hri = new org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon(r);\n    try {\n        LOG.info(\"Added: \" + org.apache.hadoop.hbase.HBaseTestCase.addContent(hri, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY), org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER)));\n        int count = count(hri, -1, false);\n        junit.framework.Assert.assertEquals(count, count(hri, 100, true));\n    } catch (java.lang.Exception e) {\n        LOG.error(\"Failed\", e);\n        throw e;\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(cluster);\n    }\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanAndRealConcurrentFlush",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    byte[] startrow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"bbb\");\n    byte[] stoprow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"ccc\");\n    try {\n        this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n        org.apache.hadoop.hbase.HBaseTestCase.addContent(this.r, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"abc\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"abd\"));\n        scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n        int count = 0;\n        while (s.next(results)) {\n            count++;\n        } \n        s.close();\n        junit.framework.Assert.assertEquals(0, count);\n        scan = new org.apache.hadoop.hbase.client.Scan(startrow, stoprow);\n        scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        s = r.getScanner(scan);\n        count = 0;\n        org.apache.hadoop.hbase.KeyValue kv = null;\n        results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        for (boolean first = true; s.next(results);) {\n            kv = results.get(0);\n            if (first) {\n                junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.BYTES_COMPARATOR.compare(startrow, kv.getRow()) == 0);\n                first = false;\n            }\n            count++;\n        }\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.BYTES_COMPARATOR.compare(stoprow, kv.getRow()) > 0);\n        junit.framework.Assert.assertTrue(count > 10);\n        s.close();\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(this.cluster);\n    }\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testStopRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 39,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    if (org.apache.hadoop.hbase.util.MetaUtils.LOG.isDebugEnabled()) {\n        org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(hri.getRegionName());\n        get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        org.apache.hadoop.hbase.client.Result res = r.get(get, null);\n        org.apache.hadoop.hbase.KeyValue[] kvs = res.raw();\n        if (kvs.length <= 0) {\n            return;\n        }\n        byte[] value = kvs[0].getValue();\n        if (value == null) {\n            return;\n        }\n        org.apache.hadoop.hbase.HRegionInfo h = org.apache.hadoop.hbase.util.Writables.getHRegionInfoOrNull(value);\n        org.apache.hadoop.hbase.util.MetaUtils.LOG.debug(((((((((\"Old \" + org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY)) + \":\") + org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER)) + \" for \") + hri.toString()) + \" in \") + r.toString()) + \" is: \") + h.toString());\n    }\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(hri.getRegionName());\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(hri));\n    r.put(put);\n    if (org.apache.hadoop.hbase.util.MetaUtils.LOG.isDebugEnabled()) {\n        org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(hri.getRegionName());\n        get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        org.apache.hadoop.hbase.client.Result res = r.get(get, null);\n        org.apache.hadoop.hbase.KeyValue[] kvs = res.raw();\n        if (kvs.length <= 0) {\n            return;\n        }\n        byte[] value = kvs[0].getValue();\n        if (value == null) {\n            return;\n        }\n        org.apache.hadoop.hbase.HRegionInfo h = org.apache.hadoop.hbase.util.Writables.getHRegionInfoOrNull(value);\n        org.apache.hadoop.hbase.util.MetaUtils.LOG.debug(((((((((\"New \" + org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY)) + \":\") + org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER)) + \" for \") + hri.toString()) + \" in \") + r.toString()) + \" is: \") + h.toString());\n    }\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 6,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "updateMETARegionInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 35,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    try {\n        this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n        org.apache.hadoop.hbase.HBaseTestCase.addContent(this.r, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        try {\n            s.next(results);\n            s.close();\n            s.next(results);\n            junit.framework.Assert.fail(\"We don't want anything more, we should be failing\");\n        } catch (org.apache.hadoop.hbase.UnknownScannerException ex) {\n            return;\n        }\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(this.cluster);\n    }\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testRaceBetweenClientAndTimeout",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestScanner",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testScanAndSyncFlush",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 16,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    doATest(true);\n}",
        "CUT_1": {
            "Body": "{\n    doATest(true);\n}",
            "ClassName": "TestMultiParallelPut",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testParallelPutWithRSAbort",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    doATest(false);\n}",
            "ClassName": "TestMultiParallelPut",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testParallelPut",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return true;\n}",
            "ClassName": "PlainTextMessageBodyProducer",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "isWriteable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return true;\n}",
            "ClassName": "DependentColumnFilter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "hasFilterRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return true;\n}",
            "ClassName": "BoundedRangeFileInputStream",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "markSupported",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestMultiParallelPut",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testParallelPutWithRSAbort",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = new org.apache.hadoop.hbase.client.HTablePool(org.apache.hadoop.hbase.client.TestHTablePool.TEST_UTIL.getConfiguration(), 4);\n    java.lang.String tableName = \"testTable\";\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestHTablePool.TEST_UTIL.getConfiguration());\n    if (admin.tableExists(tableName)) {\n        admin.deleteTable(tableName);\n    }\n    org.apache.hadoop.hbase.HTableDescriptor tableDescriptor = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    tableDescriptor.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(\"randomFamily\"));\n    admin.createTable(tableDescriptor);\n    org.apache.hadoop.hbase.client.HTableInterface[] tables = new org.apache.hadoop.hbase.client.HTableInterface[4];\n    for (int i = 0; i < 4; ++i) {\n        tables[i] = pool.getTable(tableName);\n    }\n    pool.closeTablePool(tableName);\n    for (int i = 0; i < 4; ++i) {\n        pool.putTable(tables[i]);\n    }\n    junit.framework.Assert.assertEquals(4, pool.getCurrentPoolSize(tableName));\n    pool.closeTablePool(tableName);\n    junit.framework.Assert.assertEquals(0, pool.getCurrentPoolSize(tableName));\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    try {\n        return ((org.apache.hadoop.hbase.client.HTable) (table)).getRegionsInfo();\n    } finally {\n        pool.putTable(table);\n    }\n}",
            "ClassName": "RegionsResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    try {\n        return table.getTableDescriptor();\n    } finally {\n        pool.putTable(table);\n    }\n}",
            "ClassName": "SchemaResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableSchema",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return pool;\n}",
            "ClassName": "RESTServlet",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTablePool",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHTablePool",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 3,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testCloseTablePool",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 22,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    testScan(\"opp\", null, \"zzz\");\n}",
        "CUT_1": {
            "Body": "{\n    return ((x == null) && (y == null)) || ((x != null) && x.equals(y));\n}",
            "ClassName": "Pair",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "equals",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "internalPreFlushcacheCommit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return null;\n}",
            "ClassName": "WildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return null;\n}",
            "ClassName": "ScanWildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HalfStoreFileReader",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "midkey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableInputFormatScan",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testScanOPPToEmpty",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testMaxKeyValueSize\");\n    org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration();\n    java.lang.String oldMaxSize = conf.get(\"hbase.client.keyvalue.maxsize\");\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    byte[] value = new byte[(4 * 1024) * 1024];\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, value);\n    ht.put(put);\n    try {\n        conf.setInt(\"hbase.client.keyvalue.maxsize\", (2 * 1024) * 1024);\n        TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testMaxKeyValueSize2\");\n        ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n        put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n        put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, value);\n        ht.put(put);\n        org.junit.Assert.fail(\"Inserting a too large KeyValue worked, should throw exception\");\n    } catch (java.lang.Exception e) {\n    }\n    conf.set(\"hbase.client.keyvalue.maxsize\", oldMaxSize);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testMaxKeyValueSize",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 21,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    LOG.info(\"Starting testRegionServerSessionExpired\");\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.getMiniHBaseCluster().getRegionServer(0).getConfiguration().setBoolean(\"hbase.regionserver.restart.on.zk.expire\", true);\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.expireRegionServerSession(0);\n    testSanity();\n}",
        "CUT_1": {
            "Body": "{\n    conf = org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.getConfiguration();\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.startMiniZKCluster();\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.startMiniCluster(1);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        org.apache.hadoop.hbase.HRegionInfo hri = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(b);\n        byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(row);\n        p.add(org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily(), org.apache.hadoop.hbase.master.TestMasterTransitions.getTestQualifier(), row);\n        t.put(p);\n        rows++;\n    }\n    s.close();\n    org.junit.Assert.assertEquals(expected, rows);\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addToEachStartKey",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        rows++;\n    }\n    s.close();\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "countOfMetaRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestZooKeeper",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testRegionServerSessionExpired",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 7,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    testScan(null, \"bba\", \"baz\");\n}",
        "CUT_1": {
            "Body": "{\n    return ((x == null) && (y == null)) || ((x != null) && x.equals(y));\n}",
            "ClassName": "Pair",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "equals",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "internalPreFlushcacheCommit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return null;\n}",
            "ClassName": "WildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return null;\n}",
            "ClassName": "ScanWildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HalfStoreFileReader",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "midkey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableInputFormatScan",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testScanEmptyToBBA",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] t1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testListTables1\");\n    byte[] t2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testListTables2\");\n    byte[] t3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testListTables3\");\n    byte[][] tables = new byte[][]{ t1, t2, t3 };\n    for (int i = 0; i < tables.length; i++) {\n        org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(tables[i], org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    }\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration());\n    org.apache.hadoop.hbase.HTableDescriptor[] ts = admin.listTables();\n    java.util.HashSet<org.apache.hadoop.hbase.HTableDescriptor> result = new java.util.HashSet<org.apache.hadoop.hbase.HTableDescriptor>(ts.length);\n    for (int i = 0; i < ts.length; i++) {\n        result.add(ts[i]);\n    }\n    int size = result.size();\n    org.junit.Assert.assertTrue(size >= tables.length);\n    for (int i = 0; (i < tables.length) && (i < size); i++) {\n        boolean found = false;\n        for (int j = 0; j < ts.length; j++) {\n            if (org.apache.hadoop.hbase.util.Bytes.equals(ts[j].getName(), tables[i])) {\n                found = true;\n                break;\n            }\n        }\n        org.junit.Assert.assertTrue(\"Not found: \" + org.apache.hadoop.hbase.util.Bytes.toString(tables[i]), found);\n    }\n}",
        "CUT_1": {
            "Body": "{\n    boolean found = false;\n    java.util.Iterator<org.apache.hadoop.hbase.rest.model.TableModel> tables = model.getTables().iterator();\n    junit.framework.Assert.assertTrue(tables.hasNext());\n    while (tables.hasNext()) {\n        org.apache.hadoop.hbase.rest.model.TableModel table = tables.next();\n        if (table.getName().equals(org.apache.hadoop.hbase.rest.TestTableResource.TABLE)) {\n            found = true;\n            break;\n        }\n    } \n    junit.framework.Assert.assertTrue(found);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "checkTableList",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 13,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    this.tables = tables;\n}",
            "ClassName": "TableListModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setTables",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.TimestampTestBase.put(loader, org.apache.hadoop.hbase.util.Bytes.toBytes(ts), ts);\n}",
            "ClassName": "TimestampTestBase",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "put",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    byte[] rowA = org.apache.hadoop.hbase.util.Bytes.toBytes(\"rowA\");\n    byte[] famA = org.apache.hadoop.hbase.util.Bytes.toBytes(\"famA\");\n    byte[] qfA = org.apache.hadoop.hbase.util.Bytes.toBytes(\"qfA\");\n    byte[] valueA = org.apache.hadoop.hbase.util.Bytes.toBytes(\"valueA\");\n    byte[] rowB = org.apache.hadoop.hbase.util.Bytes.toBytes(\"rowB\");\n    byte[] famB = org.apache.hadoop.hbase.util.Bytes.toBytes(\"famB\");\n    byte[] qfB = org.apache.hadoop.hbase.util.Bytes.toBytes(\"qfB\");\n    byte[] valueB = org.apache.hadoop.hbase.util.Bytes.toBytes(\"valueB\");\n    org.apache.hadoop.hbase.KeyValue kvA = new org.apache.hadoop.hbase.KeyValue(rowA, famA, qfA, valueA);\n    org.apache.hadoop.hbase.KeyValue kvB = new org.apache.hadoop.hbase.KeyValue(rowB, famB, qfB, valueB);\n    org.apache.hadoop.hbase.client.Result result = new org.apache.hadoop.hbase.client.Result(new org.apache.hadoop.hbase.KeyValue[]{ kvA, kvB });\n    byte[] rb = org.apache.hadoop.hbase.util.Writables.getBytes(result);\n    org.apache.hadoop.hbase.client.Result deResult = ((org.apache.hadoop.hbase.client.Result) (org.apache.hadoop.hbase.util.Writables.getWritable(rb, new org.apache.hadoop.hbase.client.Result())));\n    junit.framework.Assert.assertTrue(\"results are not equivalent, first key mismatch\", result.sorted()[0].equals(deResult.sorted()[0]));\n    junit.framework.Assert.assertTrue(\"results are not equivalent, second key mismatch\", result.sorted()[1].equals(deResult.sorted()[1]));\n    org.apache.hadoop.hbase.client.Result r = new org.apache.hadoop.hbase.client.Result();\n    byte[] b = org.apache.hadoop.hbase.util.Writables.getBytes(r);\n    org.apache.hadoop.hbase.client.Result deserialized = ((org.apache.hadoop.hbase.client.Result) (org.apache.hadoop.hbase.util.Writables.getWritable(b, new org.apache.hadoop.hbase.client.Result())));\n    junit.framework.Assert.assertEquals(r.size(), deserialized.size());\n}",
            "ClassName": "TestSerialization",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testResult",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 5,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testListTables",
        "NumberOfAsserts": 2,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 27,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    org.apache.hadoop.conf.Configuration c = org.apache.hadoop.hbase.master.TestOldLogsCleaner.TEST_UTIL.getConfiguration();\n    org.apache.hadoop.fs.Path oldLogDir = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.master.TestOldLogsCleaner.TEST_UTIL.getTestDir(), org.apache.hadoop.hbase.HConstants.HREGION_OLDLOGDIR_NAME);\n    org.apache.hadoop.fs.FileSystem fs = org.apache.hadoop.fs.FileSystem.get(c);\n    java.util.concurrent.atomic.AtomicBoolean stop = new java.util.concurrent.atomic.AtomicBoolean(false);\n    org.apache.hadoop.hbase.master.OldLogsCleaner cleaner = new org.apache.hadoop.hbase.master.OldLogsCleaner(1000, stop, c, fs, oldLogDir);\n    long now = java.lang.System.currentTimeMillis();\n    fs.delete(oldLogDir, true);\n    fs.mkdirs(oldLogDir);\n    fs.createNewFile(new org.apache.hadoop.fs.Path(oldLogDir, \"a\"));\n    fs.createNewFile(new org.apache.hadoop.fs.Path(oldLogDir, \"1.hlog.dat.a\"));\n    fs.createNewFile(new org.apache.hadoop.fs.Path(oldLogDir, \"1.hlog.dat.\" + now));\n    for (int i = 0; i < 30; i++) {\n        fs.createNewFile(new org.apache.hadoop.fs.Path(oldLogDir, (1 + \"hlog.dat.\") + ((now - 6000000) - i)));\n    }\n    fs.createNewFile(new org.apache.hadoop.fs.Path(oldLogDir, \"a.hlog.dat.\" + (now + 10000)));\n    org.junit.Assert.assertEquals(34, fs.listStatus(oldLogDir).length);\n    cleaner.chore();\n    org.junit.Assert.assertEquals(14, fs.listStatus(oldLogDir).length);\n    cleaner.chore();\n    org.junit.Assert.assertEquals(1, fs.listStatus(oldLogDir).length);\n}",
        "CUT_1": {
            "Body": "{\n    if (this.log == null) {\n        org.apache.hadoop.fs.Path logdir = new org.apache.hadoop.fs.Path(this.fs.getHomeDirectory(), (org.apache.hadoop.hbase.HConstants.HREGION_LOGDIR_NAME + \"_\") + java.lang.System.currentTimeMillis());\n        org.apache.hadoop.fs.Path oldLogDir = new org.apache.hadoop.fs.Path(this.fs.getHomeDirectory(), org.apache.hadoop.hbase.HConstants.HREGION_OLDLOGDIR_NAME);\n        this.log = new org.apache.hadoop.hbase.regionserver.wal.HLog(this.fs, logdir, oldLogDir, this.conf, null);\n    }\n    return this.log;\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getLog",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return new org.apache.hadoop.fs.Path(oldLogDir, (java.lang.System.currentTimeMillis() + \".\") + p.getName());\n}",
            "ClassName": "HLog",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getHLogArchivePath",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.fs.Path basedir = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.TestStore.DIR + methodName);\n    org.apache.hadoop.fs.Path logdir = new org.apache.hadoop.fs.Path((org.apache.hadoop.hbase.regionserver.TestStore.DIR + methodName) + \"/logs\");\n    org.apache.hadoop.fs.Path oldLogDir = new org.apache.hadoop.fs.Path(basedir, org.apache.hadoop.hbase.HConstants.HREGION_OLDLOGDIR_NAME);\n    org.apache.hadoop.hbase.HColumnDescriptor hcd = new org.apache.hadoop.hbase.HColumnDescriptor(family);\n    org.apache.hadoop.hbase.HBaseConfiguration conf = new org.apache.hadoop.hbase.HBaseConfiguration();\n    org.apache.hadoop.fs.FileSystem fs = org.apache.hadoop.fs.FileSystem.get(conf);\n    org.apache.hadoop.fs.Path reconstructionLog = null;\n    org.apache.hadoop.util.Progressable reporter = null;\n    fs.delete(logdir, true);\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(table);\n    htd.addFamily(hcd);\n    org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(htd, null, null, false);\n    org.apache.hadoop.hbase.regionserver.wal.HLog hlog = new org.apache.hadoop.hbase.regionserver.wal.HLog(fs, logdir, oldLogDir, conf, null);\n    org.apache.hadoop.hbase.regionserver.HRegion region = new org.apache.hadoop.hbase.regionserver.HRegion(basedir, hlog, fs, conf, info, null);\n    store = new org.apache.hadoop.hbase.regionserver.Store(basedir, region, hcd, fs, reconstructionLog, conf, reporter);\n}",
            "ClassName": "TestStore",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "init",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 17,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    if (args.length < 1) {\n        org.apache.hadoop.hbase.regionserver.HRegion.printUsageAndExit(null);\n    }\n    boolean majorCompact = false;\n    if (args.length > 1) {\n        if (!args[1].toLowerCase().startsWith(\"major\")) {\n            org.apache.hadoop.hbase.regionserver.HRegion.printUsageAndExit((\"ERROR: Unrecognized option <\" + args[1]) + \">\");\n        }\n        majorCompact = true;\n    }\n    org.apache.hadoop.fs.Path tableDir = new org.apache.hadoop.fs.Path(args[0]);\n    org.apache.hadoop.conf.Configuration c = org.apache.hadoop.hbase.HBaseConfiguration.create();\n    org.apache.hadoop.fs.FileSystem fs = org.apache.hadoop.fs.FileSystem.get(c);\n    org.apache.hadoop.fs.Path logdir = new org.apache.hadoop.fs.Path(c.get(\"hbase.tmp.dir\"), (\"hlog\" + tableDir.getName()) + java.lang.System.currentTimeMillis());\n    org.apache.hadoop.fs.Path oldLogDir = new org.apache.hadoop.fs.Path(c.get(\"hbase.tmp.dir\"), org.apache.hadoop.hbase.HConstants.HREGION_OLDLOGDIR_NAME);\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(fs, logdir, oldLogDir, c, null);\n    try {\n        org.apache.hadoop.hbase.regionserver.HRegion.processTable(fs, tableDir, log, c, majorCompact);\n    } finally {\n        log.close();\n        org.apache.hadoop.hbase.io.hfile.BlockCache bc = org.apache.hadoop.hbase.regionserver.StoreFile.getBlockCache(c);\n        if (bc != null)\n            bc.shutdown();\n\n    }\n}",
            "ClassName": "HRegion",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "main",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 27,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.fs.Path path = master.getRootDir();\n    org.apache.hadoop.fs.FileSystem fs = path.getFileSystem(master.getConfiguration());\n    return org.apache.hadoop.hbase.util.FSUtils.getTableFragmentation(fs, path);\n}",
            "ClassName": "FSUtils",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableFragmentation",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestOldLogsCleaner",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 1,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testLogCleaning",
        "NumberOfAsserts": 3,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 22,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 5,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    java.lang.String name = \"testTableNameClash\";\n    admin.createTable(new org.apache.hadoop.hbase.HTableDescriptor(name + \"SOMEUPPERCASE\"));\n    admin.createTable(new org.apache.hadoop.hbase.HTableDescriptor(name));\n    new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration(), name);\n}",
        "CUT_1": {
            "Body": "{\n    this.admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration());\n}",
            "ClassName": "TestAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(getName());\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.client.TestTimestamp.COLUMN_NAME));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    return new org.apache.hadoop.hbase.client.HTable(conf, getName());\n}",
            "ClassName": "TestTimestamp",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    this.name = name;\n}",
            "ClassName": "ColumnSchemaModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setName",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    this.name = name;\n}",
            "ClassName": "TableModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setName",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    this.name = name;\n}",
            "ClassName": "HTableDescriptor",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setName",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testTableNameClash",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 6,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testDeletes\");\n    byte[][] ROWS = makeNAscii(org.apache.hadoop.hbase.client.TestFromClientSide.ROW, 6);\n    byte[][] FAMILIES = makeNAscii(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, 3);\n    byte[][] VALUES = makeN(org.apache.hadoop.hbase.client.TestFromClientSide.VALUE, 5);\n    long[] ts = new long[]{ 1000, 2000, 3000, 4000, 5000 };\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, FAMILIES);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[0], VALUES[0]);\n    put.add(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[1], VALUES[1]);\n    ht.put(put);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    delete.deleteFamily(FAMILIES[0], ts[0]);\n    ht.delete(delete);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addFamily(FAMILIES[0]);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ ts[1] }, new byte[][]{ VALUES[1] }, 0, 0);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addFamily(FAMILIES[0]);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ ts[1] }, new byte[][]{ VALUES[1] }, 0, 0);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[4], VALUES[4]);\n    put.add(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[2], VALUES[2]);\n    put.add(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[3], VALUES[3]);\n    put.add(FAMILIES[0], null, ts[4], VALUES[4]);\n    put.add(FAMILIES[0], null, ts[2], VALUES[2]);\n    put.add(FAMILIES[0], null, ts[3], VALUES[3]);\n    ht.put(put);\n    delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    delete.deleteColumn(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    ht.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ ts[1], ts[2], ts[3] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3] }, 0, 2);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ ts[1], ts[2], ts[3] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3] }, 0, 2);\n    delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    delete.deleteColumn(FAMILIES[0], null);\n    ht.delete(delete);\n    delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    delete.deleteColumns(FAMILIES[0], null);\n    ht.delete(delete);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[0], VALUES[0]);\n    put.add(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[4], VALUES[4]);\n    ht.put(put);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addFamily(FAMILIES[0]);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ ts[1], ts[2], ts[3] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3] }, 0, 2);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addFamily(FAMILIES[0]);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ ts[1], ts[2], ts[3] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3] }, 0, 2);\n    put = new org.apache.hadoop.hbase.client.Put(ROWS[0]);\n    put.add(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[0], VALUES[0]);\n    put.add(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[1], VALUES[1]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[2], VALUES[2]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[3], VALUES[3]);\n    ht.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(ROWS[1]);\n    put.add(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[0], VALUES[0]);\n    put.add(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[1], VALUES[1]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[2], VALUES[2]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[3], VALUES[3]);\n    ht.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(ROWS[2]);\n    put.add(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[0], VALUES[0]);\n    put.add(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[1], VALUES[1]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[2], VALUES[2]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[3], VALUES[3]);\n    ht.put(put);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[2]);\n    get.addFamily(FAMILIES[1]);\n    get.addFamily(FAMILIES[2]);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    org.junit.Assert.assertTrue(((\"Expected 4 key but received \" + result.size()) + \": \") + result, result.size() == 4);\n    delete = new org.apache.hadoop.hbase.client.Delete(ROWS[0]);\n    delete.deleteFamily(FAMILIES[2]);\n    ht.delete(delete);\n    delete = new org.apache.hadoop.hbase.client.Delete(ROWS[1]);\n    delete.deleteColumns(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    ht.delete(delete);\n    delete = new org.apache.hadoop.hbase.client.Delete(ROWS[2]);\n    delete.deleteColumn(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    delete.deleteColumn(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    delete.deleteColumn(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    ht.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(FAMILIES[1]);\n    get.addFamily(FAMILIES[2]);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    org.junit.Assert.assertTrue(\"Expected 2 keys but received \" + result.size(), result.size() == 2);\n    assertNResult(result, ROWS[0], FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ ts[0], ts[1] }, new byte[][]{ VALUES[0], VALUES[1] }, 0, 1);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[0]);\n    scan.addFamily(FAMILIES[1]);\n    scan.addFamily(FAMILIES[2]);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    org.junit.Assert.assertTrue(\"Expected 2 keys but received \" + result.size(), result.size() == 2);\n    assertNResult(result, ROWS[0], FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ ts[0], ts[1] }, new byte[][]{ VALUES[0], VALUES[1] }, 0, 1);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[1]);\n    get.addFamily(FAMILIES[1]);\n    get.addFamily(FAMILIES[2]);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    org.junit.Assert.assertTrue(\"Expected 2 keys but received \" + result.size(), result.size() == 2);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[1]);\n    scan.addFamily(FAMILIES[1]);\n    scan.addFamily(FAMILIES[2]);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    org.junit.Assert.assertTrue(\"Expected 2 keys but received \" + result.size(), result.size() == 2);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[2]);\n    get.addFamily(FAMILIES[1]);\n    get.addFamily(FAMILIES[2]);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    org.junit.Assert.assertEquals(1, result.size());\n    assertNResult(result, ROWS[2], FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ ts[2] }, new byte[][]{ VALUES[2] }, 0, 0);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[2]);\n    scan.addFamily(FAMILIES[1]);\n    scan.addFamily(FAMILIES[2]);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    org.junit.Assert.assertEquals(1, result.size());\n    assertNResult(result, ROWS[2], FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ ts[2] }, new byte[][]{ VALUES[2] }, 0, 0);\n    delete = new org.apache.hadoop.hbase.client.Delete(ROWS[3]);\n    delete.deleteFamily(FAMILIES[1]);\n    ht.delete(delete);\n    put = new org.apache.hadoop.hbase.client.Put(ROWS[3]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, VALUES[0]);\n    ht.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(ROWS[4]);\n    put.add(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, VALUES[1]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, VALUES[2]);\n    ht.put(put);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[3]);\n    get.addFamily(FAMILIES[1]);\n    get.addFamily(FAMILIES[2]);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    org.junit.Assert.assertTrue(\"Expected 1 key but received \" + result.size(), result.size() == 1);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[4]);\n    get.addFamily(FAMILIES[1]);\n    get.addFamily(FAMILIES[2]);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    org.junit.Assert.assertTrue(\"Expected 2 keys but received \" + result.size(), result.size() == 2);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[3]);\n    scan.addFamily(FAMILIES[1]);\n    scan.addFamily(FAMILIES[2]);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    org.apache.hadoop.hbase.client.ResultScanner scanner = ht.getScanner(scan);\n    result = scanner.next();\n    org.junit.Assert.assertTrue(\"Expected 1 key but received \" + result.size(), result.size() == 1);\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(result.sorted()[0].getRow(), ROWS[3]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(result.sorted()[0].getValue(), VALUES[0]));\n    result = scanner.next();\n    org.junit.Assert.assertTrue(\"Expected 2 keys but received \" + result.size(), result.size() == 2);\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(result.sorted()[0].getRow(), ROWS[4]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(result.sorted()[1].getRow(), ROWS[4]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(result.sorted()[0].getValue(), VALUES[1]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(result.sorted()[1].getValue(), VALUES[2]));\n    scanner.close();\n    for (int i = 0; i < 10; i++) {\n        byte[] bytes = org.apache.hadoop.hbase.util.Bytes.toBytes(i);\n        put = new org.apache.hadoop.hbase.client.Put(bytes);\n        put.add(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, bytes);\n        ht.put(put);\n    }\n    for (int i = 0; i < 10; i++) {\n        byte[] bytes = org.apache.hadoop.hbase.util.Bytes.toBytes(i);\n        get = new org.apache.hadoop.hbase.client.Get(bytes);\n        get.addFamily(FAMILIES[0]);\n        result = ht.get(get);\n        org.junit.Assert.assertTrue(result.size() == 1);\n    }\n    java.util.ArrayList<org.apache.hadoop.hbase.client.Delete> deletes = new java.util.ArrayList<org.apache.hadoop.hbase.client.Delete>();\n    for (int i = 0; i < 10; i++) {\n        byte[] bytes = org.apache.hadoop.hbase.util.Bytes.toBytes(i);\n        delete = new org.apache.hadoop.hbase.client.Delete(bytes);\n        delete.deleteFamily(FAMILIES[0]);\n        deletes.add(delete);\n    }\n    ht.delete(deletes);\n    for (int i = 0; i < 10; i++) {\n        byte[] bytes = org.apache.hadoop.hbase.util.Bytes.toBytes(i);\n        get = new org.apache.hadoop.hbase.client.Get(bytes);\n        get.addFamily(FAMILIES[0]);\n        result = ht.get(get);\n        org.junit.Assert.assertTrue(result.size() == 0);\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] fam = org.apache.hadoop.hbase.util.Bytes.toBytes(\"info\");\n    byte[][] families = new byte[][]{ fam };\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, families);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"table_name\");\n    byte[] serverinfo = org.apache.hadoop.hbase.util.Bytes.toBytes(\"serverinfo\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(fam, serverinfo, org.apache.hadoop.hbase.HConstants.LATEST_TIMESTAMP - 5, org.apache.hadoop.hbase.util.Bytes.toBytes(\"value\"));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(row);\n    region.delete(delete, null, true);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row).addColumn(fam, serverinfo);\n    org.apache.hadoop.hbase.client.Result result = region.get(get, null);\n    junit.framework.Assert.assertEquals(1, result.size());\n    delete = new org.apache.hadoop.hbase.client.Delete(row, org.apache.hadoop.hbase.HConstants.LATEST_TIMESTAMP - 3, null);\n    region.delete(delete, null, true);\n    get = new org.apache.hadoop.hbase.client.Get(row).addColumn(fam, serverinfo);\n    result = region.get(get, null);\n    junit.framework.Assert.assertEquals(0, result.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDeleteRowWithFutureTs",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(family, qualifier);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertNResult(result, row, family, qualifier, stamps, values, start, end);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getAllVersionsAndVerify",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] fam = org.apache.hadoop.hbase.util.Bytes.toBytes(\"info\");\n    byte[][] families = new byte[][]{ fam };\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, families);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"table_name\");\n    byte[] serverinfo = org.apache.hadoop.hbase.util.Bytes.toBytes(\"serverinfo\");\n    byte[] splitA = org.apache.hadoop.hbase.util.Bytes.toBytes(\"splitA\");\n    byte[] splitB = org.apache.hadoop.hbase.util.Bytes.toBytes(\"splitB\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(fam, splitA, org.apache.hadoop.hbase.util.Bytes.toBytes(\"reference_A\"));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(fam, splitB, org.apache.hadoop.hbase.util.Bytes.toBytes(\"reference_B\"));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(fam, serverinfo, org.apache.hadoop.hbase.util.Bytes.toBytes(\"ip_address\"));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(row);\n    delete.deleteColumns(fam, splitA);\n    region.delete(delete, null, true);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row).addColumn(fam, serverinfo);\n    org.apache.hadoop.hbase.client.Result result = region.get(get, null);\n    junit.framework.Assert.assertEquals(1, result.size());\n    get = new org.apache.hadoop.hbase.client.Get(row).addColumn(fam, splitA);\n    result = region.get(get, null);\n    junit.framework.Assert.assertEquals(0, result.size());\n    get = new org.apache.hadoop.hbase.client.Get(row).addColumn(fam, splitB);\n    result = region.get(get, null);\n    junit.framework.Assert.assertEquals(1, result.size());\n    java.lang.Thread.sleep(10);\n    put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(fam, splitA, org.apache.hadoop.hbase.util.Bytes.toBytes(\"reference_A\"));\n    region.put(put);\n    get = new org.apache.hadoop.hbase.client.Get(row);\n    result = region.get(get, null);\n    junit.framework.Assert.assertEquals(3, result.size());\n    delete = new org.apache.hadoop.hbase.client.Delete(row);\n    region.delete(delete, null, false);\n    junit.framework.Assert.assertEquals(0, region.get(get, null).size());\n    java.lang.Thread.sleep(10);\n    region.put(new org.apache.hadoop.hbase.client.Put(row).add(fam, splitA, org.apache.hadoop.hbase.util.Bytes.toBytes(\"reference_A\")));\n    result = region.get(get, null);\n    junit.framework.Assert.assertEquals(1, result.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDelete_mixed",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 2,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 46,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 4
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[0]);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[4], QUALIFIERS[0], VALUES[0]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[2], QUALIFIERS[2]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[2], QUALIFIERS[2], VALUES[2]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(FAMILIES[7]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[7], QUALIFIERS[7], VALUES[7]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(FAMILIES[4]);\n    result = ht.get(get);\n    assertDoubleResult(result, ROWS[0], FAMILIES[4], QUALIFIERS[0], VALUES[0], FAMILIES[4], QUALIFIERS[4], VALUES[4]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[4]);\n    result = ht.get(get);\n    assertDoubleResult(result, ROWS[0], FAMILIES[4], QUALIFIERS[0], VALUES[0], FAMILIES[4], QUALIFIERS[4], VALUES[4]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(FAMILIES[4]);\n    get.addFamily(FAMILIES[7]);\n    result = ht.get(get);\n    assertNResult(result, ROWS[0], FAMILIES, QUALIFIERS, VALUES, new int[][]{ new int[]{ 4, 0, 0 }, new int[]{ 4, 4, 4 }, new int[]{ 7, 7, 7 } });\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(FAMILIES[2]);\n    get.addFamily(FAMILIES[4]);\n    get.addFamily(FAMILIES[6]);\n    get.addFamily(FAMILIES[7]);\n    result = ht.get(get);\n    assertNResult(result, ROWS[0], FAMILIES, QUALIFIERS, VALUES, new int[][]{ new int[]{ 2, 2, 2 }, new int[]{ 2, 4, 4 }, new int[]{ 4, 0, 0 }, new int[]{ 4, 4, 4 }, new int[]{ 6, 6, 6 }, new int[]{ 6, 7, 7 }, new int[]{ 7, 7, 7 } });\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[2], QUALIFIERS[2]);\n    get.addColumn(FAMILIES[2], QUALIFIERS[4]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[4]);\n    get.addColumn(FAMILIES[6], QUALIFIERS[6]);\n    get.addColumn(FAMILIES[6], QUALIFIERS[7]);\n    get.addColumn(FAMILIES[7], QUALIFIERS[7]);\n    get.addColumn(FAMILIES[7], QUALIFIERS[8]);\n    result = ht.get(get);\n    assertNResult(result, ROWS[0], FAMILIES, QUALIFIERS, VALUES, new int[][]{ new int[]{ 2, 2, 2 }, new int[]{ 2, 4, 4 }, new int[]{ 4, 0, 0 }, new int[]{ 4, 4, 4 }, new int[]{ 6, 6, 6 }, new int[]{ 6, 7, 7 }, new int[]{ 7, 7, 7 } });\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    result = ht.get(get);\n    assertNResult(result, ROWS[0], FAMILIES, QUALIFIERS, VALUES, new int[][]{ new int[]{ 2, 2, 2 }, new int[]{ 2, 4, 4 }, new int[]{ 4, 0, 0 }, new int[]{ 4, 4, 4 }, new int[]{ 6, 6, 6 }, new int[]{ 6, 7, 7 }, new int[]{ 7, 7, 7 }, new int[]{ 9, 0, 0 } });\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[1]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[3]);\n    get.addColumn(FAMILIES[2], QUALIFIERS[3]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "singleRowGetTest",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 57,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 4,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testDeletes",
        "NumberOfAsserts": 19,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 207,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    final int times = 100;\n    org.apache.hadoop.hbase.HColumnDescriptor fam1 = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam1\");\n    org.apache.hadoop.hbase.HColumnDescriptor fam2 = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam2\");\n    org.apache.hadoop.hbase.HColumnDescriptor fam3 = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam3\");\n    for (int i = 0; i < times; i++) {\n        org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(\"table\" + i);\n        htd.addFamily(fam1);\n        htd.addFamily(fam2);\n        htd.addFamily(fam3);\n        this.admin.createTable(htd);\n    }\n    for (int i = 0; i < times; i++) {\n        java.lang.String tableName = \"table\" + i;\n        this.admin.disableTable(tableName);\n        this.admin.enableTable(tableName);\n        this.admin.disableTable(tableName);\n        this.admin.deleteTable(tableName);\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(name);\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HBaseTestCase.fam1, versions, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_COMPRESSION, false, false, java.lang.Integer.MAX_VALUE, org.apache.hadoop.hbase.HConstants.FOREVER, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_BLOOMFILTER, org.apache.hadoop.hbase.HConstants.REPLICATION_SCOPE_LOCAL));\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HBaseTestCase.fam2, versions, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_COMPRESSION, false, false, java.lang.Integer.MAX_VALUE, org.apache.hadoop.hbase.HConstants.FOREVER, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_BLOOMFILTER, org.apache.hadoop.hbase.HConstants.REPLICATION_SCOPE_LOCAL));\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HBaseTestCase.fam3, versions, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_COMPRESSION, false, false, java.lang.Integer.MAX_VALUE, org.apache.hadoop.hbase.HConstants.FOREVER, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_BLOOMFILTER, org.apache.hadoop.hbase.HConstants.REPLICATION_SCOPE_LOCAL));\n    return htd;\n}",
            "ClassName": "HBaseTestCase",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableDescriptor",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    super.setUp();\n    client = new org.apache.hadoop.hbase.rest.client.Client(new org.apache.hadoop.hbase.rest.client.Cluster().add(\"localhost\", testServletPort));\n    admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    if (admin.tableExists(org.apache.hadoop.hbase.rest.TestRowResource.TABLE)) {\n        return;\n    }\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.rest.TestRowResource.TABLE);\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.KeyValue.parseColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1))[0]));\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.KeyValue.parseColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2))[0]));\n    admin.createTable(htd);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    int i = 0;\n    for (byte[] family : families) {\n        org.apache.hadoop.hbase.HColumnDescriptor hcd = new org.apache.hadoop.hbase.HColumnDescriptor(family, numVersions[i], org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_COMPRESSION, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_IN_MEMORY, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_BLOCKCACHE, java.lang.Integer.MAX_VALUE, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_TTL, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_BLOOMFILTER, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_REPLICATION_SCOPE);\n        desc.addFamily(hcd);\n        i++;\n    }\n    new org.apache.hadoop.hbase.client.HBaseAdmin(getConfiguration()).createTable(desc);\n    return new org.apache.hadoop.hbase.client.HTable(getConfiguration(), tableName);\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 11,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    super.setUp();\n    client = new org.apache.hadoop.hbase.rest.client.Client(new org.apache.hadoop.hbase.rest.client.Cluster().add(\"localhost\", testServletPort));\n    admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    if (admin.tableExists(org.apache.hadoop.hbase.rest.TestTableResource.TABLE)) {\n        return;\n    }\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.rest.TestTableResource.TABLE);\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.KeyValue.parseColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestTableResource.COLUMN))[0]));\n    admin.createTable(htd);\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.rest.TestTableResource.TABLE);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    for (byte[] family : families) {\n        htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(family));\n    }\n    org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(htd, null, null, false);\n    org.apache.hadoop.fs.Path path = new org.apache.hadoop.fs.Path(DIR + callingMethod);\n    region = org.apache.hadoop.hbase.regionserver.HRegion.createHRegion(info, path, conf);\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "initHRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 2,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testHundredsOfTable",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 20,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testJiraTest1014\");\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, 10);\n    long manualStamp = 12345;\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, manualStamp, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n    ht.put(put);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, manualStamp, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, manualStamp - 1);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, manualStamp + 1);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration());\n    admin.split(t.getTableName());\n    java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> regions = waitOnSplit(t);\n    org.junit.Assert.assertTrue(regions.size() > 1);\n    return regions;\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "splitTable",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testJiraTest1014",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 11,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.HColumnDescriptor fam1 = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam1\");\n    org.apache.hadoop.hbase.HColumnDescriptor fam2 = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam2\");\n    org.apache.hadoop.hbase.HColumnDescriptor fam3 = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam3\");\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(\"myTestTable\");\n    htd.addFamily(fam1);\n    htd.addFamily(fam2);\n    htd.addFamily(fam3);\n    this.admin.createTable(htd);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(\"myTestTable\");\n    org.apache.hadoop.hbase.HTableDescriptor confirmedHtd = table.getTableDescriptor();\n    org.junit.Assert.assertEquals(htd.compareTo(confirmedHtd), 0);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(name);\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HBaseTestCase.fam1, versions, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_COMPRESSION, false, false, java.lang.Integer.MAX_VALUE, org.apache.hadoop.hbase.HConstants.FOREVER, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_BLOOMFILTER, org.apache.hadoop.hbase.HConstants.REPLICATION_SCOPE_LOCAL));\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HBaseTestCase.fam2, versions, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_COMPRESSION, false, false, java.lang.Integer.MAX_VALUE, org.apache.hadoop.hbase.HConstants.FOREVER, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_BLOOMFILTER, org.apache.hadoop.hbase.HConstants.REPLICATION_SCOPE_LOCAL));\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HBaseTestCase.fam3, versions, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_COMPRESSION, false, false, java.lang.Integer.MAX_VALUE, org.apache.hadoop.hbase.HConstants.FOREVER, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_BLOOMFILTER, org.apache.hadoop.hbase.HConstants.REPLICATION_SCOPE_LOCAL));\n    return htd;\n}",
            "ClassName": "HBaseTestCase",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableDescriptor",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    super.setUp();\n    client = new org.apache.hadoop.hbase.rest.client.Client(new org.apache.hadoop.hbase.rest.client.Cluster().add(\"localhost\", testServletPort));\n    admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    if (admin.tableExists(org.apache.hadoop.hbase.rest.TestRowResource.TABLE)) {\n        return;\n    }\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.rest.TestRowResource.TABLE);\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.KeyValue.parseColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1))[0]));\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.KeyValue.parseColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2))[0]));\n    admin.createTable(htd);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    super.setUp();\n    testUtil = new org.apache.hadoop.hbase.HBaseTestingUtility();\n    testVals = makeTestVals();\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(getName());\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.filter.TestDependentColumnFilter.FAMILIES[0]));\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.filter.TestDependentColumnFilter.FAMILIES[1]));\n    org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(htd, null, null, false);\n    this.region = org.apache.hadoop.hbase.regionserver.HRegion.createHRegion(info, testUtil.getTestDir(), testUtil.getConfiguration());\n    addData();\n}",
            "ClassName": "TestDependentColumnFilter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 11,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    for (byte[] family : families) {\n        htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(family));\n    }\n    org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(htd, null, null, false);\n    org.apache.hadoop.fs.Path path = new org.apache.hadoop.fs.Path(DIR + callingMethod);\n    region = org.apache.hadoop.hbase.regionserver.HRegion.createHRegion(info, path, conf);\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "initHRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    int i = 0;\n    for (byte[] family : families) {\n        org.apache.hadoop.hbase.HColumnDescriptor hcd = new org.apache.hadoop.hbase.HColumnDescriptor(family, numVersions[i], org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_COMPRESSION, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_IN_MEMORY, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_BLOCKCACHE, java.lang.Integer.MAX_VALUE, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_TTL, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_BLOOMFILTER, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_REPLICATION_SCOPE);\n        desc.addFamily(hcd);\n        i++;\n    }\n    new org.apache.hadoop.hbase.client.HBaseAdmin(getConfiguration()).createTable(desc);\n    return new org.apache.hadoop.hbase.client.HTable(getConfiguration(), tableName);\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 11,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testGetTableDescriptor",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 13,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = new org.apache.hadoop.hbase.client.HTablePool(((org.apache.hadoop.hbase.HBaseConfiguration) (null)), java.lang.Integer.MAX_VALUE);\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testTable\");\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    junit.framework.Assert.assertNotNull(table);\n    pool.putTable(table);\n    org.apache.hadoop.hbase.client.HTableInterface sameTable = pool.getTable(tableName);\n    junit.framework.Assert.assertSame(table, sameTable);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    try {\n        return ((org.apache.hadoop.hbase.client.HTable) (table)).getRegionsInfo();\n    } finally {\n        pool.putTable(table);\n    }\n}",
            "ClassName": "RegionsResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    try {\n        return table.getTableDescriptor();\n    } finally {\n        pool.putTable(table);\n    }\n}",
            "ClassName": "SchemaResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableSchema",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return pool;\n}",
            "ClassName": "RESTServlet",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTablePool",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    servlet.getMetrics().incrementRequests(1);\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = null;\n    try {\n        java.util.List<org.apache.hadoop.hbase.rest.model.RowModel> rows = model.getRows();\n        table = pool.getTable(tableName);\n        ((org.apache.hadoop.hbase.client.HTable) (table)).setAutoFlush(false);\n        for (org.apache.hadoop.hbase.rest.model.RowModel row : rows) {\n            byte[] key = row.getKey();\n            org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(key);\n            for (org.apache.hadoop.hbase.rest.model.CellModel cell : row.getCells()) {\n                byte[][] parts = org.apache.hadoop.hbase.KeyValue.parseColumn(cell.getColumn());\n                if ((parts.length == 2) && (parts[1].length > 0)) {\n                    put.add(parts[0], parts[1], cell.getTimestamp(), cell.getValue());\n                } else {\n                    put.add(parts[0], null, cell.getTimestamp(), cell.getValue());\n                }\n            }\n            table.put(put);\n            if (org.apache.hadoop.hbase.rest.RowResource.LOG.isDebugEnabled()) {\n                org.apache.hadoop.hbase.rest.RowResource.LOG.debug(\"PUT \" + put.toString());\n            }\n        }\n        ((org.apache.hadoop.hbase.client.HTable) (table)).setAutoFlush(true);\n        table.flushCommits();\n        javax.ws.rs.core.Response.ResponseBuilder response = javax.ws.rs.core.Response.ok();\n        return response.build();\n    } catch (java.io.IOException e) {\n        throw new javax.ws.rs.WebApplicationException(e, Response.Status.SERVICE_UNAVAILABLE);\n    } finally {\n        if (table != null) {\n            pool.putTable(table);\n        }\n    }\n}",
            "ClassName": "RowResource",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "update",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 36,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHTablePool",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testTableWithByteArrayName",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 9,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] familyName = org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY;\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testForceSplit\");\n    final org.apache.hadoop.hbase.client.HTable table = org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.createTable(tableName, familyName);\n    byte[] k = new byte[3];\n    int rowCount = 0;\n    for (byte b1 = 'a'; b1 < 'z'; b1++) {\n        for (byte b2 = 'a'; b2 < 'z'; b2++) {\n            for (byte b3 = 'a'; b3 < 'z'; b3++) {\n                k[0] = b1;\n                k[1] = b2;\n                k[2] = b3;\n                org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(k);\n                put.add(familyName, new byte[0], k);\n                table.put(put);\n                rowCount++;\n            }\n        }\n    }\n    java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> m = table.getRegionsInfo();\n    java.lang.System.out.println(((\"Initial regions (\" + m.size()) + \"): \") + m);\n    org.junit.Assert.assertTrue(m.size() == 1);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.ResultScanner scanner = table.getScanner(scan);\n    int rows = 0;\n    for (@java.lang.SuppressWarnings(\"unused\")\n    org.apache.hadoop.hbase.client.Result result : scanner) {\n        rows++;\n    }\n    scanner.close();\n    org.junit.Assert.assertEquals(rowCount, rows);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scanner = table.getScanner(scan);\n    scanner.next();\n    final java.util.concurrent.atomic.AtomicInteger count = new java.util.concurrent.atomic.AtomicInteger(0);\n    java.lang.Thread t = new java.lang.Thread(\"CheckForSplit\") {\n        public void run() {\n            for (int i = 0; i < 20; i++) {\n                try {\n                    java.lang.Thread.sleep(1000);\n                } catch (java.lang.InterruptedException e) {\n                    continue;\n                }\n                java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> regions = null;\n                try {\n                    regions = table.getRegionsInfo();\n                } catch (java.io.IOException e) {\n                    e.printStackTrace();\n                }\n                if (regions == null)\n                    continue;\n\n                count.set(regions.size());\n                if (count.get() >= 2)\n                    break;\n\n                LOG.debug(\"Cycle waiting on split\");\n            }\n        }\n    };\n    t.start();\n    admin.split(org.apache.hadoop.hbase.util.Bytes.toString(tableName));\n    t.join();\n    rows = 1;\n    for (@java.lang.SuppressWarnings(\"unused\")\n    org.apache.hadoop.hbase.client.Result result : scanner) {\n        rows++;\n        if (rows > rowCount) {\n            scanner.close();\n            org.junit.Assert.assertTrue((\"Scanned more than expected (\" + rowCount) + \")\", false);\n        }\n    }\n    scanner.close();\n    org.junit.Assert.assertEquals(rowCount, rows);\n}",
        "CUT_1": {
            "Body": "{\n    byte[] k = new byte[3];\n    int rowCount = 0;\n    for (byte b1 = 'a'; b1 < 'z'; b1++) {\n        for (byte b2 = 'a'; b2 < 'z'; b2++) {\n            for (byte b3 = 'a'; b3 < 'z'; b3++) {\n                k[0] = b1;\n                k[1] = b2;\n                k[2] = b3;\n                org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(k);\n                put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, new byte[0], k);\n                t.put(put);\n                rowCount++;\n            }\n        }\n    }\n    return rowCount;\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "loadTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 18,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    java.lang.Runnable runnable = new java.lang.Runnable() {\n        public void run() {\n            try {\n                org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n                org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n                scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n                org.apache.hadoop.hbase.client.ResultScanner s = t.getScanner(scan);\n                s.close();\n            } catch (java.io.IOException e) {\n                LOG.fatal(\"could not re-open meta table because\", e);\n                junit.framework.Assert.fail();\n            }\n            org.apache.hadoop.hbase.client.ResultScanner scanner = null;\n            try {\n                org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n                scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n                scanner = table.getScanner(scan);\n                LOG.info(\"Obtained scanner \" + scanner);\n                for (org.apache.hadoop.hbase.client.Result r : scanner) {\n                    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(r.getRow(), row));\n                    junit.framework.Assert.assertEquals(1, r.size());\n                    byte[] bytes = r.value();\n                    junit.framework.Assert.assertNotNull(bytes);\n                    junit.framework.Assert.assertTrue(tableName.equals(org.apache.hadoop.hbase.util.Bytes.toString(bytes)));\n                }\n                LOG.info(\"Success!\");\n            } catch (java.lang.Exception e) {\n                e.printStackTrace();\n                junit.framework.Assert.fail();\n            } finally {\n                if (scanner != null) {\n                    LOG.info(\"Closing scanner \" + scanner);\n                    scanner.close();\n                }\n            }\n        }\n    };\n    return new java.lang.Thread(runnable);\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startVerificationThread",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 40,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_3": {
            "Body": "{\n    byte[] k = new byte[3];\n    int rowCount = 0;\n    for (byte b1 = 'a'; b1 <= 'z'; b1++) {\n        for (byte b2 = 'a'; b2 <= 'z'; b2++) {\n            for (byte b3 = 'a'; b3 <= 'z'; b3++) {\n                k[0] = b1;\n                k[1] = b2;\n                k[2] = b3;\n                org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(k);\n                put.add(f, null, k);\n                t.put(put);\n                rowCount++;\n            }\n        }\n    }\n    return rowCount;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "loadTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 18,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.util.TestMergeTool.FAMILY);\n    org.apache.hadoop.hbase.regionserver.InternalScanner scanner = merged.getScanner(scan);\n    try {\n        java.util.List<org.apache.hadoop.hbase.KeyValue> testRes = null;\n        while (true) {\n            testRes = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n            boolean hasNext = scanner.next(testRes);\n            if (!hasNext) {\n                break;\n            }\n        } \n    } finally {\n        scanner.close();\n    }\n    for (int i = 0; i < upperbound; i++) {\n        for (int j = 0; j < rows[i].length; j++) {\n            org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(rows[i][j]);\n            get.addFamily(org.apache.hadoop.hbase.util.TestMergeTool.FAMILY);\n            org.apache.hadoop.hbase.client.Result result = merged.get(get, null);\n            junit.framework.Assert.assertEquals(1, result.size());\n            byte[] bytes = result.sorted()[0].getValue();\n            junit.framework.Assert.assertNotNull(org.apache.hadoop.hbase.util.Bytes.toStringBinary(rows[i][j]), bytes);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(bytes, rows[i][j]));\n        }\n    }\n}",
            "ClassName": "TestMergeTool",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "verifyMerge",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 28,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(this.conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    java.util.List<byte[]> rows = new java.util.ArrayList<byte[]>();\n    org.apache.hadoop.hbase.client.ResultScanner s = t.getScanner(new org.apache.hadoop.hbase.client.Scan());\n    for (org.apache.hadoop.hbase.client.Result result : s) {\n        LOG.info(\"getMetaTableRows: row -> \" + org.apache.hadoop.hbase.util.Bytes.toStringBinary(result.getRow()));\n        rows.add(result.getRow());\n    }\n    s.close();\n    return rows;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getMetaTableRows",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 11,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 9,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testForceSplit",
        "NumberOfAsserts": 4,
        "NumberOfAsynchronousWaits": 1,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 75,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 17,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = new org.apache.hadoop.hbase.client.HTablePool(((org.apache.hadoop.hbase.HBaseConfiguration) (null)), java.lang.Integer.MAX_VALUE);\n    java.lang.String tableName1 = \"testTable1\";\n    java.lang.String tableName2 = \"testTable2\";\n    org.apache.hadoop.hbase.client.HTableInterface table1 = pool.getTable(tableName1);\n    org.apache.hadoop.hbase.client.HTableInterface table2 = pool.getTable(tableName2);\n    junit.framework.Assert.assertNotNull(table2);\n    pool.putTable(table1);\n    pool.putTable(table2);\n    org.apache.hadoop.hbase.client.HTableInterface sameTable1 = pool.getTable(tableName1);\n    org.apache.hadoop.hbase.client.HTableInterface sameTable2 = pool.getTable(tableName2);\n    junit.framework.Assert.assertSame(table1, sameTable1);\n    junit.framework.Assert.assertSame(table2, sameTable2);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    try {\n        return ((org.apache.hadoop.hbase.client.HTable) (table)).getRegionsInfo();\n    } finally {\n        pool.putTable(table);\n    }\n}",
            "ClassName": "RegionsResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    try {\n        return table.getTableDescriptor();\n    } finally {\n        pool.putTable(table);\n    }\n}",
            "ClassName": "SchemaResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableSchema",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return pool;\n}",
            "ClassName": "RESTServlet",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTablePool",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    servlet.getMetrics().incrementRequests(1);\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = null;\n    try {\n        java.util.List<org.apache.hadoop.hbase.rest.model.RowModel> rows = model.getRows();\n        table = pool.getTable(tableName);\n        ((org.apache.hadoop.hbase.client.HTable) (table)).setAutoFlush(false);\n        for (org.apache.hadoop.hbase.rest.model.RowModel row : rows) {\n            byte[] key = row.getKey();\n            org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(key);\n            for (org.apache.hadoop.hbase.rest.model.CellModel cell : row.getCells()) {\n                byte[][] parts = org.apache.hadoop.hbase.KeyValue.parseColumn(cell.getColumn());\n                if ((parts.length == 2) && (parts[1].length > 0)) {\n                    put.add(parts[0], parts[1], cell.getTimestamp(), cell.getValue());\n                } else {\n                    put.add(parts[0], null, cell.getTimestamp(), cell.getValue());\n                }\n            }\n            table.put(put);\n            if (org.apache.hadoop.hbase.rest.RowResource.LOG.isDebugEnabled()) {\n                org.apache.hadoop.hbase.rest.RowResource.LOG.debug(\"PUT \" + put.toString());\n            }\n        }\n        ((org.apache.hadoop.hbase.client.HTable) (table)).setAutoFlush(true);\n        table.flushCommits();\n        javax.ws.rs.core.Response.ResponseBuilder response = javax.ws.rs.core.Response.ok();\n        return response.build();\n    } catch (java.io.IOException e) {\n        throw new javax.ws.rs.WebApplicationException(e, Response.Status.SERVICE_UNAVAILABLE);\n    } finally {\n        if (table != null) {\n            pool.putTable(table);\n        }\n    }\n}",
            "ClassName": "RowResource",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "update",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 36,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    java.lang.String schemaPath = (\"/\" + org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2) + \"/schema\";\n    org.apache.hadoop.hbase.rest.model.TableSchemaModel model;\n    org.apache.hadoop.hbase.rest.client.Response response;\n    junit.framework.Assert.assertFalse(admin.tableExists(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2));\n    model = org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.buildTestModel(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2);\n    org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.checkModel(model, org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2);\n    response = client.put(schemaPath, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF, model.createProtobufOutput());\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    admin.enableTable(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2);\n    response = client.get(schemaPath, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    model = new org.apache.hadoop.hbase.rest.model.TableSchemaModel();\n    model.getObjectFromMessage(response.getBody());\n    org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.checkModel(model, org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2);\n    client.delete(schemaPath);\n    junit.framework.Assert.assertFalse(admin.tableExists(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2));\n}",
            "ClassName": "TestSchemaResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableCreateAndDeletePB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 18,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHTablePool",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testTablesWithDifferentNames",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 14,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testNull\");\n    try {\n        org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(null, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n        org.junit.Assert.fail(\"Creating a table with null name passed, should have failed\");\n    } catch (java.lang.Exception e) {\n    }\n    try {\n        org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, ((byte[]) (null)));\n        org.junit.Assert.fail(\"Creating a table with a null family passed, should fail\");\n    } catch (java.lang.Exception e) {\n    }\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    try {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(((byte[]) (null)));\n        put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n        ht.put(put);\n        org.junit.Assert.fail(\"Inserting a null row worked, should throw exception\");\n    } catch (java.lang.Exception e) {\n    }\n    {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n        put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, null, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n        ht.put(put);\n        getTestNull(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n        scanTestNull(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n        org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n        delete.deleteColumns(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, null);\n        ht.delete(delete);\n        org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n        org.apache.hadoop.hbase.client.Result result = ht.get(get);\n        assertEmptyResult(result);\n    }\n    byte[] TABLE2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testNull2\");\n    ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE2, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    try {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n        put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n        ht.put(put);\n        getTestNull(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n        scanTestNull(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n        org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n        getTestNull(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n        scanTestNull(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n        org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n        delete.deleteColumns(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY);\n        ht.delete(delete);\n        org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n        org.apache.hadoop.hbase.client.Result result = ht.get(get);\n        assertEmptyResult(result);\n    } catch (java.lang.Exception e) {\n        throw new java.io.IOException(\"Using a row with null qualifier threw exception, should \");\n    }\n    try {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n        put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, null);\n        ht.put(put);\n        org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n        get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n        org.apache.hadoop.hbase.client.Result result = ht.get(get);\n        assertSingleResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, null);\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n        result = getSingleScanResult(ht, scan);\n        assertSingleResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, null);\n        org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n        delete.deleteColumns(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n        ht.delete(delete);\n        get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n        result = ht.get(get);\n        assertEmptyResult(result);\n    } catch (java.lang.Exception e) {\n        throw new java.io.IOException(\"Null values should be allowed, but threw exception\");\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(c, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.Result res = t.get(get);\n    org.apache.hadoop.hbase.KeyValue[] kvs = res.raw();\n    if (kvs.length <= 0) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    byte[] value = kvs[0].getValue();\n    if (value == null) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(value);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    info.setOffline(onlineOffline);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(info));\n    t.put(put);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(row);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER);\n    t.delete(delete);\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "changeOnlineStatus",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(this.conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    java.util.ArrayList<org.apache.hadoop.hbase.client.Delete> deletes = new java.util.ArrayList<org.apache.hadoop.hbase.client.Delete>();\n    org.apache.hadoop.hbase.client.ResultScanner s = t.getScanner(new org.apache.hadoop.hbase.client.Scan());\n    for (org.apache.hadoop.hbase.client.Result result : s) {\n        LOG.info(\"emptyMetaTable: remove row -> \" + org.apache.hadoop.hbase.util.Bytes.toStringBinary(result.getRow()));\n        org.apache.hadoop.hbase.client.Delete del = new org.apache.hadoop.hbase.client.Delete(result.getRow());\n        deletes.add(del);\n    }\n    s.close();\n    t.delete(deletes);\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "emptyMetaTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testNull",
        "NumberOfAsserts": 3,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 75,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    java.util.Set<java.lang.String> deadServers = new java.util.HashSet<java.lang.String>();\n    final java.lang.String hostname123 = \"one,123,3\";\n    org.junit.Assert.assertFalse(org.apache.hadoop.hbase.master.ServerManager.isDead(deadServers, hostname123, false));\n    org.junit.Assert.assertFalse(org.apache.hadoop.hbase.master.ServerManager.isDead(deadServers, hostname123, true));\n    deadServers.add(hostname123);\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.master.ServerManager.isDead(deadServers, hostname123, false));\n    org.junit.Assert.assertFalse(org.apache.hadoop.hbase.master.ServerManager.isDead(deadServers, \"one:1\", true));\n    org.junit.Assert.assertFalse(org.apache.hadoop.hbase.master.ServerManager.isDead(deadServers, \"one:1234\", true));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.master.ServerManager.isDead(deadServers, \"one:123\", true));\n}",
        "CUT_1": {
            "Body": "{\n    this.deadServers = deadServers;\n}",
            "ClassName": "ClusterStatus",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setDeadServers",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return this.deadServers;\n}",
            "ClassName": "ServerManager",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getDeadServers",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return deadServers.size();\n}",
            "ClassName": "ClusterStatus",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getDeadServers",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    this.deadServers.remove(serverName);\n}",
            "ClassName": "ServerManager",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "removeDeadServer",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return this.serverManager;\n}",
            "ClassName": "HMaster",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getServerManager",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestServerManager",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testIsDead",
        "NumberOfAsserts": 6,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 11,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    try {\n        org.apache.hadoop.hbase.client.HTable localMeta = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n        org.apache.hadoop.conf.Configuration otherConf = org.apache.hadoop.hbase.HBaseConfiguration.create(conf);\n        otherConf.set(org.apache.hadoop.hbase.HConstants.ZOOKEEPER_QUORUM, \"127.0.0.1\");\n        org.apache.hadoop.hbase.client.HTable ipMeta = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n        localMeta.exists(new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.HConstants.LAST_ROW));\n        ipMeta.exists(new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.HConstants.LAST_ROW));\n        org.junit.Assert.assertFalse(org.apache.hadoop.hbase.client.HConnectionManager.getClientZooKeeperWatcher(conf).getZooKeeperWrapper() == org.apache.hadoop.hbase.client.HConnectionManager.getClientZooKeeperWatcher(otherConf).getZooKeeperWrapper());\n        org.junit.Assert.assertFalse(org.apache.hadoop.hbase.client.HConnectionManager.getConnection(conf).getZooKeeperWrapper().getQuorumServers().equals(org.apache.hadoop.hbase.client.HConnectionManager.getConnection(otherConf).getZooKeeperWrapper().getQuorumServers()));\n    } catch (java.lang.Exception e) {\n        e.printStackTrace();\n        org.junit.Assert.fail();\n    }\n}",
        "CUT_1": {
            "Body": "{\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    this.server = cluster.getRegionServerThreads().get(0).getRegionServer();\n    this.log = server.getLog();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    for (int i = 1; i <= 256; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + java.lang.String.format(\"%1$04d\", i)));\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, value);\n        table.put(put);\n        if ((i % 32) == 0) {\n            try {\n                java.lang.Thread.sleep(2000);\n            } catch (java.lang.InterruptedException e) {\n            }\n        }\n    }\n}",
            "ClassName": "TestLogRolling",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startAndWriteData",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    return ((org.apache.hadoop.hbase.client.ServerConnection) (org.apache.hadoop.hbase.client.HConnectionManager.getConnection(conf)));\n}",
            "ClassName": "ServerConnectionManager",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getConnection",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        org.apache.hadoop.hbase.HRegionInfo hri = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(b);\n        byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(row);\n        p.add(org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily(), org.apache.hadoop.hbase.master.TestMasterTransitions.getTestQualifier(), row);\n        t.put(p);\n        rows++;\n    }\n    s.close();\n    org.junit.Assert.assertEquals(expected, rows);\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addToEachStartKey",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        rows++;\n    }\n    s.close();\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "countOfMetaRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(c, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.Result res = t.get(get);\n    org.apache.hadoop.hbase.KeyValue[] kvs = res.raw();\n    if (kvs.length <= 0) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    byte[] value = kvs[0].getValue();\n    if (value == null) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(value);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    info.setOffline(onlineOffline);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(info));\n    t.put(put);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(row);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER);\n    t.delete(delete);\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "changeOnlineStatus",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestZooKeeper",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testMultipleZK",
        "NumberOfAsserts": 3,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 15,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    final byte[] tableAname = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testMiscHTableStuffA\");\n    final byte[] tableBname = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testMiscHTableStuffB\");\n    final byte[] attrName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"TESTATTR\");\n    final byte[] attrValue = org.apache.hadoop.hbase.util.Bytes.toBytes(\"somevalue\");\n    byte[] value = org.apache.hadoop.hbase.util.Bytes.toBytes(\"value\");\n    org.apache.hadoop.hbase.client.HTable a = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(tableAname, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.apache.hadoop.hbase.client.HTable b = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(tableBname, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, value);\n    a.put(put);\n    org.apache.hadoop.hbase.client.HTable newA = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration(), tableAname);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.apache.hadoop.hbase.client.ResultScanner s = newA.getScanner(scan);\n    try {\n        for (org.apache.hadoop.hbase.client.Result r : s) {\n            put = new org.apache.hadoop.hbase.client.Put(r.getRow());\n            for (org.apache.hadoop.hbase.KeyValue kv : r.sorted()) {\n                put.add(kv);\n            }\n            b.put(put);\n        }\n    } finally {\n        s.close();\n    }\n    org.apache.hadoop.hbase.client.HTable anotherA = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration(), tableAname);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    anotherA.get(get);\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration());\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(a.getTableDescriptor());\n    admin.disableTable(tableAname);\n    desc.setValue(attrName, attrValue);\n    for (org.apache.hadoop.hbase.HColumnDescriptor c : desc.getFamilies())\n        c.setValue(attrName, attrValue);\n\n    admin.modifyTable(tableAname, org.apache.hadoop.hbase.HConstants.Modify.TABLE_SET_HTD, desc);\n    admin.enableTable(tableAname);\n    desc = a.getTableDescriptor();\n    org.junit.Assert.assertTrue(\"wrong table descriptor returned\", org.apache.hadoop.hbase.util.Bytes.compareTo(desc.getName(), tableAname) == 0);\n    value = desc.getValue(attrName);\n    org.junit.Assert.assertFalse(\"missing HTD attribute value\", value == null);\n    org.junit.Assert.assertFalse(\"HTD attribute value is incorrect\", org.apache.hadoop.hbase.util.Bytes.compareTo(value, attrValue) != 0);\n    for (org.apache.hadoop.hbase.HColumnDescriptor c : desc.getFamilies()) {\n        value = c.getValue(attrName);\n        org.junit.Assert.assertFalse(\"missing HCD attribute value\", value == null);\n        org.junit.Assert.assertFalse(\"HCD attribute value is incorrect\", org.apache.hadoop.hbase.util.Bytes.compareTo(value, attrValue) != 0);\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    this.server = cluster.getRegionServerThreads().get(0).getRegionServer();\n    this.log = server.getLog();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    for (int i = 1; i <= 256; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + java.lang.String.format(\"%1$04d\", i)));\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, value);\n        table.put(put);\n        if ((i % 32) == 0) {\n            try {\n                java.lang.Thread.sleep(2000);\n            } catch (java.lang.InterruptedException e) {\n            }\n        }\n    }\n}",
            "ClassName": "TestLogRolling",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startAndWriteData",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(c, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.Result res = t.get(get);\n    org.apache.hadoop.hbase.KeyValue[] kvs = res.raw();\n    if (kvs.length <= 0) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    byte[] value = kvs[0].getValue();\n    if (value == null) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(value);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    info.setOffline(onlineOffline);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(info));\n    t.put(put);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(row);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER);\n    t.delete(delete);\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "changeOnlineStatus",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        org.apache.hadoop.hbase.HRegionInfo hri = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(b);\n        byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(row);\n        p.add(org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily(), org.apache.hadoop.hbase.master.TestMasterTransitions.getTestQualifier(), row);\n        t.put(p);\n        rows++;\n    }\n    s.close();\n    org.junit.Assert.assertEquals(expected, rows);\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addToEachStartKey",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 4,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testMiscHTableStuff",
        "NumberOfAsserts": 5,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 50,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    testScan(\"yyx\", null, \"zzz\");\n}",
        "CUT_1": {
            "Body": "{\n    return ((x == null) && (y == null)) || ((x != null) && x.equals(y));\n}",
            "ClassName": "Pair",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "equals",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "internalPreFlushcacheCommit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return null;\n}",
            "ClassName": "WildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return null;\n}",
            "ClassName": "ScanWildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HalfStoreFileReader",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "midkey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableInputFormatScan",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testScanYYXToEmpty",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration(), \"testTableNotFoundExceptionWithoutAnyTables\");\n}",
        "CUT_1": {
            "Body": "{\n    this.admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration());\n}",
            "ClassName": "TestAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.msginterval\", 100);\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration().setInt(\"hbase.client.pause\", 250);\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration().setInt(\"hbase.client.retries.number\", 6);\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.Get g = new org.apache.hadoop.hbase.client.Get(row);\n    org.junit.Assert.assertTrue(t.get(g).size() > 0);\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "assertRegionIsBackOnline",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        org.apache.hadoop.hbase.HRegionInfo hri = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(b);\n        byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(row);\n        p.add(org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily(), org.apache.hadoop.hbase.master.TestMasterTransitions.getTestQualifier(), row);\n        t.put(p);\n        rows++;\n    }\n    s.close();\n    org.junit.Assert.assertEquals(expected, rows);\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addToEachStartKey",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testTableNotFoundExceptionWithoutAnyTables",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testMasterAdmin\");\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.createTable(tableName, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    this.admin.disableTable(tableName);\n    try {\n        new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration(), tableName);\n    } catch (org.apache.hadoop.hbase.client.RegionOfflineException e) {\n    }\n    this.admin.addColumn(tableName, new org.apache.hadoop.hbase.HColumnDescriptor(\"col2\"));\n    this.admin.enableTable(tableName);\n    try {\n        this.admin.deleteColumn(tableName, org.apache.hadoop.hbase.util.Bytes.toBytes(\"col2\"));\n    } catch (org.apache.hadoop.hbase.TableNotDisabledException e) {\n    }\n    this.admin.disableTable(tableName);\n    this.admin.deleteColumn(tableName, org.apache.hadoop.hbase.util.Bytes.toBytes(\"col2\"));\n    this.admin.deleteTable(tableName);\n}",
        "CUT_1": {
            "Body": "{\n    this.admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration());\n}",
            "ClassName": "TestAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    disableTable(org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n}",
            "ClassName": "HBaseAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "disableTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return this.tableName;\n}",
            "ClassName": "HTable",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableName",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testEnableDisableAddColumnDeleteColumn",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 18,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor[] tables = admin.listTables();\n    int numTables = tables.length;\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.createTable(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testCreateTable\"), org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    tables = this.admin.listTables();\n    org.junit.Assert.assertEquals(numTables + 1, tables.length);\n}",
        "CUT_1": {
            "Body": "{\n    this.tables = tables;\n}",
            "ClassName": "TableListModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setTables",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return tables;\n}",
            "ClassName": "TableListModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTables",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    tables.add(table);\n}",
            "ClassName": "TableListModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "add",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return tables.get(index);\n}",
            "ClassName": "TableListModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "get",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    boolean found = false;\n    java.util.Iterator<org.apache.hadoop.hbase.rest.model.TableModel> tables = model.getTables().iterator();\n    junit.framework.Assert.assertTrue(tables.hasNext());\n    while (tables.hasNext()) {\n        org.apache.hadoop.hbase.rest.model.TableModel table = tables.next();\n        if (table.getName().equals(org.apache.hadoop.hbase.rest.TestTableResource.TABLE)) {\n            found = true;\n            break;\n        }\n    } \n    junit.framework.Assert.assertTrue(found);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "checkTableList",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 13,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testCreateTable",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 7,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    testScan(\"yzy\", null, \"zzz\");\n}",
        "CUT_1": {
            "Body": "{\n    return ((x == null) && (y == null)) || ((x != null) && x.equals(y));\n}",
            "ClassName": "Pair",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "equals",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "internalPreFlushcacheCommit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return null;\n}",
            "ClassName": "WildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return null;\n}",
            "ClassName": "ScanWildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HalfStoreFileReader",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "midkey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableInputFormatScan",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testScanYZYToEmpty",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    final byte[] tableAname = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testGetClosestRowBefore\");\n    final byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\");\n    byte[] firstRow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"ro\");\n    byte[] beforeFirstRow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"rn\");\n    byte[] beforeSecondRow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"rov\");\n    org.apache.hadoop.hbase.client.HTable table = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(tableAname, new byte[][]{ org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.util.Bytes.toBytes(\"info2\") });\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(firstRow);\n    org.apache.hadoop.hbase.client.Put put2 = new org.apache.hadoop.hbase.client.Put(row);\n    byte[] zero = new byte[]{ 0 };\n    byte[] one = new byte[]{ 1 };\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, zero);\n    put2.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, one);\n    table.put(put);\n    table.put(put2);\n    org.apache.hadoop.hbase.client.Result result = null;\n    result = table.getRowOrBefore(beforeFirstRow, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.junit.Assert.assertTrue(result == null);\n    result = table.getRowOrBefore(firstRow, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.junit.Assert.assertTrue(result.containsColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(result.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null), zero));\n    result = table.getRowOrBefore(beforeSecondRow, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.junit.Assert.assertTrue(result.containsColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(result.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null), zero));\n    result = table.getRowOrBefore(row, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.junit.Assert.assertTrue(result.containsColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(result.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null), one));\n    result = table.getRowOrBefore(org.apache.hadoop.hbase.util.Bytes.add(row, one), org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.junit.Assert.assertTrue(result.containsColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(result.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null), one));\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(c, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.Result res = t.get(get);\n    org.apache.hadoop.hbase.KeyValue[] kvs = res.raw();\n    if (kvs.length <= 0) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    byte[] value = kvs[0].getValue();\n    if (value == null) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(value);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    info.setOffline(onlineOffline);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(info));\n    t.put(put);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(row);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER);\n    t.delete(delete);\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "changeOnlineStatus",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    final int COUNT = 5;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(getName());\n    for (int i = 0; i < COUNT; i++) {\n        byte[] regionName = org.apache.hadoop.hbase.HRegionInfo.createRegionName(tableName, org.apache.hadoop.hbase.util.Bytes.toBytes(i == 0 ? \"\" : java.lang.Integer.toString(i)), java.lang.Long.toString(java.lang.System.currentTimeMillis()));\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(regionName);\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER, org.apache.hadoop.hbase.util.Bytes.toBytes(\"localhost:1234\"));\n        t.put(put);\n    }\n    long sleepTime = conf.getLong(\"hbase.master.meta.thread.rescanfrequency\", 10000);\n    int tries = conf.getInt(\"hbase.client.retries.number\", 5);\n    int count = 0;\n    do {\n        tries -= 1;\n        try {\n            java.lang.Thread.sleep(sleepTime);\n        } catch (java.lang.InterruptedException e) {\n        }\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITA_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITB_QUALIFIER);\n        org.apache.hadoop.hbase.client.ResultScanner scanner = t.getScanner(scan);\n        try {\n            count = 0;\n            org.apache.hadoop.hbase.client.Result r;\n            while ((r = scanner.next()) != null) {\n                if (!r.isEmpty()) {\n                    count += 1;\n                }\n            } \n        } finally {\n            scanner.close();\n        }\n    } while ((count != 0) && (tries >= 0) );\n    junit.framework.Assert.assertTrue(tries >= 0);\n    junit.framework.Assert.assertEquals(0, count);\n}",
            "ClassName": "TestEmptyMetaInfo",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testEmptyMetaInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 41,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress>> result = new java.util.ArrayList<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress>>();\n    java.util.Set<org.apache.hadoop.hbase.master.MetaRegion> regions = this.regionManager.getMetaRegionsForTable(tableName);\n    byte[] firstRowInTable = org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.util.Bytes.toString(tableName) + \",,\");\n    for (org.apache.hadoop.hbase.master.MetaRegion m : regions) {\n        byte[] metaRegionName = m.getRegionName();\n        org.apache.hadoop.hbase.ipc.HRegionInterface srvr = this.connection.getHRegionConnection(m.getServer());\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(firstRowInTable);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        long scannerid = srvr.openScanner(metaRegionName, scan);\n        try {\n            while (true) {\n                org.apache.hadoop.hbase.client.Result data = srvr.next(scannerid);\n                if ((data == null) || (data.size() <= 0))\n                    break;\n\n                org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(data.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER));\n                if (org.apache.hadoop.hbase.util.Bytes.equals(info.getTableDesc().getName(), tableName)) {\n                    byte[] value = data.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n                    if (value != null) {\n                        org.apache.hadoop.hbase.HServerAddress server = new org.apache.hadoop.hbase.HServerAddress(org.apache.hadoop.hbase.util.Bytes.toString(value));\n                        result.add(new org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress>(info, server));\n                    }\n                } else {\n                    break;\n                }\n            } \n        } finally {\n            srvr.close(scannerid);\n        }\n    }\n    return result;\n}",
            "ClassName": "HMaster",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 34,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.HRegionInfo.parseRegionName(regionName)[0];\n    java.util.Set<org.apache.hadoop.hbase.master.MetaRegion> regions = regionManager.getMetaRegionsForTable(tableName);\n    for (org.apache.hadoop.hbase.master.MetaRegion m : regions) {\n        byte[] metaRegionName = m.getRegionName();\n        org.apache.hadoop.hbase.ipc.HRegionInterface srvr = connection.getHRegionConnection(m.getServer());\n        org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(regionName);\n        get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        org.apache.hadoop.hbase.client.Result data = srvr.get(metaRegionName, get);\n        if ((data == null) || (data.size() <= 0))\n            continue;\n\n        org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(data.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER));\n        byte[] value = data.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        if (value != null) {\n            org.apache.hadoop.hbase.HServerAddress server = new org.apache.hadoop.hbase.HServerAddress(org.apache.hadoop.hbase.util.Bytes.toString(value));\n            return new org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress>(info, server);\n        }\n    }\n    return null;\n}",
            "ClassName": "HMaster",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableRegionFromName",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testGetClosestRowBefore",
        "NumberOfAsserts": 9,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 31,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.HTable table = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testGet_EmptyTable\"), org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.apache.hadoop.hbase.client.Result r = table.get(get);\n    org.junit.Assert.assertTrue(r.isEmpty());\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.regionserver.TestScanner.ROW_KEY);\n    get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.Result result = region.get(get, null);\n    byte[] bytes = result.value();\n    validateRegionInfo(bytes);\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getRegionInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.Get g = new org.apache.hadoop.hbase.client.Get(row);\n    org.junit.Assert.assertTrue(t.get(g).size() > 0);\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "assertRegionIsBackOnline",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testGet_EmptyTable",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 7,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testVersions\");\n    long[] STAMPS = makeStamps(20);\n    byte[][] VALUES = makeNAscii(org.apache.hadoop.hbase.client.TestFromClientSide.VALUE, 20);\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, 10);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    ht.put(put);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6]);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(2);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[4], STAMPS[5] }, new byte[][]{ VALUES[4], VALUES[5] }, 0, 1);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(2);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[4], STAMPS[5] }, new byte[][]{ VALUES[4], VALUES[5] }, 0, 1);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6]);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(2);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[4], STAMPS[5] }, new byte[][]{ VALUES[4], VALUES[5] }, 0, 1);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(2);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[4], STAMPS[5] }, new byte[][]{ VALUES[4], VALUES[5] }, 0, 1);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3], VALUES[3]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6], VALUES[6]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[7], VALUES[7]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[8], VALUES[8]);\n    ht.put(put);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions();\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1], STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[7], STAMPS[8] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3], VALUES[4], VALUES[5], VALUES[6], VALUES[7], VALUES[8] }, 0, 7);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions();\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1], STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[7], STAMPS[8] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3], VALUES[4], VALUES[5], VALUES[6], VALUES[7], VALUES[8] }, 0, 7);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.setMaxVersions();\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1], STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[7], STAMPS[8] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3], VALUES[4], VALUES[5], VALUES[6], VALUES[7], VALUES[8] }, 0, 7);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.setMaxVersions();\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1], STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[7], STAMPS[8] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3], VALUES[4], VALUES[5], VALUES[6], VALUES[7], VALUES[8] }, 0, 7);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[7], VALUES[7]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[7], VALUES[7]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[9]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[9]);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[9], VALUES[9]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[11], VALUES[11]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[13], VALUES[13]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[15], VALUES[15]);\n    ht.put(put);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[7], STAMPS[8], STAMPS[9], STAMPS[11], STAMPS[13], STAMPS[15] }, new byte[][]{ VALUES[3], VALUES[4], VALUES[5], VALUES[6], VALUES[7], VALUES[8], VALUES[9], VALUES[11], VALUES[13], VALUES[15] }, 0, 9);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[7], STAMPS[8], STAMPS[9], STAMPS[11], STAMPS[13], STAMPS[15] }, new byte[][]{ VALUES[3], VALUES[4], VALUES[5], VALUES[6], VALUES[7], VALUES[8], VALUES[9], VALUES[11], VALUES[13], VALUES[15] }, 0, 9);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    delete.deleteColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[11]);\n    delete.deleteColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[7]);\n    ht.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1], STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[8], STAMPS[9], STAMPS[13], STAMPS[15] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3], VALUES[4], VALUES[5], VALUES[6], VALUES[8], VALUES[9], VALUES[13], VALUES[15] }, 0, 9);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1], STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[8], STAMPS[9], STAMPS[13], STAMPS[15] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3], VALUES[4], VALUES[5], VALUES[6], VALUES[8], VALUES[9], VALUES[13], VALUES[15] }, 0, 9);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(family, qualifier);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    get.setTimeRange(stamps[start], stamps[end] + 1);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertNResult(result, row, family, qualifier, stamps, values, start, end);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getVersionRangeAndVerify",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row);\n    scan.addColumn(family, qualifier);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    scan.setTimeRange(stamps[start], stamps[end] + 1);\n    org.apache.hadoop.hbase.client.Result result = getSingleScanResult(ht, scan);\n    assertNResult(result, row, family, qualifier, stamps, values, start, end);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "scanVersionRangeAndVerify",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testVersions",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 128,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testSimpleMissing\");\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    byte[][] ROWS = makeN(org.apache.hadoop.hbase.client.TestFromClientSide.ROW, 4);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[0]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[0], ROWS[1]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(ROWS[2]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n    ht.put(put);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[1]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[3]);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[3]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[0], ROWS[2]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[2]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[2], org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[2]);\n    get.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[2], org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[2]);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[2], org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[2], org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[0], ROWS[3]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[2], org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[2], ROWS[3]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[2], org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(ROWS[ROWIDX + 1]);\n    org.apache.hadoop.hbase.client.Result result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[ROWIDX + 1], ROWS[ROWIDX + 2]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, ROWS[ROWIDX]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX + 1]);\n    scan.addFamily(FAMILIES[FAMILYIDX - 1]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "scanVerifySingleEmpty",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(family, null);\n    org.apache.hadoop.hbase.client.Result result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, row, family, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY, value);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(family, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, row, family, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY, value);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(family);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, row, family, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY, value);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, row, family, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY, value);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "scanTestNull",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 17,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row);\n    scan.addColumn(family, qualifier);\n    scan.setTimeStamp(stamp);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    org.apache.hadoop.hbase.client.Result result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "scanVersionAndVerifyMissing",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(this.conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    java.util.List<byte[]> rows = new java.util.ArrayList<byte[]>();\n    org.apache.hadoop.hbase.client.ResultScanner s = t.getScanner(new org.apache.hadoop.hbase.client.Scan());\n    for (org.apache.hadoop.hbase.client.Result result : s) {\n        LOG.info(\"getMetaTableRows: row -> \" + org.apache.hadoop.hbase.util.Bytes.toStringBinary(result.getRow()));\n        rows.add(result.getRow());\n    }\n    s.close();\n    return rows;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getMetaTableRows",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 11,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testSimpleMissing",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 73,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    this.admin.createTable(new org.apache.hadoop.hbase.HTableDescriptor());\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(getName());\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.client.TestTimestamp.COLUMN_NAME));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    return new org.apache.hadoop.hbase.client.HTable(conf, getName());\n}",
            "ClassName": "TestTimestamp",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.HBaseConfiguration.create();\n    org.apache.hadoop.hbase.LocalHBaseCluster cluster = new org.apache.hadoop.hbase.LocalHBaseCluster(conf);\n    cluster.startup();\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.util.Bytes.toBytes(cluster.getClass().getName()));\n    admin.createTable(htd);\n    cluster.shutdown();\n}",
            "ClassName": "LocalHBaseCluster",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "main",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    super.setUp();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.client.TestGetRowVersions.TABLE_NAME);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.client.TestGetRowVersions.CONTENTS));\n    this.admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    this.admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.client.TestGetRowVersions.TABLE_NAME);\n}",
            "ClassName": "TestGetRowVersions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    super.setUp();\n    client = new org.apache.hadoop.hbase.rest.client.Client(new org.apache.hadoop.hbase.rest.client.Cluster().add(\"localhost\", testServletPort));\n    admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    if (admin.tableExists(org.apache.hadoop.hbase.rest.TestTableResource.TABLE)) {\n        return;\n    }\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.rest.TestTableResource.TABLE);\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.KeyValue.parseColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestTableResource.COLUMN))[0]));\n    admin.createTable(htd);\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.rest.TestTableResource.TABLE);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testEmptyHHTableDescriptor",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    final byte[] CONTENTS_FAMILY = org.apache.hadoop.hbase.util.Bytes.toBytes(\"contents\");\n    final byte[] SMALL_FAMILY = org.apache.hadoop.hbase.util.Bytes.toBytes(\"smallfam\");\n    final byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    final byte[] row2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row2\");\n    final byte[] value = org.apache.hadoop.hbase.util.Bytes.toBytes(\"abcd\");\n    org.apache.hadoop.hbase.client.HTable table = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testPut\"), new byte[][]{ CONTENTS_FAMILY, SMALL_FAMILY });\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(CONTENTS_FAMILY, null, value);\n    table.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row2);\n    put.add(CONTENTS_FAMILY, null, value);\n    org.junit.Assert.assertEquals(put.size(), 1);\n    org.junit.Assert.assertEquals(put.getFamilyMap().get(CONTENTS_FAMILY).size(), 1);\n    org.apache.hadoop.hbase.KeyValue kv = put.getFamilyMap().get(CONTENTS_FAMILY).get(0);\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(kv.getFamily(), CONTENTS_FAMILY));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(kv.getQualifier(), new byte[0]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(kv.getValue(), value));\n    table.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(CONTENTS_FAMILY, null);\n    org.apache.hadoop.hbase.client.ResultScanner scanner = table.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r : scanner) {\n        for (org.apache.hadoop.hbase.KeyValue key : r.sorted()) {\n            java.lang.System.out.println((org.apache.hadoop.hbase.util.Bytes.toString(r.getRow()) + \": \") + key.toString());\n        }\n    }\n}",
        "CUT_1": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"test_table\");\n    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testFamily\");\n    initHRegion(tableName, getName(), family);\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row111\");\n    byte[] row2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row222\");\n    byte[] row3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row333\");\n    byte[] row4 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row444\");\n    byte[] row5 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row555\");\n    byte[] col1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub111\");\n    byte[] col2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub222\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(10L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row2);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(15L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row3);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(20L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row4);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(30L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row5);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(40L));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row3, row4);\n    scan.setMaxVersions();\n    scan.addColumn(family, col1);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    junit.framework.Assert.assertEquals(false, s.next(results));\n    junit.framework.Assert.assertEquals(0, results.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_StopRow1542",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 34,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"test_table\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"columnA\");\n    byte[] fam2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"columnB\");\n    initHRegion(tableName, getName(), fam1, fam2);\n    byte[] rowA = org.apache.hadoop.hbase.util.Bytes.toBytes(\"rowA\");\n    byte[] rowB = org.apache.hadoop.hbase.util.Bytes.toBytes(\"rowB\");\n    byte[] value = org.apache.hadoop.hbase.util.Bytes.toBytes(\"value\");\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(rowA);\n    delete.deleteFamily(fam1);\n    region.delete(delete, null, true);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(rowA);\n    put.add(fam2, null, value);\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(rowB);\n    put.add(fam1, null, value);\n    put.add(fam2, null, value);\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(fam1).addFamily(fam2);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    s.next(results);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(rowA, results.get(0).getRow()));\n    results.clear();\n    s.next(results);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(rowB, results.get(0).getRow()));\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_DeleteOneFamilyNotAnother",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 28,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam1\");\n    byte[] fam2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam2\");\n    byte[] qf1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"qualifier\");\n    byte[] val1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"value1\");\n    byte[] val2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"value2\");\n    java.lang.Integer lockId = null;\n    byte[][] families = new byte[][]{ fam1, fam2 };\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, families);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(fam1, qf1, val1);\n    region.put(put);\n    long ts = java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.KeyValue kv = new org.apache.hadoop.hbase.KeyValue(row1, fam2, qf1, ts, org.apache.hadoop.hbase.KeyValue.Type.Put, val2);\n    put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(kv);\n    org.apache.hadoop.hbase.regionserver.Store store = region.getStore(fam1);\n    store.memstore.kvset.size();\n    boolean res = region.checkAndPut(row1, fam1, qf1, val1, put, lockId, true);\n    junit.framework.Assert.assertEquals(true, res);\n    store.memstore.kvset.size();\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row1);\n    get.addColumn(fam2, qf1);\n    org.apache.hadoop.hbase.KeyValue[] actual = region.get(get, null).raw();\n    org.apache.hadoop.hbase.KeyValue[] expected = new org.apache.hadoop.hbase.KeyValue[]{ kv };\n    junit.framework.Assert.assertEquals(expected.length, actual.length);\n    for (int i = 0; i < actual.length; i++) {\n        junit.framework.Assert.assertEquals(expected[i], actual[i]);\n    }\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testCheckAndPut_ThatPutWasWritten",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 33,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 2,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testPut",
        "NumberOfAsserts": 5,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 28,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    final byte[] FAM1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam1\");\n    final byte[] FAM2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam2\");\n    org.apache.hadoop.hbase.client.HTable table = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testHBase737\"), new byte[][]{ FAM1, FAM2 });\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(FAM1, org.apache.hadoop.hbase.util.Bytes.toBytes(\"letters\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"abcdefg\"));\n    table.put(put);\n    try {\n        java.lang.Thread.sleep(1000);\n    } catch (java.lang.InterruptedException i) {\n    }\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(FAM1, org.apache.hadoop.hbase.util.Bytes.toBytes(\"numbers\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"123456\"));\n    table.put(put);\n    try {\n        java.lang.Thread.sleep(1000);\n    } catch (java.lang.InterruptedException i) {\n    }\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(FAM2, org.apache.hadoop.hbase.util.Bytes.toBytes(\"letters\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"hijklmnop\"));\n    table.put(put);\n    long[] times = new long[3];\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(FAM1);\n    scan.addFamily(FAM2);\n    org.apache.hadoop.hbase.client.ResultScanner s = table.getScanner(scan);\n    try {\n        int index = 0;\n        org.apache.hadoop.hbase.client.Result r = null;\n        while ((r = s.next()) != null) {\n            for (org.apache.hadoop.hbase.KeyValue key : r.sorted()) {\n                times[index++] = key.getTimestamp();\n            }\n        } \n    } finally {\n        s.close();\n    }\n    for (int i = 0; i < (times.length - 1); i++) {\n        for (int j = i + 1; j < times.length; j++) {\n            org.junit.Assert.assertTrue(times[j] > times[i]);\n        }\n    }\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    for (int i = 0; i < times.length; i++) {\n        times[i] = 0;\n    }\n    try {\n        java.lang.Thread.sleep(1000);\n    } catch (java.lang.InterruptedException i) {\n    }\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(FAM1);\n    scan.addFamily(FAM2);\n    s = table.getScanner(scan);\n    try {\n        int index = 0;\n        org.apache.hadoop.hbase.client.Result r = null;\n        while ((r = s.next()) != null) {\n            for (org.apache.hadoop.hbase.KeyValue key : r.sorted()) {\n                times[index++] = key.getTimestamp();\n            }\n        } \n    } finally {\n        s.close();\n    }\n    for (int i = 0; i < (times.length - 1); i++) {\n        for (int j = i + 1; j < times.length; j++) {\n            org.junit.Assert.assertTrue(times[j] > times[i]);\n        }\n    }\n}",
        "CUT_1": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"test_table\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"columnA\");\n    byte[] fam2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"columnB\");\n    initHRegion(tableName, getName(), fam1, fam2);\n    byte[] rowA = org.apache.hadoop.hbase.util.Bytes.toBytes(\"rowA\");\n    byte[] rowB = org.apache.hadoop.hbase.util.Bytes.toBytes(\"rowB\");\n    byte[] value = org.apache.hadoop.hbase.util.Bytes.toBytes(\"value\");\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(rowA);\n    delete.deleteFamily(fam1);\n    region.delete(delete, null, true);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(rowA);\n    put.add(fam2, null, value);\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(rowB);\n    put.add(fam1, null, value);\n    put.add(fam2, null, value);\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(fam1).addFamily(fam2);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    s.next(results);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(rowA, results.get(0).getRow()));\n    results.clear();\n    s.next(results);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(rowB, results.get(0).getRow()));\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_DeleteOneFamilyNotAnother",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 28,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"test_table\");\n    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testFamily\");\n    initHRegion(tableName, getName(), family);\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row111\");\n    byte[] row2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row222\");\n    byte[] row3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row333\");\n    byte[] row4 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row444\");\n    byte[] row5 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row555\");\n    byte[] col1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub111\");\n    byte[] col2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub222\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(10L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row2);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(15L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row3);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(20L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row4);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(30L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row5);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(40L));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row3, row4);\n    scan.setMaxVersions();\n    scan.addColumn(family, col1);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    junit.framework.Assert.assertEquals(false, s.next(results));\n    junit.framework.Assert.assertEquals(0, results.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_StopRow1542",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 34,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    this.server = cluster.getRegionServerThreads().get(0).getRegionServer();\n    this.log = server.getLog();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    for (int i = 1; i <= 256; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + java.lang.String.format(\"%1$04d\", i)));\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, value);\n        table.put(put);\n        if ((i % 32) == 0) {\n            try {\n                java.lang.Thread.sleep(2000);\n            } catch (java.lang.InterruptedException e) {\n            }\n        }\n    }\n}",
            "ClassName": "TestLogRolling",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startAndWriteData",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 9,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testHBase737",
        "NumberOfAsserts": 2,
        "NumberOfAsynchronousWaits": 3,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 71,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 6,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    LOG.info(\"Starting testRegionServerSessionExpired\");\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.expireMasterSession();\n    testSanity();\n}",
        "CUT_1": {
            "Body": "{\n    conf = org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.getConfiguration();\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.startMiniZKCluster();\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.startMiniCluster(1);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        rows++;\n    }\n    s.close();\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "countOfMetaRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    while (true) {\n        int rows = 0;\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n        for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n            byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n            if ((b == null) || (b.length <= 0))\n                break;\n\n            rows++;\n        }\n        s.close();\n        if (rows == countOfRegions)\n            break;\n\n        org.apache.hadoop.hbase.master.TestMasterTransitions.LOG.info(\"Found=\" + rows);\n        org.apache.hadoop.hbase.util.Threads.sleep(1000);\n    } \n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "waitUntilAllRegionsAssigned",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestZooKeeper",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testMasterSessionExpired",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 6,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] name = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testReadOnlyTable\");\n    org.apache.hadoop.hbase.client.HTable table = org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.createTable(name, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    byte[] value = org.apache.hadoop.hbase.util.Bytes.toBytes(\"somedata\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(value);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, value);\n    table.put(put);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    i.setOffline(!online);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(i.getRegionName());\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(i));\n    return put;\n}",
            "ClassName": "ChangeTableState",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "updateRegionInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    this.server = cluster.getRegionServerThreads().get(0).getRegionServer();\n    this.log = server.getLog();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    for (int i = 1; i <= 256; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + java.lang.String.format(\"%1$04d\", i)));\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, value);\n        table.put(put);\n        if ((i % 32) == 0) {\n            try {\n                java.lang.Thread.sleep(2000);\n            } catch (java.lang.InterruptedException e) {\n            }\n        }\n    }\n}",
            "ClassName": "TestLogRolling",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startAndWriteData",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(c, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.Result res = t.get(get);\n    org.apache.hadoop.hbase.KeyValue[] kvs = res.raw();\n    if (kvs.length <= 0) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    byte[] value = kvs[0].getValue();\n    if (value == null) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(value);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    info.setOffline(onlineOffline);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(info));\n    t.put(put);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(row);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER);\n    t.delete(delete);\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "changeOnlineStatus",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(i.getRegionName());\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(i));\n    server.put(regionName, put);\n    org.apache.hadoop.hbase.master.ModifyTableMeta.LOG.debug(\"updated HTableDescriptor for region \" + i.getRegionNameAsString());\n}",
            "ClassName": "ModifyTableMeta",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "updateRegionInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testReadOnlyTable",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 8,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = new org.apache.hadoop.hbase.client.HTablePool(((org.apache.hadoop.hbase.HBaseConfiguration) (null)), 2);\n    java.lang.String tableName = \"testTable\";\n    org.apache.hadoop.hbase.client.HTableInterface table1 = pool.getTable(tableName);\n    org.apache.hadoop.hbase.client.HTableInterface table2 = pool.getTable(tableName);\n    org.apache.hadoop.hbase.client.HTableInterface table3 = pool.getTable(tableName);\n    pool.putTable(table1);\n    pool.putTable(table2);\n    pool.putTable(table3);\n    org.apache.hadoop.hbase.client.HTableInterface sameTable1 = pool.getTable(tableName);\n    org.apache.hadoop.hbase.client.HTableInterface sameTable2 = pool.getTable(tableName);\n    org.apache.hadoop.hbase.client.HTableInterface sameTable3 = pool.getTable(tableName);\n    junit.framework.Assert.assertSame(table1, sameTable1);\n    junit.framework.Assert.assertSame(table2, sameTable2);\n    junit.framework.Assert.assertNotSame(table3, sameTable3);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    try {\n        return ((org.apache.hadoop.hbase.client.HTable) (table)).getRegionsInfo();\n    } finally {\n        pool.putTable(table);\n    }\n}",
            "ClassName": "RegionsResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    try {\n        return table.getTableDescriptor();\n    } finally {\n        pool.putTable(table);\n    }\n}",
            "ClassName": "SchemaResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableSchema",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return pool;\n}",
            "ClassName": "RESTServlet",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTablePool",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    java.lang.Integer i = maxAgeMap.get(tableName);\n    if (i != null) {\n        return i.intValue();\n    }\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    try {\n        int maxAge = org.apache.hadoop.hbase.rest.Constants.DEFAULT_MAX_AGE;\n        for (org.apache.hadoop.hbase.HColumnDescriptor family : table.getTableDescriptor().getFamilies()) {\n            int ttl = family.getTimeToLive();\n            if (ttl < 0) {\n                continue;\n            }\n            if (ttl < maxAge) {\n                maxAge = ttl;\n            }\n        }\n        maxAgeMap.put(tableName, maxAge);\n        return maxAge;\n    } finally {\n        pool.putTable(table);\n    }\n}",
            "ClassName": "RESTServlet",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getMaxAge",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    servlet.getMetrics().incrementRequests(1);\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = null;\n    try {\n        java.util.List<org.apache.hadoop.hbase.rest.model.RowModel> rows = model.getRows();\n        table = pool.getTable(tableName);\n        ((org.apache.hadoop.hbase.client.HTable) (table)).setAutoFlush(false);\n        for (org.apache.hadoop.hbase.rest.model.RowModel row : rows) {\n            byte[] key = row.getKey();\n            org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(key);\n            for (org.apache.hadoop.hbase.rest.model.CellModel cell : row.getCells()) {\n                byte[][] parts = org.apache.hadoop.hbase.KeyValue.parseColumn(cell.getColumn());\n                if ((parts.length == 2) && (parts[1].length > 0)) {\n                    put.add(parts[0], parts[1], cell.getTimestamp(), cell.getValue());\n                } else {\n                    put.add(parts[0], null, cell.getTimestamp(), cell.getValue());\n                }\n            }\n            table.put(put);\n            if (org.apache.hadoop.hbase.rest.RowResource.LOG.isDebugEnabled()) {\n                org.apache.hadoop.hbase.rest.RowResource.LOG.debug(\"PUT \" + put.toString());\n            }\n        }\n        ((org.apache.hadoop.hbase.client.HTable) (table)).setAutoFlush(true);\n        table.flushCommits();\n        javax.ws.rs.core.Response.ResponseBuilder response = javax.ws.rs.core.Response.ok();\n        return response.build();\n    } catch (java.io.IOException e) {\n        throw new javax.ws.rs.WebApplicationException(e, Response.Status.SERVICE_UNAVAILABLE);\n    } finally {\n        if (table != null) {\n            pool.putTable(table);\n        }\n    }\n}",
            "ClassName": "RowResource",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "update",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 36,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHTablePool",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testTableWithMaxSize",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 16,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.ipc.HBaseServer.Call call = org.apache.hadoop.hbase.ipc.HBaseServer.CurCall.get();\n    if (call != null) {\n        return call.connection.socket.getInetAddress();\n    }\n    return null;\n}",
            "ClassName": "HBaseServer",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getRemoteIp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.HRegion r = org.apache.hadoop.hbase.regionserver.HRegion.newHRegion(org.apache.hadoop.hbase.HTableDescriptor.getTableDir(rootDir, regionInfo.getTableDesc().getName()), this.hlog, this.fs, conf, regionInfo, this.cacheFlusher);\n    r.initialize(null, new org.apache.hadoop.util.Progressable() {\n        public void progress() {\n            addProcessingMessage(regionInfo);\n        }\n    });\n    return r;\n}",
            "ClassName": "HRegionServer",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "instantiateRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return this.lockId;\n}",
            "ClassName": "Get",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getLockId",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    if (!this.changedReaderObservers.remove(o)) {\n        org.apache.hadoop.hbase.regionserver.Store.LOG.warn(\"Not in set\" + o);\n    }\n}",
            "ClassName": "Store",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "deleteChangedReaderObserver",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_5": {
            "Body": "{\n    this.tables = tables;\n}",
            "ClassName": "TableListModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setTables",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestRegionServerOperationQueue",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testNothing",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 2,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testVersionLimits\");\n    byte[][] FAMILIES = makeNAscii(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, 3);\n    int[] LIMITS = new int[]{ 1, 3, 5 };\n    long[] STAMPS = makeStamps(10);\n    byte[][] VALUES = makeNAscii(org.apache.hadoop.hbase.client.TestFromClientSide.VALUE, 10);\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, FAMILIES, LIMITS);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0], VALUES[0]);\n    put.add(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    put.add(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0], VALUES[0]);\n    put.add(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    put.add(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    put.add(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3], VALUES[3]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0], VALUES[0]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3], VALUES[3]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6], VALUES[6]);\n    ht.put(put);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1] }, new byte[][]{ VALUES[1] }, 0, 0);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addFamily(FAMILIES[0]);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1] }, new byte[][]{ VALUES[1] }, 0, 0);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1] }, new byte[][]{ VALUES[1] }, 0, 0);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addFamily(FAMILIES[0]);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1] }, new byte[][]{ VALUES[1] }, 0, 0);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1], STAMPS[2], STAMPS[3] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3] }, 0, 2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addFamily(FAMILIES[1]);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1], STAMPS[2], STAMPS[3] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3] }, 0, 2);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1], STAMPS[2], STAMPS[3] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3] }, 0, 2);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addFamily(FAMILIES[1]);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1], STAMPS[2], STAMPS[3] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3] }, 0, 2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6] }, new byte[][]{ VALUES[2], VALUES[3], VALUES[4], VALUES[5], VALUES[6] }, 0, 4);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addFamily(FAMILIES[2]);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6] }, new byte[][]{ VALUES[2], VALUES[3], VALUES[4], VALUES[5], VALUES[6] }, 0, 4);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6] }, new byte[][]{ VALUES[2], VALUES[3], VALUES[4], VALUES[5], VALUES[6] }, 0, 4);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addFamily(FAMILIES[2]);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6] }, new byte[][]{ VALUES[2], VALUES[3], VALUES[4], VALUES[5], VALUES[6] }, 0, 4);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    org.junit.Assert.assertTrue(\"Expected 9 keys but received \" + result.size(), result.size() == 9);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addFamily(FAMILIES[0]);\n    get.addFamily(FAMILIES[1]);\n    get.addFamily(FAMILIES[2]);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    org.junit.Assert.assertTrue(\"Expected 9 keys but received \" + result.size(), result.size() == 9);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.addColumn(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.addColumn(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    org.junit.Assert.assertTrue(\"Expected 9 keys but received \" + result.size(), result.size() == 9);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    org.junit.Assert.assertTrue(\"Expected 9 keys but received \" + result.size(), result.size() == 9);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    scan.addFamily(FAMILIES[0]);\n    scan.addFamily(FAMILIES[1]);\n    scan.addFamily(FAMILIES[2]);\n    result = getSingleScanResult(ht, scan);\n    org.junit.Assert.assertTrue(\"Expected 9 keys but received \" + result.size(), result.size() == 9);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    scan.addColumn(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.addColumn(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.addColumn(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    result = getSingleScanResult(ht, scan);\n    org.junit.Assert.assertTrue(\"Expected 9 keys but received \" + result.size(), result.size() == 9);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row);\n    scan.addColumn(family, qualifier);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    scan.setTimeRange(stamps[start], stamps[end] + 1);\n    org.apache.hadoop.hbase.client.Result result = getSingleScanResult(ht, scan);\n    assertNResult(result, row, family, qualifier, stamps, values, start, end);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "scanVersionRangeAndVerify",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row);\n    scan.addColumn(family, qualifier);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    org.apache.hadoop.hbase.client.Result result = getSingleScanResult(ht, scan);\n    assertNResult(result, row, family, qualifier, stamps, values, start, end);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "scanAllVersionsAndVerify",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row);\n    scan.addColumn(family, qualifier);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    scan.setTimeRange(stamps[start + 1], java.lang.Long.MAX_VALUE);\n    org.apache.hadoop.hbase.client.Result result = getSingleScanResult(ht, scan);\n    assertNResult(result, row, family, qualifier, stamps, values, start + 1, end);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "scanVersionRangeAndVerifyGreaterThan",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(family, qualifier);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    get.setTimeRange(stamps[start], stamps[end] + 1);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertNResult(result, row, family, qualifier, stamps, values, start, end);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getVersionRangeAndVerify",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testVersionLimits",
        "NumberOfAsserts": 6,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 119,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper zkw = new org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper(conf, org.apache.hadoop.hbase.EmptyWatcher.instance);\n    zkw.ensureExists(\"/l1/l2/l3/l4\");\n    try {\n        zkw.deleteZNode(\"/l1/l2\");\n        org.junit.Assert.fail(\"We should not be able to delete if znode has childs\");\n    } catch (org.apache.zookeeper.KeeperException ex) {\n        org.junit.Assert.assertNotNull(zkw.getData(\"/l1/l2/l3\", \"l4\"));\n    }\n    zkw.deleteZNode(\"/l1/l2\", true);\n    org.junit.Assert.assertNull(zkw.getData(\"/l1/l2/l3\", \"l4\"));\n    zkw.deleteZNode(\"/l1\");\n    org.junit.Assert.assertNull(zkw.getData(\"/l1\", \"l2\"));\n}",
        "CUT_1": {
            "Body": "{\n    java.util.List<org.apache.hadoop.hbase.KeyValue> l1 = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    l1.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col5, data));\n    l1.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col1, data));\n    l1.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col2, data));\n    scanners.add(new org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.Scanner(l1));\n    java.util.List<org.apache.hadoop.hbase.KeyValue> l2 = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    l2.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col1, data));\n    l2.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col2, data));\n    scanners.add(new org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.Scanner(l2));\n    java.util.List<org.apache.hadoop.hbase.KeyValue> l3 = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col3, data));\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col4, data));\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row1, fam2, col1, data));\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row1, fam2, col2, data));\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col3, data));\n    scanners.add(new org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.Scanner(l3));\n    java.util.List<org.apache.hadoop.hbase.KeyValue> l4 = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    scanners.add(new org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.Scanner(l4));\n    org.apache.hadoop.hbase.regionserver.KeyValueHeap kvh = new org.apache.hadoop.hbase.regionserver.KeyValueHeap(scanners, org.apache.hadoop.hbase.KeyValue.COMPARATOR);\n    while (kvh.next() != null);\n    for (org.apache.hadoop.hbase.regionserver.KeyValueScanner scanner : scanners) {\n        junit.framework.Assert.assertTrue(((org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.Scanner) (scanner)).isClosed());\n    }\n}",
            "ClassName": "TestKeyValueHeap",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScannerLeak",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 25,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    int end1 = s1 + l1;\n    int end2 = s2 + l2;\n    for (int i = s1, j = s2; (i < end1) && (j < end2); i++ , j++) {\n        int a = b1[i] & 0xff;\n        int b = b2[j] & 0xff;\n        if (a != b) {\n            return a - b;\n        }\n    }\n    return l1 - l2;\n}",
            "ClassName": "Bytes",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "compareTo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper zkw = new org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper(conf, org.apache.hadoop.hbase.EmptyWatcher.instance);\n    java.lang.String quorumServers = zkw.getQuorumServers();\n    int sessionTimeout = 5 * 1000;\n    byte[] password = nodeZK.getSessionPassword();\n    long sessionID = nodeZK.getSessionID();\n    org.apache.zookeeper.ZooKeeper zk = new org.apache.zookeeper.ZooKeeper(quorumServers, sessionTimeout, org.apache.hadoop.hbase.EmptyWatcher.instance, sessionID, password);\n    zk.close();\n    final long sleep = sessionTimeout * 5L;\n    LOG.info(\"ZK Closed; sleeping=\" + sleep);\n    java.lang.Thread.sleep(sleep);\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "expireSession",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 13,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_4": {
            "Body": "{\n    java.util.List<org.apache.hadoop.hbase.KeyValue> l1 = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    l1.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col5, data));\n    l1.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col1, data));\n    l1.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col2, data));\n    scanners.add(new org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.Scanner(l1));\n    java.util.List<org.apache.hadoop.hbase.KeyValue> l2 = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    l2.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col1, data));\n    l2.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col2, data));\n    scanners.add(new org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.Scanner(l2));\n    java.util.List<org.apache.hadoop.hbase.KeyValue> l3 = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col3, data));\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col4, data));\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row1, fam2, col1, data));\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row1, fam2, col2, data));\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col3, data));\n    scanners.add(new org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.Scanner(l3));\n    java.util.List<org.apache.hadoop.hbase.KeyValue> expected = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    expected.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col1, data));\n    org.apache.hadoop.hbase.regionserver.KeyValueHeap kvh = new org.apache.hadoop.hbase.regionserver.KeyValueHeap(scanners, org.apache.hadoop.hbase.KeyValue.COMPARATOR);\n    org.apache.hadoop.hbase.KeyValue seekKv = new org.apache.hadoop.hbase.KeyValue(row2, fam1, null, null);\n    kvh.seek(seekKv);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> actual = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    actual.add(kvh.peek());\n    junit.framework.Assert.assertEquals(expected.size(), actual.size());\n    for (int i = 0; i < expected.size(); i++) {\n        junit.framework.Assert.assertEquals(expected.get(i), actual.get(i));\n        if (org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.PRINT) {\n            java.lang.System.out.println((((\"expected \" + expected.get(i)) + \"\\nactual   \") + actual.get(i)) + \"\\n\");\n        }\n    }\n}",
            "ClassName": "TestKeyValueHeap",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSeek",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 32,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    java.util.List<org.apache.hadoop.hbase.KeyValue> l1 = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    l1.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col5, data));\n    l1.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col1, data));\n    l1.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col2, data));\n    scanners.add(new org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.Scanner(l1));\n    java.util.List<org.apache.hadoop.hbase.KeyValue> l2 = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    l2.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col1, data));\n    l2.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col2, data));\n    scanners.add(new org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.Scanner(l2));\n    java.util.List<org.apache.hadoop.hbase.KeyValue> l3 = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col3, data));\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col4, data));\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row1, fam2, col1, data));\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row1, fam2, col2, data));\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col3, data));\n    scanners.add(new org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.Scanner(l3));\n    java.util.List<org.apache.hadoop.hbase.KeyValue> expected = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    expected.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col1, data));\n    expected.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col2, data));\n    expected.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col3, data));\n    expected.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col4, data));\n    expected.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col5, data));\n    expected.add(new org.apache.hadoop.hbase.KeyValue(row1, fam2, col1, data));\n    expected.add(new org.apache.hadoop.hbase.KeyValue(row1, fam2, col2, data));\n    expected.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col1, data));\n    expected.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col2, data));\n    expected.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col3, data));\n    org.apache.hadoop.hbase.regionserver.KeyValueHeap kvh = new org.apache.hadoop.hbase.regionserver.KeyValueHeap(scanners, org.apache.hadoop.hbase.KeyValue.COMPARATOR);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> actual = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    while (kvh.peek() != null) {\n        actual.add(kvh.next());\n    } \n    junit.framework.Assert.assertEquals(expected.size(), actual.size());\n    for (int i = 0; i < expected.size(); i++) {\n        junit.framework.Assert.assertEquals(expected.get(i), actual.get(i));\n        if (org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.PRINT) {\n            java.lang.System.out.println((((\"expected \" + expected.get(i)) + \"\\nactual   \") + actual.get(i)) + \"\\n\");\n        }\n    }\n    for (int i = 0; i < (actual.size() - 1); i++) {\n        int ret = org.apache.hadoop.hbase.KeyValue.COMPARATOR.compare(actual.get(i), actual.get(i + 1));\n        junit.framework.Assert.assertTrue(ret < 0);\n    }\n}",
            "ClassName": "TestKeyValueHeap",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSorted",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 45,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestZooKeeper",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testZNodeDeletes",
        "NumberOfAsserts": 4,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 14,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testJiraTest1182\");\n    byte[][] VALUES = makeNAscii(org.apache.hadoop.hbase.client.TestFromClientSide.VALUE, 7);\n    long[] STAMPS = makeStamps(7);\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, 10);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0], VALUES[0]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3], VALUES[3]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    ht.put(put);\n    getVersionRangeAndVerifyGreaterThan(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 5);\n    getVersionRangeAndVerifyGreaterThan(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 2, 5);\n    getVersionRangeAndVerifyGreaterThan(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 4, 5);\n    scanVersionRangeAndVerifyGreaterThan(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 5);\n    scanVersionRangeAndVerifyGreaterThan(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 2, 5);\n    scanVersionRangeAndVerifyGreaterThan(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 4, 5);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    getVersionRangeAndVerifyGreaterThan(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 5);\n    getVersionRangeAndVerifyGreaterThan(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 2, 5);\n    getVersionRangeAndVerifyGreaterThan(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 4, 5);\n    scanVersionRangeAndVerifyGreaterThan(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 5);\n    scanVersionRangeAndVerifyGreaterThan(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 2, 5);\n    scanVersionRangeAndVerifyGreaterThan(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 4, 5);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration());\n    admin.split(t.getTableName());\n    java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> regions = waitOnSplit(t);\n    org.junit.Assert.assertTrue(regions.size() > 1);\n    return regions;\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "splitTable",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(family, qualifier);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    get.setTimeRange(stamps[start], stamps[end] + 1);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertNResult(result, row, family, qualifier, stamps, values, start, end);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getVersionRangeAndVerify",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testJiraTest1182",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 27,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testDuplicateVersions\");\n    long[] STAMPS = makeStamps(20);\n    byte[][] VALUES = makeNAscii(org.apache.hadoop.hbase.client.TestFromClientSide.VALUE, 20);\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, 10);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    ht.put(put);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6]);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(2);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[4], STAMPS[5] }, new byte[][]{ VALUES[4], VALUES[5] }, 0, 1);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(2);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[4], STAMPS[5] }, new byte[][]{ VALUES[4], VALUES[5] }, 0, 1);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6]);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(2);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[4], STAMPS[5] }, new byte[][]{ VALUES[4], VALUES[5] }, 0, 1);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(2);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[4], STAMPS[5] }, new byte[][]{ VALUES[4], VALUES[5] }, 0, 1);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3], VALUES[3]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[14]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6], VALUES[6]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[7], VALUES[7]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[8], VALUES[8]);\n    ht.put(put);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(7);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[7], STAMPS[8] }, new byte[][]{ VALUES[2], VALUES[3], VALUES[14], VALUES[5], VALUES[6], VALUES[7], VALUES[8] }, 0, 6);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(7);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[7], STAMPS[8] }, new byte[][]{ VALUES[2], VALUES[3], VALUES[14], VALUES[5], VALUES[6], VALUES[7], VALUES[8] }, 0, 6);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.setMaxVersions(7);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[7], STAMPS[8] }, new byte[][]{ VALUES[2], VALUES[3], VALUES[14], VALUES[5], VALUES[6], VALUES[7], VALUES[8] }, 0, 6);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.setMaxVersions(7);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[7], STAMPS[8] }, new byte[][]{ VALUES[2], VALUES[3], VALUES[14], VALUES[5], VALUES[6], VALUES[7], VALUES[8] }, 0, 6);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[14]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[7], VALUES[7]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[14]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[7], VALUES[7]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[9]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[9]);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[9], VALUES[9]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[11], VALUES[11]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[13], VALUES[13]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[15], VALUES[15]);\n    ht.put(put);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[7], STAMPS[8], STAMPS[9], STAMPS[11], STAMPS[13], STAMPS[15] }, new byte[][]{ VALUES[3], VALUES[14], VALUES[5], VALUES[6], VALUES[7], VALUES[8], VALUES[9], VALUES[11], VALUES[13], VALUES[15] }, 0, 9);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[7], STAMPS[8], STAMPS[9], STAMPS[11], STAMPS[13], STAMPS[15] }, new byte[][]{ VALUES[3], VALUES[14], VALUES[5], VALUES[6], VALUES[7], VALUES[8], VALUES[9], VALUES[11], VALUES[13], VALUES[15] }, 0, 9);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    delete.deleteColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[11]);\n    delete.deleteColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[7]);\n    ht.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[8], STAMPS[9], STAMPS[13], STAMPS[15] }, new byte[][]{ VALUES[3], VALUES[14], VALUES[5], VALUES[6], VALUES[8], VALUES[9], VALUES[13], VALUES[15] }, 0, 7);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[8], STAMPS[9], STAMPS[13], STAMPS[15] }, new byte[][]{ VALUES[3], VALUES[14], VALUES[5], VALUES[6], VALUES[8], VALUES[9], VALUES[13], VALUES[15] }, 0, 7);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(family, qualifier);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    get.setTimeRange(stamps[start], stamps[end] + 1);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertNResult(result, row, family, qualifier, stamps, values, start, end);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getVersionRangeAndVerify",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row);\n    scan.addColumn(family, qualifier);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    scan.setTimeRange(stamps[start], stamps[end] + 1);\n    org.apache.hadoop.hbase.client.Result result = getSingleScanResult(ht, scan);\n    assertNResult(result, row, family, qualifier, stamps, values, start, end);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "scanVersionRangeAndVerify",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testDuplicateVersions",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 129,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\");\n    org.apache.hadoop.hbase.HColumnDescriptor hcd = new org.apache.hadoop.hbase.HColumnDescriptor(family);\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.regionserver.TestStoreReconstruction.TABLE);\n    htd.addFamily(hcd);\n    org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(htd, null, null, false);\n    org.apache.hadoop.fs.Path oldLogDir = new org.apache.hadoop.fs.Path(this.dir, org.apache.hadoop.hbase.HConstants.HREGION_OLDLOGDIR_NAME);\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(cluster.getFileSystem(), this.dir, oldLogDir, conf, null);\n    org.apache.hadoop.hbase.regionserver.HRegion region = new org.apache.hadoop.hbase.regionserver.HRegion(dir, log, cluster.getFileSystem(), conf, info, null);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> result = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    java.util.NavigableSet<byte[]> qualifiers = new java.util.concurrent.ConcurrentSkipListSet<byte[]>(org.apache.hadoop.hbase.util.Bytes.BYTES_COMPARATOR);\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.regionserver.TestStoreReconstruction.TABLE);\n    final byte[] rowName = tableName;\n    final byte[] regionName = info.getRegionName();\n    for (int j = 0; j < org.apache.hadoop.hbase.regionserver.TestStoreReconstruction.TOTAL_EDITS; j++) {\n        byte[] qualifier = org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(j));\n        byte[] column = org.apache.hadoop.hbase.util.Bytes.toBytes(\"column:\" + java.lang.Integer.toString(j));\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit edit = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        edit.add(new org.apache.hadoop.hbase.KeyValue(rowName, family, qualifier, java.lang.System.currentTimeMillis(), column));\n        log.append(info, tableName, edit, java.lang.System.currentTimeMillis());\n    }\n    long logSeqId = log.startCacheFlush();\n    log.completeCacheFlush(regionName, tableName, logSeqId, info.isMetaRegion());\n    org.apache.hadoop.hbase.regionserver.wal.WALEdit edit = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n    edit.add(new org.apache.hadoop.hbase.KeyValue(rowName, org.apache.hadoop.hbase.util.Bytes.toBytes(\"another family\"), rowName, java.lang.System.currentTimeMillis(), rowName));\n    log.append(info, tableName, edit, java.lang.System.currentTimeMillis());\n    log.sync();\n    log.close();\n    java.util.List<org.apache.hadoop.fs.Path> splits = org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(new org.apache.hadoop.fs.Path(conf.get(org.apache.hadoop.hbase.HConstants.HBASE_DIR)), this.dir, oldLogDir, cluster.getFileSystem(), conf);\n    org.junit.Assert.assertEquals(1, splits.size());\n    org.junit.Assert.assertTrue(cluster.getFileSystem().exists(splits.get(0)));\n    org.apache.hadoop.hbase.regionserver.Store store = new org.apache.hadoop.hbase.regionserver.Store(dir, region, hcd, cluster.getFileSystem(), splits.get(0), conf, null);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(rowName);\n    store.get(get, qualifiers, result);\n    org.junit.Assert.assertEquals(org.apache.hadoop.hbase.regionserver.TestStoreReconstruction.TOTAL_EDITS, result.size());\n}",
        "CUT_1": {
            "Body": "{\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(getName());\n    final byte[] rowName = tableName;\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(this.fs, this.dir, this.oldLogDir, this.conf, null);\n    final int howmany = 3;\n    org.apache.hadoop.hbase.HRegionInfo[] infos = new org.apache.hadoop.hbase.HRegionInfo[3];\n    for (int i = 0; i < howmany; i++) {\n        infos[i] = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(tableName), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\" + i), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\" + (i + 1)), false);\n    }\n    try {\n        for (int ii = 0; ii < howmany; ii++) {\n            for (int i = 0; i < howmany; i++) {\n                for (int j = 0; j < howmany; j++) {\n                    org.apache.hadoop.hbase.regionserver.wal.WALEdit edit = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n                    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\");\n                    byte[] qualifier = org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(j));\n                    byte[] column = org.apache.hadoop.hbase.util.Bytes.toBytes(\"column:\" + java.lang.Integer.toString(j));\n                    edit.add(new org.apache.hadoop.hbase.KeyValue(rowName, family, qualifier, java.lang.System.currentTimeMillis(), column));\n                    java.lang.System.out.println(((\"Region \" + i) + \": \") + edit);\n                    log.append(infos[i], tableName, edit, java.lang.System.currentTimeMillis());\n                }\n            }\n            log.hflush();\n            log.rollWriter();\n        }\n        java.util.List<org.apache.hadoop.fs.Path> splits = org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(this.testDir, this.dir, this.oldLogDir, this.fs, this.conf);\n        verifySplits(splits, howmany);\n        log = null;\n    } finally {\n        if (log != null) {\n            log.closeAndDelete();\n        }\n    }\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSplit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 34,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    final int COL_COUNT = 10;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"tablename\");\n    final byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\");\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = null;\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(fs, dir, this.oldLogDir, this.conf, null);\n    try {\n        long timestamp = java.lang.System.currentTimeMillis();\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit cols = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        for (int i = 0; i < COL_COUNT; i++) {\n            cols.add(new org.apache.hadoop.hbase.KeyValue(row, org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\"), org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(i)), timestamp, new byte[]{ ((byte) (i + '0')) }));\n        }\n        org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(tableName), row, org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.util.Bytes.toString(row) + \"1\"), false);\n        final byte[] regionName = info.getRegionName();\n        log.append(info, tableName, cols, java.lang.System.currentTimeMillis());\n        long logSeqId = log.startCacheFlush();\n        log.completeCacheFlush(regionName, tableName, logSeqId, info.isMetaRegion());\n        log.close();\n        org.apache.hadoop.fs.Path filename = log.computeFilename(log.getFilenum());\n        log = null;\n        reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, filename, conf);\n        for (int i = 0; i < 1; i++) {\n            org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = reader.next(null);\n            if (entry == null)\n                break;\n\n            org.apache.hadoop.hbase.regionserver.wal.HLogKey key = entry.getKey();\n            org.apache.hadoop.hbase.regionserver.wal.WALEdit val = entry.getEdit();\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(regionName, key.getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, key.getTablename()));\n            org.apache.hadoop.hbase.KeyValue kv = val.getKeyValues().get(0);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(row, kv.getRow()));\n            junit.framework.Assert.assertEquals(((byte) (i + '0')), kv.getValue()[0]);\n            java.lang.System.out.println((key + \" \") + val);\n        }\n        org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = null;\n        while ((entry = reader.next(null)) != null) {\n            org.apache.hadoop.hbase.regionserver.wal.HLogKey key = entry.getKey();\n            org.apache.hadoop.hbase.regionserver.wal.WALEdit val = entry.getEdit();\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(regionName, key.getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, key.getTablename()));\n            org.apache.hadoop.hbase.KeyValue kv = val.getKeyValues().get(0);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, kv.getRow()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, kv.getFamily()));\n            junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH, val.getKeyValues().get(0).getValue()));\n            java.lang.System.out.println((key + \" \") + val);\n        } \n    } finally {\n        if (log != null) {\n            log.closeAndDelete();\n        }\n        if (reader != null) {\n            reader.close();\n        }\n    }\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 6,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testEditAdd",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 56,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.fs.Path basedir = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.TestStore.DIR + methodName);\n    org.apache.hadoop.fs.Path logdir = new org.apache.hadoop.fs.Path((org.apache.hadoop.hbase.regionserver.TestStore.DIR + methodName) + \"/logs\");\n    org.apache.hadoop.fs.Path oldLogDir = new org.apache.hadoop.fs.Path(basedir, org.apache.hadoop.hbase.HConstants.HREGION_OLDLOGDIR_NAME);\n    org.apache.hadoop.hbase.HColumnDescriptor hcd = new org.apache.hadoop.hbase.HColumnDescriptor(family);\n    org.apache.hadoop.hbase.HBaseConfiguration conf = new org.apache.hadoop.hbase.HBaseConfiguration();\n    org.apache.hadoop.fs.FileSystem fs = org.apache.hadoop.fs.FileSystem.get(conf);\n    org.apache.hadoop.fs.Path reconstructionLog = null;\n    org.apache.hadoop.util.Progressable reporter = null;\n    fs.delete(logdir, true);\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(table);\n    htd.addFamily(hcd);\n    org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(htd, null, null, false);\n    org.apache.hadoop.hbase.regionserver.wal.HLog hlog = new org.apache.hadoop.hbase.regionserver.wal.HLog(fs, logdir, oldLogDir, conf, null);\n    org.apache.hadoop.hbase.regionserver.HRegion region = new org.apache.hadoop.hbase.regionserver.HRegion(basedir, hlog, fs, conf, info, null);\n    store = new org.apache.hadoop.hbase.regionserver.Store(basedir, region, hcd, fs, reconstructionLog, conf, reporter);\n}",
            "ClassName": "TestStore",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "init",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 17,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    final int COL_COUNT = 10;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"tablename\");\n    final byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\");\n    this.conf.setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = null;\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(this.fs, dir, this.oldLogDir, this.conf, null);\n    try {\n        long timestamp = java.lang.System.currentTimeMillis();\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit cols = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        for (int i = 0; i < COL_COUNT; i++) {\n            cols.add(new org.apache.hadoop.hbase.KeyValue(row, org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\"), org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(i)), timestamp, new byte[]{ ((byte) (i + '0')) }));\n        }\n        org.apache.hadoop.hbase.HRegionInfo hri = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(tableName), org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.HConstants.EMPTY_END_ROW);\n        log.append(hri, tableName, cols, java.lang.System.currentTimeMillis());\n        long logSeqId = log.startCacheFlush();\n        log.completeCacheFlush(hri.getRegionName(), tableName, logSeqId, false);\n        log.close();\n        org.apache.hadoop.fs.Path filename = log.computeFilename(log.getFilenum());\n        log = null;\n        reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, filename, conf);\n        org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = reader.next();\n        junit.framework.Assert.assertEquals(COL_COUNT, entry.getEdit().size());\n        int idx = 0;\n        for (org.apache.hadoop.hbase.KeyValue val : entry.getEdit().getKeyValues()) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getRegionName(), entry.getKey().getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, entry.getKey().getTablename()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(row, val.getRow()));\n            junit.framework.Assert.assertEquals(((byte) (idx + '0')), val.getValue()[0]);\n            java.lang.System.out.println((entry.getKey() + \" \") + val);\n            idx++;\n        }\n        entry = reader.next();\n        junit.framework.Assert.assertEquals(1, entry.getEdit().size());\n        for (org.apache.hadoop.hbase.KeyValue val : entry.getEdit().getKeyValues()) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getRegionName(), entry.getKey().getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, entry.getKey().getTablename()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, val.getRow()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, val.getFamily()));\n            junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH, val.getValue()));\n            java.lang.System.out.println((entry.getKey() + \" \") + val);\n        }\n    } finally {\n        if (log != null) {\n            log.closeAndDelete();\n        }\n        if (reader != null) {\n            reader.close();\n        }\n    }\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testAppend",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 51,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    for (byte[] family : families) {\n        htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(family));\n    }\n    org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(htd, null, null, false);\n    org.apache.hadoop.fs.Path path = new org.apache.hadoop.fs.Path(DIR + callingMethod);\n    region = org.apache.hadoop.hbase.regionserver.HRegion.createHRegion(info, path, conf);\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "initHRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestStoreReconstruction",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 1,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "runReconstructionLog",
        "NumberOfAsserts": 3,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 36,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 2,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.ipc.HBaseServer.Call call = org.apache.hadoop.hbase.ipc.HBaseServer.CurCall.get();\n    if (call != null) {\n        return call.connection.socket.getInetAddress();\n    }\n    return null;\n}",
            "ClassName": "HBaseServer",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getRemoteIp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.HRegion r = org.apache.hadoop.hbase.regionserver.HRegion.newHRegion(org.apache.hadoop.hbase.HTableDescriptor.getTableDir(rootDir, regionInfo.getTableDesc().getName()), this.hlog, this.fs, conf, regionInfo, this.cacheFlusher);\n    r.initialize(null, new org.apache.hadoop.util.Progressable() {\n        public void progress() {\n            addProcessingMessage(regionInfo);\n        }\n    });\n    return r;\n}",
            "ClassName": "HRegionServer",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "instantiateRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return this.lockId;\n}",
            "ClassName": "Get",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getLockId",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    if (!this.changedReaderObservers.remove(o)) {\n        org.apache.hadoop.hbase.regionserver.Store.LOG.warn(\"Not in set\" + o);\n    }\n}",
            "ClassName": "Store",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "deleteChangedReaderObserver",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_5": {
            "Body": "{\n    this.tables = tables;\n}",
            "ClassName": "TableListModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setTables",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testMillions",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 2,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    final byte[] CONTENTS_FAMILY = org.apache.hadoop.hbase.util.Bytes.toBytes(\"contents\");\n    final byte[] SMALL_FAMILY = org.apache.hadoop.hbase.util.Bytes.toBytes(\"smallfam\");\n    final int NB_BATCH_ROWS = 10;\n    final byte[] value = org.apache.hadoop.hbase.util.Bytes.toBytes(\"abcd\");\n    org.apache.hadoop.hbase.client.HTable table = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowsPut\"), new byte[][]{ CONTENTS_FAMILY, SMALL_FAMILY });\n    java.util.ArrayList<org.apache.hadoop.hbase.client.Put> rowsUpdate = new java.util.ArrayList<org.apache.hadoop.hbase.client.Put>();\n    for (int i = 0; i < NB_BATCH_ROWS; i++) {\n        byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + i);\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n        put.add(CONTENTS_FAMILY, null, value);\n        rowsUpdate.add(put);\n    }\n    table.put(rowsUpdate);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(CONTENTS_FAMILY);\n    org.apache.hadoop.hbase.client.ResultScanner scanner = table.getScanner(scan);\n    int nbRows = 0;\n    for (@java.lang.SuppressWarnings(\"unused\")\n    org.apache.hadoop.hbase.client.Result row : scanner)\n        nbRows++;\n\n    org.junit.Assert.assertEquals(NB_BATCH_ROWS, nbRows);\n}",
        "CUT_1": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"test_table\");\n    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testFamily\");\n    initHRegion(tableName, getName(), family);\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row111\");\n    byte[] row2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row222\");\n    byte[] row3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row333\");\n    byte[] row4 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row444\");\n    byte[] row5 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row555\");\n    byte[] col1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub111\");\n    byte[] col2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub222\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(10L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row2);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(15L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row3);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(20L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row4);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(30L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row5);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(40L));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row3, row4);\n    scan.setMaxVersions();\n    scan.addColumn(family, col1);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    junit.framework.Assert.assertEquals(false, s.next(results));\n    junit.framework.Assert.assertEquals(0, results.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_StopRow1542",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 34,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    int count = 0;\n    for (char c = 'a'; c <= 'c'; c++) {\n        byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"ab\" + c);\n        int i;\n        for (i = 0; i < 2500; i++) {\n            byte[] b = org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.String.format(\"%10d\", i));\n            org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n            put.add(family, b, b);\n            region.put(put);\n            count++;\n        }\n    }\n    {\n        byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"abf\");\n        int i;\n        for (i = 0; i < 100000; i++) {\n            byte[] b = org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.String.format(\"%10d\", i));\n            org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n            put.add(family, b, b);\n            region.put(put);\n            count++;\n        }\n    }\n    return count;\n}",
            "ClassName": "TestWideScanner",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addWideContent",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 26,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"test_table\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"columnA\");\n    byte[] fam2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"columnB\");\n    initHRegion(tableName, getName(), fam1, fam2);\n    byte[] rowA = org.apache.hadoop.hbase.util.Bytes.toBytes(\"rowA\");\n    byte[] rowB = org.apache.hadoop.hbase.util.Bytes.toBytes(\"rowB\");\n    byte[] value = org.apache.hadoop.hbase.util.Bytes.toBytes(\"value\");\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(rowA);\n    delete.deleteFamily(fam1);\n    region.delete(delete, null, true);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(rowA);\n    put.add(fam2, null, value);\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(rowB);\n    put.add(fam1, null, value);\n    put.add(fam2, null, value);\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(fam1).addFamily(fam2);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    s.next(results);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(rowA, results.get(0).getRow()));\n    results.clear();\n    s.next(results);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(rowB, results.get(0).getRow()));\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_DeleteOneFamilyNotAnother",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 28,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 2,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testRowsPut",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 24,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testWeirdCacheBehaviour\");\n    byte[][] FAMILIES = new byte[][]{ org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-blob\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-type\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-date\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-tags\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-group\") };\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, FAMILIES);\n    java.lang.String value = \"this is the value\";\n    java.lang.String value2 = \"this is some other value\";\n    java.lang.String keyPrefix1 = java.util.UUID.randomUUID().toString();\n    java.lang.String keyPrefix2 = java.util.UUID.randomUUID().toString();\n    java.lang.String keyPrefix3 = java.util.UUID.randomUUID().toString();\n    putRows(ht, 3, value, keyPrefix1);\n    putRows(ht, 3, value, keyPrefix2);\n    putRows(ht, 3, value, keyPrefix3);\n    ht.flushCommits();\n    putRows(ht, 3, value2, keyPrefix1);\n    putRows(ht, 3, value2, keyPrefix2);\n    putRows(ht, 3, value2, keyPrefix3);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testWeirdCacheBehaviour\"));\n    java.lang.System.out.println(\"Checking values for key: \" + keyPrefix1);\n    org.junit.Assert.assertEquals(\"Got back incorrect number of rows from scan\", 3, getNumberOfRows(keyPrefix1, value2, table));\n    java.lang.System.out.println(\"Checking values for key: \" + keyPrefix2);\n    org.junit.Assert.assertEquals(\"Got back incorrect number of rows from scan\", 3, getNumberOfRows(keyPrefix2, value2, table));\n    java.lang.System.out.println(\"Checking values for key: \" + keyPrefix3);\n    org.junit.Assert.assertEquals(\"Got back incorrect number of rows from scan\", 3, getNumberOfRows(keyPrefix3, value2, table));\n    deleteColumns(ht, value2, keyPrefix1);\n    deleteColumns(ht, value2, keyPrefix2);\n    deleteColumns(ht, value2, keyPrefix3);\n    java.lang.System.out.println(\"Starting important checks.....\");\n    org.junit.Assert.assertEquals(\"Got back incorrect number of rows from scan: \" + keyPrefix1, 0, getNumberOfRows(keyPrefix1, value2, table));\n    org.junit.Assert.assertEquals(\"Got back incorrect number of rows from scan: \" + keyPrefix2, 0, getNumberOfRows(keyPrefix2, value2, table));\n    org.junit.Assert.assertEquals(\"Got back incorrect number of rows from scan: \" + keyPrefix3, 0, getNumberOfRows(keyPrefix3, value2, table));\n    ht.setScannerCaching(0);\n    org.junit.Assert.assertEquals(\"Got back incorrect number of rows from scan\", 0, getNumberOfRows(keyPrefix1, value2, table));\n    ht.setScannerCaching(100);\n    org.junit.Assert.assertEquals(\"Got back incorrect number of rows from scan\", 0, getNumberOfRows(keyPrefix2, value2, table));\n}",
        "CUT_1": {
            "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testWeirdCacheBehaviour\");\n    byte[][] FAMILIES = new byte[][]{ org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-blob\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-type\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-date\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-tags\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-group\") };\n    initHRegion(TABLE, getName(), FAMILIES);\n    java.lang.String value = \"this is the value\";\n    java.lang.String value2 = \"this is some other value\";\n    java.lang.String keyPrefix1 = \"prefix1\";\n    java.lang.String keyPrefix2 = \"prefix2\";\n    java.lang.String keyPrefix3 = \"prefix3\";\n    putRows(this.region, 3, value, keyPrefix1);\n    putRows(this.region, 3, value, keyPrefix2);\n    putRows(this.region, 3, value, keyPrefix3);\n    putRows(this.region, 3, value2, keyPrefix1);\n    putRows(this.region, 3, value2, keyPrefix2);\n    putRows(this.region, 3, value2, keyPrefix3);\n    java.lang.System.out.println(\"Checking values for key: \" + keyPrefix1);\n    junit.framework.Assert.assertEquals(\"Got back incorrect number of rows from scan\", 3, getNumberOfRows(keyPrefix1, value2, this.region));\n    java.lang.System.out.println(\"Checking values for key: \" + keyPrefix2);\n    junit.framework.Assert.assertEquals(\"Got back incorrect number of rows from scan\", 3, getNumberOfRows(keyPrefix2, value2, this.region));\n    java.lang.System.out.println(\"Checking values for key: \" + keyPrefix3);\n    junit.framework.Assert.assertEquals(\"Got back incorrect number of rows from scan\", 3, getNumberOfRows(keyPrefix3, value2, this.region));\n    deleteColumns(this.region, value2, keyPrefix1);\n    deleteColumns(this.region, value2, keyPrefix2);\n    deleteColumns(this.region, value2, keyPrefix3);\n    java.lang.System.out.println(\"Starting important checks.....\");\n    junit.framework.Assert.assertEquals(\"Got back incorrect number of rows from scan: \" + keyPrefix1, 0, getNumberOfRows(keyPrefix1, value2, this.region));\n    junit.framework.Assert.assertEquals(\"Got back incorrect number of rows from scan: \" + keyPrefix2, 0, getNumberOfRows(keyPrefix2, value2, this.region));\n    junit.framework.Assert.assertEquals(\"Got back incorrect number of rows from scan: \" + keyPrefix3, 0, getNumberOfRows(keyPrefix3, value2, this.region));\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testWeirdCacheBehaviour",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 29,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    for (int i = 0; i < numRows; i++) {\n        java.lang.String row = (key + \"_\") + java.util.UUID.randomUUID().toString();\n        java.lang.System.out.println(java.lang.String.format(\"Saving row: %s, with value %s\", row, value));\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(row));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-blob\"), null, org.apache.hadoop.hbase.util.Bytes.toBytes(\"value for blob\"));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-type\"), null, org.apache.hadoop.hbase.util.Bytes.toBytes(\"statement\"));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-date\"), null, org.apache.hadoop.hbase.util.Bytes.toBytes(\"20090921010101999\"));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-tags\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"qual2\"), org.apache.hadoop.hbase.util.Bytes.toBytes(value));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-group\"), null, org.apache.hadoop.hbase.util.Bytes.toBytes(\"adhocTransactionGroupId\"));\n        ht.put(put);\n    }\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "putRows",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 13,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    for (int i = 0; i < numRows; i++) {\n        java.lang.String row = (key + \"_\") + i;\n        java.lang.System.out.println(java.lang.String.format(\"Saving row: %s, with value %s\", row, value));\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(row));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-blob\"), null, org.apache.hadoop.hbase.util.Bytes.toBytes(\"value for blob\"));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-type\"), null, org.apache.hadoop.hbase.util.Bytes.toBytes(\"statement\"));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-date\"), null, org.apache.hadoop.hbase.util.Bytes.toBytes(\"20090921010101999\"));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-tags\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"qual2\"), org.apache.hadoop.hbase.util.Bytes.toBytes(value));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-group\"), null, org.apache.hadoop.hbase.util.Bytes.toBytes(\"adhocTransactionGroupId\"));\n        r.put(put);\n    }\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "putRows",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 13,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.filter.FilterList allFilters = new org.apache.hadoop.hbase.filter.FilterList();\n    allFilters.addFilter(new org.apache.hadoop.hbase.filter.PrefixFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(keyPrefix)));\n    org.apache.hadoop.hbase.filter.SingleColumnValueFilter filter = new org.apache.hadoop.hbase.filter.SingleColumnValueFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-tags\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"qual2\"), org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, org.apache.hadoop.hbase.util.Bytes.toBytes(value));\n    filter.setFilterIfMissing(true);\n    allFilters.addFilter(filter);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-blob\"));\n    scan.addFamily(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-type\"));\n    scan.addFamily(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-date\"));\n    scan.addFamily(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-tags\"));\n    scan.addFamily(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-group\"));\n    scan.setFilter(allFilters);\n    return ht.getScanner(scan);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "buildScanner",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 15,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(conf, TABLE_NAME);\n    for (int i = 0; i < ROWS.length; i++) {\n        for (int j = 0; j < TIMESTAMPS.length; j++) {\n            org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(ROWS[i]);\n            get.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n            get.setTimeStamp(TIMESTAMPS[j]);\n            org.apache.hadoop.hbase.client.Result result = t.get(get);\n            int cellCount = 0;\n            for (@java.lang.SuppressWarnings(\"unused\")\n            org.apache.hadoop.hbase.KeyValue kv : result.sorted()) {\n                cellCount++;\n            }\n            junit.framework.Assert.assertTrue(cellCount == 1);\n        }\n    }\n    int count = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.apache.hadoop.hbase.client.ResultScanner s = t.getScanner(scan);\n    try {\n        for (org.apache.hadoop.hbase.client.Result rr = null; (rr = s.next()) != null;) {\n            java.lang.System.out.println(rr.toString());\n            count += 1;\n        }\n        junit.framework.Assert.assertEquals(\"Number of rows should be 2\", 2, count);\n    } finally {\n        s.close();\n    }\n    count = 0;\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.setTimeRange(1000L, java.lang.Long.MAX_VALUE);\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    s = t.getScanner(scan);\n    try {\n        while (s.next() != null) {\n            count += 1;\n        } \n        junit.framework.Assert.assertEquals(\"Number of rows should be 2\", 2, count);\n    } finally {\n        s.close();\n    }\n    count = 0;\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.setTimeStamp(1000L);\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    s = t.getScanner(scan);\n    try {\n        while (s.next() != null) {\n            count += 1;\n        } \n        junit.framework.Assert.assertEquals(\"Number of rows should be 2\", 2, count);\n    } finally {\n        s.close();\n    }\n    count = 0;\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.setTimeRange(100L, 1000L);\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    s = t.getScanner(scan);\n    try {\n        while (s.next() != null) {\n            count += 1;\n        } \n        junit.framework.Assert.assertEquals(\"Number of rows should be 2\", 2, count);\n    } finally {\n        s.close();\n    }\n    count = 0;\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.setTimeStamp(100L);\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    s = t.getScanner(scan);\n    try {\n        while (s.next() != null) {\n            count += 1;\n        } \n        junit.framework.Assert.assertEquals(\"Number of rows should be 2\", 2, count);\n    } finally {\n        s.close();\n    }\n}",
            "ClassName": "TestScanMultipleVersions",
            "CyclomaticComplexity": 8,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanMultipleVersions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 82,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testWeirdCacheBehaviour",
        "NumberOfAsserts": 8,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 35,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testSuperSimple\");\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n    ht.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, TABLE);\n    org.apache.hadoop.hbase.client.ResultScanner scanner = ht.getScanner(scan);\n    org.apache.hadoop.hbase.client.Result result = scanner.next();\n    org.junit.Assert.assertTrue(\"Expected null result\", result == null);\n    scanner.close();\n    java.lang.System.out.println(\"Done.\");\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(getConfiguration(), tableName);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.ResultScanner resScan = table.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result res : resScan) {\n        org.apache.hadoop.hbase.client.Delete del = new org.apache.hadoop.hbase.client.Delete(res.getRow());\n        table.delete(del);\n    }\n    return table;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "truncateTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 10,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        org.apache.hadoop.hbase.HRegionInfo hri = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(b);\n        byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(row);\n        p.add(org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily(), org.apache.hadoop.hbase.master.TestMasterTransitions.getTestQualifier(), row);\n        t.put(p);\n        rows++;\n    }\n    s.close();\n    org.junit.Assert.assertEquals(expected, rows);\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addToEachStartKey",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testSuperSimple",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 14,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.TestShell.jruby.runScriptlet(PathType.ABSOLUTE, \"src/test/ruby/tests_runner.rb\");\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestShell.TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.msginterval\", 100);\n    org.apache.hadoop.hbase.client.TestShell.TEST_UTIL.getConfiguration().setInt(\"hbase.client.pause\", 250);\n    org.apache.hadoop.hbase.client.TestShell.TEST_UTIL.getConfiguration().setInt(\"hbase.client.retries.number\", 6);\n    org.apache.hadoop.hbase.client.TestShell.TEST_UTIL.startMiniCluster();\n    java.util.List<java.lang.String> loadPaths = new java.util.ArrayList();\n    loadPaths.add(\"src/main/ruby\");\n    loadPaths.add(\"src/test/ruby\");\n    org.apache.hadoop.hbase.client.TestShell.jruby.getProvider().setLoadPaths(loadPaths);\n    org.apache.hadoop.hbase.client.TestShell.jruby.put(\"$TEST_CLUSTER\", org.apache.hadoop.hbase.client.TestShell.TEST_UTIL);\n}",
            "ClassName": "TestShell",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 11,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestShell.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestShell",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    if (!fs.exists(src)) {\n        throw new java.io.FileNotFoundException(src.toString());\n    }\n    if (!fs.rename(src, tgt)) {\n        throw new java.io.IOException(((\"Failed rename of \" + src) + \" to \") + tgt);\n    }\n    return tgt;\n}",
            "ClassName": "StoreFile",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "rename",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 2,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    java.util.List<org.apache.hadoop.hbase.KeyValue> keys = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    org.apache.hadoop.hbase.client.Result r = new org.apache.hadoop.hbase.client.Result(keys);\n    junit.framework.Assert.assertTrue(r.isEmpty());\n    byte[] rb = org.apache.hadoop.hbase.util.Writables.getBytes(r);\n    org.apache.hadoop.hbase.client.Result deserializedR = ((org.apache.hadoop.hbase.client.Result) (org.apache.hadoop.hbase.util.Writables.getWritable(rb, new org.apache.hadoop.hbase.client.Result())));\n    junit.framework.Assert.assertTrue(deserializedR.isEmpty());\n}",
            "ClassName": "TestSerialization",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testResultEmpty",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    for (org.apache.hadoop.hbase.regionserver.StoreFile src : storeFiles) {\n        byte[] family = src.getFamily();\n        java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile> v = byFamily.get(family);\n        if (v == null) {\n            v = new java.util.ArrayList<org.apache.hadoop.hbase.regionserver.StoreFile>();\n            byFamily.put(family, v);\n        }\n        v.add(src);\n    }\n    return byFamily;\n}",
            "ClassName": "HRegion",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "filesByFamily",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestShell",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testRunShellTests",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    testScan(\"yyy\", null, \"zzz\");\n}",
        "CUT_1": {
            "Body": "{\n    return ((x == null) && (y == null)) || ((x != null) && x.equals(y));\n}",
            "ClassName": "Pair",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "equals",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return null;\n}",
            "ClassName": "ScanWildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "internalPreFlushcacheCommit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HalfStoreFileReader",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "midkey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return null;\n}",
            "ClassName": "WildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableInputFormatScan",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testScanYYYToEmpty",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    testScan(null, \"opp\", \"opo\");\n}",
        "CUT_1": {
            "Body": "{\n    return ((x == null) && (y == null)) || ((x != null) && x.equals(y));\n}",
            "ClassName": "Pair",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "equals",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "internalPreFlushcacheCommit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return null;\n}",
            "ClassName": "WildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return null;\n}",
            "ClassName": "ScanWildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HalfStoreFileReader",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "midkey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableInputFormatScan",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testScanEmptyToOPP",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testGetConfiguration\");\n    byte[][] FAMILIES = new byte[][]{ org.apache.hadoop.hbase.util.Bytes.toBytes(\"foo\") };\n    org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration();\n    org.apache.hadoop.hbase.client.HTable table = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, FAMILIES);\n    org.junit.Assert.assertSame(conf, table.getConfiguration());\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(getConfiguration(), tableName);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.ResultScanner resScan = table.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result res : resScan) {\n        org.apache.hadoop.hbase.client.Delete del = new org.apache.hadoop.hbase.client.Delete(res.getRow());\n        table.delete(del);\n    }\n    return table;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "truncateTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 10,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testGetConfiguration",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 7,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.ipc.HBaseServer.Call call = org.apache.hadoop.hbase.ipc.HBaseServer.CurCall.get();\n    if (call != null) {\n        return call.connection.socket.getInetAddress();\n    }\n    return null;\n}",
            "ClassName": "HBaseServer",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getRemoteIp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.HRegion r = org.apache.hadoop.hbase.regionserver.HRegion.newHRegion(org.apache.hadoop.hbase.HTableDescriptor.getTableDir(rootDir, regionInfo.getTableDesc().getName()), this.hlog, this.fs, conf, regionInfo, this.cacheFlusher);\n    r.initialize(null, new org.apache.hadoop.util.Progressable() {\n        public void progress() {\n            addProcessingMessage(regionInfo);\n        }\n    });\n    return r;\n}",
            "ClassName": "HRegionServer",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "instantiateRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return this.lockId;\n}",
            "ClassName": "Get",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getLockId",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    if (!this.changedReaderObservers.remove(o)) {\n        org.apache.hadoop.hbase.regionserver.Store.LOG.warn(\"Not in set\" + o);\n    }\n}",
            "ClassName": "Store",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "deleteChangedReaderObserver",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_5": {
            "Body": "{\n    this.tables = tables;\n}",
            "ClassName": "TableListModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setTables",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testMultipleRegionsAndBatchPuts",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 2,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testJiraTest861\");\n    byte[][] VALUES = makeNAscii(org.apache.hadoop.hbase.client.TestFromClientSide.VALUE, 7);\n    long[] STAMPS = makeStamps(7);\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, 10);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3], VALUES[3]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    ht.put(put);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5]);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5]);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0], VALUES[0]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6], VALUES[6]);\n    ht.put(put);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0], VALUES[0]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3], VALUES[3]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6], VALUES[6]);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0], VALUES[0]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3], VALUES[3]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6], VALUES[6]);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration());\n    admin.split(t.getTableName());\n    java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> regions = waitOnSplit(t);\n    org.junit.Assert.assertTrue(regions.size() > 1);\n    return regions;\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "splitTable",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(family, qualifier);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertNResult(result, row, family, qualifier, stamps, values, start, end);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getAllVersionsAndVerify",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testJiraTest861",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 37,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    testScan(null, \"app\", \"apo\");\n}",
        "CUT_1": {
            "Body": "{\n    return ((x == null) && (y == null)) || ((x != null) && x.equals(y));\n}",
            "ClassName": "Pair",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "equals",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "internalPreFlushcacheCommit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return null;\n}",
            "ClassName": "WildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return null;\n}",
            "ClassName": "ScanWildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HalfStoreFileReader",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "midkey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableInputFormatScan",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testScanEmptyToAPP",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    final byte[] table = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testTableExist\");\n    boolean exist = false;\n    exist = this.admin.tableExists(table);\n    org.junit.Assert.assertEquals(false, exist);\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.createTable(table, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    exist = this.admin.tableExists(table);\n    org.junit.Assert.assertEquals(true, exist);\n}",
        "CUT_1": {
            "Body": "{\n    this.table = table;\n}",
            "ClassName": "TableInputFormatBase",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setHTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return this.table;\n}",
            "ClassName": "TableInputFormatBase",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getHTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    this.admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration());\n}",
            "ClassName": "TestAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    super.setUp();\n    client = new org.apache.hadoop.hbase.rest.client.Client(new org.apache.hadoop.hbase.rest.client.Cluster().add(\"localhost\", testServletPort));\n    admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    if (admin.tableExists(org.apache.hadoop.hbase.rest.TestTableResource.TABLE)) {\n        return;\n    }\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.rest.TestTableResource.TABLE);\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.KeyValue.parseColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestTableResource.COLUMN))[0]));\n    admin.createTable(htd);\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.rest.TestTableResource.TABLE);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testTableExist",
        "NumberOfAsserts": 2,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 9,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    testScan(\"obb\", \"opp\", \"opo\");\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.ipc.HBaseServer.Call call = org.apache.hadoop.hbase.ipc.HBaseServer.CurCall.get();\n    if (call != null) {\n        return call.connection.socket.getInetAddress();\n    }\n    return null;\n}",
            "ClassName": "HBaseServer",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getRemoteIp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.HRegion r = org.apache.hadoop.hbase.regionserver.HRegion.newHRegion(org.apache.hadoop.hbase.HTableDescriptor.getTableDir(rootDir, regionInfo.getTableDesc().getName()), this.hlog, this.fs, conf, regionInfo, this.cacheFlusher);\n    r.initialize(null, new org.apache.hadoop.util.Progressable() {\n        public void progress() {\n            addProcessingMessage(regionInfo);\n        }\n    });\n    return r;\n}",
            "ClassName": "HRegionServer",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "instantiateRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return this.lockId;\n}",
            "ClassName": "Get",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getLockId",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    if (!this.changedReaderObservers.remove(o)) {\n        org.apache.hadoop.hbase.regionserver.Store.LOG.warn(\"Not in set\" + o);\n    }\n}",
            "ClassName": "Store",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "deleteChangedReaderObserver",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_5": {
            "Body": "{\n    this.tables = tables;\n}",
            "ClassName": "TableListModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setTables",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableInputFormatScan",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testScanOBBToOPP",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.ipc.HBaseServer.Call call = org.apache.hadoop.hbase.ipc.HBaseServer.CurCall.get();\n    if (call != null) {\n        return call.connection.socket.getInetAddress();\n    }\n    return null;\n}",
            "ClassName": "HBaseServer",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getRemoteIp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.HRegion r = org.apache.hadoop.hbase.regionserver.HRegion.newHRegion(org.apache.hadoop.hbase.HTableDescriptor.getTableDir(rootDir, regionInfo.getTableDesc().getName()), this.hlog, this.fs, conf, regionInfo, this.cacheFlusher);\n    r.initialize(null, new org.apache.hadoop.util.Progressable() {\n        public void progress() {\n            addProcessingMessage(regionInfo);\n        }\n    });\n    return r;\n}",
            "ClassName": "HRegionServer",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "instantiateRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return this.lockId;\n}",
            "ClassName": "Get",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getLockId",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    if (!this.changedReaderObservers.remove(o)) {\n        org.apache.hadoop.hbase.regionserver.Store.LOG.warn(\"Not in set\" + o);\n    }\n}",
            "ClassName": "Store",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "deleteChangedReaderObserver",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_5": {
            "Body": "{\n    this.tables = tables;\n}",
            "ClassName": "TableListModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setTables",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testMultipleRowMultipleFamily",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 2,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    final byte[] CONTENTS_FAMILY = org.apache.hadoop.hbase.util.Bytes.toBytes(\"contents\");\n    final byte[] value = org.apache.hadoop.hbase.util.Bytes.toBytes(\"abcd\");\n    final byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    final byte[] row2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row2\");\n    byte[] qualifier = org.apache.hadoop.hbase.util.Bytes.toBytes(\"qf1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    org.apache.hadoop.hbase.KeyValue kv = new org.apache.hadoop.hbase.KeyValue(row1, CONTENTS_FAMILY, qualifier, value);\n    boolean ok = true;\n    try {\n        put.add(kv);\n    } catch (java.io.IOException e) {\n        ok = false;\n    }\n    org.junit.Assert.assertEquals(true, ok);\n    kv = new org.apache.hadoop.hbase.KeyValue(row2, CONTENTS_FAMILY, qualifier, value);\n    ok = false;\n    try {\n        put.add(kv);\n    } catch (java.io.IOException e) {\n        ok = true;\n    }\n    org.junit.Assert.assertEquals(true, ok);\n}",
        "CUT_1": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam1\");\n    byte[] fam2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam2\");\n    byte[] qf1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"qualifier\");\n    byte[] val1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"value1\");\n    byte[] val2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"value2\");\n    java.lang.Integer lockId = null;\n    byte[][] families = new byte[][]{ fam1, fam2 };\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, families);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(fam1, qf1, val1);\n    region.put(put);\n    long ts = java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.KeyValue kv = new org.apache.hadoop.hbase.KeyValue(row1, fam2, qf1, ts, org.apache.hadoop.hbase.KeyValue.Type.Put, val2);\n    put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(kv);\n    org.apache.hadoop.hbase.regionserver.Store store = region.getStore(fam1);\n    store.memstore.kvset.size();\n    boolean res = region.checkAndPut(row1, fam1, qf1, val1, put, lockId, true);\n    junit.framework.Assert.assertEquals(true, res);\n    store.memstore.kvset.size();\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row1);\n    get.addColumn(fam2, qf1);\n    org.apache.hadoop.hbase.KeyValue[] actual = region.get(get, null).raw();\n    org.apache.hadoop.hbase.KeyValue[] expected = new org.apache.hadoop.hbase.KeyValue[]{ kv };\n    junit.framework.Assert.assertEquals(expected.length, actual.length);\n    for (int i = 0; i < actual.length; i++) {\n        junit.framework.Assert.assertEquals(expected[i], actual[i]);\n    }\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testCheckAndPut_ThatPutWasWritten",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 33,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"test_table\");\n    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testFamily\");\n    initHRegion(tableName, getName(), family);\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row111\");\n    byte[] row2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row222\");\n    byte[] row3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row333\");\n    byte[] row4 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row444\");\n    byte[] row5 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row555\");\n    byte[] col1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub111\");\n    byte[] col2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub222\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(10L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row2);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(15L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row3);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(20L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row4);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(30L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row5);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(40L));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row3, row4);\n    scan.setMaxVersions();\n    scan.addColumn(family, col1);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    junit.framework.Assert.assertEquals(false, s.next(results));\n    junit.framework.Assert.assertEquals(0, results.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_StopRow1542",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 34,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam1\");\n    byte[] qf1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"qualifier\");\n    byte[] val1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"value1\");\n    byte[] val2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"value2\");\n    java.lang.Integer lockId = null;\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, fam1);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(fam1, qf1, val1);\n    region.put(put);\n    boolean res = region.checkAndPut(row1, fam1, qf1, val2, put, lockId, true);\n    junit.framework.Assert.assertEquals(false, res);\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testCheckAndPut_WithWrongValue",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    byte[] qf1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"qualifier1\");\n    byte[] qf2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"qualifier2\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam1\");\n    byte[][] families = new byte[][]{ fam1 };\n    long ts1 = java.lang.System.currentTimeMillis();\n    long ts2 = ts1 + 1;\n    long ts3 = ts1 + 2;\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, families);\n    org.apache.hadoop.hbase.client.Put put = null;\n    org.apache.hadoop.hbase.KeyValue kv13 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf1, ts3, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv12 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf1, ts2, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv11 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf1, ts1, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv23 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf2, ts3, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv22 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf2, ts2, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv21 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf2, ts1, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(kv13);\n    put.add(kv12);\n    put.add(kv11);\n    put.add(kv23);\n    put.add(kv22);\n    put.add(kv21);\n    region.put(put);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> expected = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    expected.add(kv13);\n    expected.add(kv12);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row1);\n    scan.addColumn(fam1, qf1);\n    scan.setMaxVersions(MAX_VERSIONS);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> actual = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    org.apache.hadoop.hbase.regionserver.InternalScanner scanner = region.getScanner(scan);\n    boolean hasNext = scanner.next(actual);\n    junit.framework.Assert.assertEquals(false, hasNext);\n    for (int i = 0; i < expected.size(); i++) {\n        junit.framework.Assert.assertEquals(expected.get(i), actual.get(i));\n    }\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_ExplicitColumns_FromMemStore_EnforceVersions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 41,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam1\");\n    byte[] qf1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"qualifier1\");\n    byte[] qf2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"quateslifier2\");\n    long ts1 = 1;\n    long ts2 = ts1 + 1;\n    long ts3 = ts1 + 2;\n    long ts4 = ts1 + 3;\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, fam1);\n    org.apache.hadoop.hbase.KeyValue kv14 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf1, ts4, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv13 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf1, ts3, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv12 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf1, ts2, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv11 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf1, ts1, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv24 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf2, ts4, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv23 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf2, ts3, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv22 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf2, ts2, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv21 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf2, ts1, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.client.Put put = null;\n    put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(kv14);\n    put.add(kv24);\n    region.put(put);\n    region.flushcache();\n    put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(kv23);\n    put.add(kv13);\n    region.put(put);\n    region.flushcache();\n    put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(kv22);\n    put.add(kv12);\n    region.put(put);\n    region.flushcache();\n    put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(kv21);\n    put.add(kv11);\n    region.put(put);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> expected = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    expected.add(kv14);\n    expected.add(kv13);\n    expected.add(kv12);\n    expected.add(kv24);\n    expected.add(kv23);\n    expected.add(kv22);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row1);\n    int versions = 3;\n    scan.setMaxVersions(versions);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> actual = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    org.apache.hadoop.hbase.regionserver.InternalScanner scanner = region.getScanner(scan);\n    boolean hasNext = scanner.next(actual);\n    junit.framework.Assert.assertEquals(false, hasNext);\n    for (int i = 0; i < expected.size(); i++) {\n        junit.framework.Assert.assertEquals(expected.get(i), actual.get(i));\n    }\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_Wildcard_FromMemStoreAndFiles_EnforceVersions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 58,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testAddKeyValue",
        "NumberOfAsserts": 2,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 24,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testSingleRowMultipleFamily\");\n    byte[][] ROWS = makeN(org.apache.hadoop.hbase.client.TestFromClientSide.ROW, 3);\n    byte[][] FAMILIES = makeNAscii(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, 10);\n    byte[][] QUALIFIERS = makeN(org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, 10);\n    byte[][] VALUES = makeN(org.apache.hadoop.hbase.client.TestFromClientSide.VALUE, 10);\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, FAMILIES);\n    org.apache.hadoop.hbase.client.Get get;\n    org.apache.hadoop.hbase.client.Scan scan;\n    org.apache.hadoop.hbase.client.Delete delete;\n    org.apache.hadoop.hbase.client.Put put;\n    org.apache.hadoop.hbase.client.Result result;\n    put = new org.apache.hadoop.hbase.client.Put(ROWS[0]);\n    put.add(FAMILIES[4], QUALIFIERS[0], VALUES[0]);\n    ht.put(put);\n    getVerifySingleColumn(ht, ROWS, 0, FAMILIES, 4, QUALIFIERS, 0, VALUES, 0);\n    scanVerifySingleColumn(ht, ROWS, 0, FAMILIES, 4, QUALIFIERS, 0, VALUES, 0);\n    getVerifySingleEmpty(ht, ROWS, 0, FAMILIES, 4, QUALIFIERS, 0);\n    scanVerifySingleEmpty(ht, ROWS, 0, FAMILIES, 4, QUALIFIERS, 0);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    getVerifySingleColumn(ht, ROWS, 0, FAMILIES, 4, QUALIFIERS, 0, VALUES, 0);\n    scanVerifySingleColumn(ht, ROWS, 0, FAMILIES, 4, QUALIFIERS, 0, VALUES, 0);\n    getVerifySingleEmpty(ht, ROWS, 0, FAMILIES, 4, QUALIFIERS, 0);\n    scanVerifySingleEmpty(ht, ROWS, 0, FAMILIES, 4, QUALIFIERS, 0);\n    put = new org.apache.hadoop.hbase.client.Put(ROWS[0]);\n    put.add(FAMILIES[2], QUALIFIERS[2], VALUES[2]);\n    put.add(FAMILIES[2], QUALIFIERS[4], VALUES[4]);\n    put.add(FAMILIES[4], QUALIFIERS[4], VALUES[4]);\n    put.add(FAMILIES[6], QUALIFIERS[6], VALUES[6]);\n    put.add(FAMILIES[6], QUALIFIERS[7], VALUES[7]);\n    put.add(FAMILIES[7], QUALIFIERS[7], VALUES[7]);\n    put.add(FAMILIES[9], QUALIFIERS[0], VALUES[0]);\n    ht.put(put);\n    singleRowGetTest(ht, ROWS, FAMILIES, QUALIFIERS, VALUES);\n    singleRowScanTest(ht, ROWS, FAMILIES, QUALIFIERS, VALUES);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    singleRowGetTest(ht, ROWS, FAMILIES, QUALIFIERS, VALUES);\n    singleRowScanTest(ht, ROWS, FAMILIES, QUALIFIERS, VALUES);\n    put = new org.apache.hadoop.hbase.client.Put(ROWS[0]);\n    put.add(FAMILIES[6], QUALIFIERS[5], VALUES[5]);\n    put.add(FAMILIES[6], QUALIFIERS[8], VALUES[8]);\n    put.add(FAMILIES[6], QUALIFIERS[9], VALUES[9]);\n    put.add(FAMILIES[4], QUALIFIERS[3], VALUES[3]);\n    ht.put(put);\n    delete = new org.apache.hadoop.hbase.client.Delete(ROWS[0]);\n    delete.deleteColumns(FAMILIES[6], QUALIFIERS[7]);\n    ht.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[6], QUALIFIERS[7]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[6], QUALIFIERS[7]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[6], QUALIFIERS[6]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[6], VALUES[6]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[6], QUALIFIERS[8]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[8], VALUES[8]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[6], QUALIFIERS[6]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[6], VALUES[6]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[6], QUALIFIERS[8]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[8], VALUES[8]);\n    delete = new org.apache.hadoop.hbase.client.Delete(ROWS[0]);\n    delete.deleteColumns(FAMILIES[6], QUALIFIERS[8]);\n    ht.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[6], QUALIFIERS[8]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[6], QUALIFIERS[8]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[6], QUALIFIERS[6]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[6], VALUES[6]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[6], QUALIFIERS[9]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[9], VALUES[9]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[6], QUALIFIERS[6]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[6], VALUES[6]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[6], QUALIFIERS[9]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[9], VALUES[9]);\n    delete = new org.apache.hadoop.hbase.client.Delete(ROWS[0]);\n    delete.deleteFamily(FAMILIES[4]);\n    ht.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[4]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[3]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(FAMILIES[4]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[4], QUALIFIERS[4]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[4], QUALIFIERS[3]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(FAMILIES[4]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[2], QUALIFIERS[2]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[2], QUALIFIERS[2], VALUES[2]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[6], QUALIFIERS[9]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[9], VALUES[9]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[6], QUALIFIERS[6]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[6], VALUES[6]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[6], QUALIFIERS[9]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[9], VALUES[9]);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[4]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[3]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(FAMILIES[4]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[4], QUALIFIERS[4]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[4], QUALIFIERS[3]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(FAMILIES[4]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[2], QUALIFIERS[2]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[2], QUALIFIERS[2], VALUES[2]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[6], QUALIFIERS[9]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[9], VALUES[9]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[6], QUALIFIERS[6]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[6], VALUES[6]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[6], QUALIFIERS[9]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[9], VALUES[9]);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[0]);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[4], QUALIFIERS[0], VALUES[0]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[2], QUALIFIERS[2]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[2], QUALIFIERS[2], VALUES[2]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(FAMILIES[7]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[7], QUALIFIERS[7], VALUES[7]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(FAMILIES[4]);\n    result = ht.get(get);\n    assertDoubleResult(result, ROWS[0], FAMILIES[4], QUALIFIERS[0], VALUES[0], FAMILIES[4], QUALIFIERS[4], VALUES[4]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[4]);\n    result = ht.get(get);\n    assertDoubleResult(result, ROWS[0], FAMILIES[4], QUALIFIERS[0], VALUES[0], FAMILIES[4], QUALIFIERS[4], VALUES[4]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(FAMILIES[4]);\n    get.addFamily(FAMILIES[7]);\n    result = ht.get(get);\n    assertNResult(result, ROWS[0], FAMILIES, QUALIFIERS, VALUES, new int[][]{ new int[]{ 4, 0, 0 }, new int[]{ 4, 4, 4 }, new int[]{ 7, 7, 7 } });\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(FAMILIES[2]);\n    get.addFamily(FAMILIES[4]);\n    get.addFamily(FAMILIES[6]);\n    get.addFamily(FAMILIES[7]);\n    result = ht.get(get);\n    assertNResult(result, ROWS[0], FAMILIES, QUALIFIERS, VALUES, new int[][]{ new int[]{ 2, 2, 2 }, new int[]{ 2, 4, 4 }, new int[]{ 4, 0, 0 }, new int[]{ 4, 4, 4 }, new int[]{ 6, 6, 6 }, new int[]{ 6, 7, 7 }, new int[]{ 7, 7, 7 } });\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[2], QUALIFIERS[2]);\n    get.addColumn(FAMILIES[2], QUALIFIERS[4]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[4]);\n    get.addColumn(FAMILIES[6], QUALIFIERS[6]);\n    get.addColumn(FAMILIES[6], QUALIFIERS[7]);\n    get.addColumn(FAMILIES[7], QUALIFIERS[7]);\n    get.addColumn(FAMILIES[7], QUALIFIERS[8]);\n    result = ht.get(get);\n    assertNResult(result, ROWS[0], FAMILIES, QUALIFIERS, VALUES, new int[][]{ new int[]{ 2, 2, 2 }, new int[]{ 2, 4, 4 }, new int[]{ 4, 0, 0 }, new int[]{ 4, 4, 4 }, new int[]{ 6, 6, 6 }, new int[]{ 6, 7, 7 }, new int[]{ 7, 7, 7 } });\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    result = ht.get(get);\n    assertNResult(result, ROWS[0], FAMILIES, QUALIFIERS, VALUES, new int[][]{ new int[]{ 2, 2, 2 }, new int[]{ 2, 4, 4 }, new int[]{ 4, 0, 0 }, new int[]{ 4, 4, 4 }, new int[]{ 6, 6, 6 }, new int[]{ 6, 7, 7 }, new int[]{ 7, 7, 7 }, new int[]{ 9, 0, 0 } });\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[1]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[3]);\n    get.addColumn(FAMILIES[2], QUALIFIERS[3]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "singleRowGetTest",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 57,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[4], QUALIFIERS[0]);\n    org.apache.hadoop.hbase.client.Result result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[0], FAMILIES[4], QUALIFIERS[0], VALUES[0]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[2], QUALIFIERS[2]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[0], FAMILIES[2], QUALIFIERS[2], VALUES[2]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(FAMILIES[7]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[0], FAMILIES[7], QUALIFIERS[7], VALUES[7]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(FAMILIES[4]);\n    result = getSingleScanResult(ht, scan);\n    assertDoubleResult(result, ROWS[0], FAMILIES[4], QUALIFIERS[0], VALUES[0], FAMILIES[4], QUALIFIERS[4], VALUES[4]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[4], QUALIFIERS[0]);\n    scan.addColumn(FAMILIES[4], QUALIFIERS[4]);\n    result = getSingleScanResult(ht, scan);\n    assertDoubleResult(result, ROWS[0], FAMILIES[4], QUALIFIERS[0], VALUES[0], FAMILIES[4], QUALIFIERS[4], VALUES[4]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(FAMILIES[4]);\n    scan.addFamily(FAMILIES[7]);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, ROWS[0], FAMILIES, QUALIFIERS, VALUES, new int[][]{ new int[]{ 4, 0, 0 }, new int[]{ 4, 4, 4 }, new int[]{ 7, 7, 7 } });\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(FAMILIES[2]);\n    scan.addFamily(FAMILIES[4]);\n    scan.addFamily(FAMILIES[6]);\n    scan.addFamily(FAMILIES[7]);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, ROWS[0], FAMILIES, QUALIFIERS, VALUES, new int[][]{ new int[]{ 2, 2, 2 }, new int[]{ 2, 4, 4 }, new int[]{ 4, 0, 0 }, new int[]{ 4, 4, 4 }, new int[]{ 6, 6, 6 }, new int[]{ 6, 7, 7 }, new int[]{ 7, 7, 7 } });\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[2], QUALIFIERS[2]);\n    scan.addColumn(FAMILIES[2], QUALIFIERS[4]);\n    scan.addColumn(FAMILIES[4], QUALIFIERS[0]);\n    scan.addColumn(FAMILIES[4], QUALIFIERS[4]);\n    scan.addColumn(FAMILIES[6], QUALIFIERS[6]);\n    scan.addColumn(FAMILIES[6], QUALIFIERS[7]);\n    scan.addColumn(FAMILIES[7], QUALIFIERS[7]);\n    scan.addColumn(FAMILIES[7], QUALIFIERS[8]);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, ROWS[0], FAMILIES, QUALIFIERS, VALUES, new int[][]{ new int[]{ 2, 2, 2 }, new int[]{ 2, 4, 4 }, new int[]{ 4, 0, 0 }, new int[]{ 4, 4, 4 }, new int[]{ 6, 6, 6 }, new int[]{ 6, 7, 7 }, new int[]{ 7, 7, 7 } });\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, ROWS[0], FAMILIES, QUALIFIERS, VALUES, new int[][]{ new int[]{ 2, 2, 2 }, new int[]{ 2, 4, 4 }, new int[]{ 4, 0, 0 }, new int[]{ 4, 4, 4 }, new int[]{ 6, 6, 6 }, new int[]{ 6, 7, 7 }, new int[]{ 7, 7, 7 }, new int[]{ 9, 0, 0 } });\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[1]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[4], QUALIFIERS[3]);\n    scan.addColumn(FAMILIES[2], QUALIFIERS[3]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "singleRowScanTest",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 57,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(ROWS[ROWIDX]);\n    get.addFamily(FAMILIES[4]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[1]);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[ROWIDX]);\n    get.addFamily(FAMILIES[4]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[2]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[ROWIDX]);\n    get.addFamily(FAMILIES[3]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[2]);\n    get.addFamily(FAMILIES[5]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[ROWIDX + 1]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getVerifySingleEmpty",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.Result result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[ROWIDX], FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX], VALUES[VALUEIDX]);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[ROWIDX]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[ROWIDX], FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX], VALUES[VALUEIDX]);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[ROWIDX], ROWS[ROWIDX + 1]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[ROWIDX], FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX], VALUES[VALUEIDX]);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, ROWS[ROWIDX + 1]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[ROWIDX], FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX], VALUES[VALUEIDX]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(FAMILIES[FAMILYIDX]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[ROWIDX], FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX], VALUES[VALUEIDX]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[ROWIDX], FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX], VALUES[VALUEIDX]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX + 1]);\n    scan.addFamily(FAMILIES[FAMILYIDX]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[ROWIDX], FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX], VALUES[VALUEIDX]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[FAMILYIDX - 1], QUALIFIERS[QUALIFIERIDX + 1]);\n    scan.addColumn(FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX]);\n    scan.addFamily(FAMILIES[FAMILYIDX + 1]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[ROWIDX], FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX], VALUES[VALUEIDX]);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "scanVerifySingleColumn",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 33,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(ROWS[ROWIDX + 1]);\n    org.apache.hadoop.hbase.client.Result result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[ROWIDX + 1], ROWS[ROWIDX + 2]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, ROWS[ROWIDX]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX + 1]);\n    scan.addFamily(FAMILIES[FAMILYIDX - 1]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "scanVerifySingleEmpty",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testSingleRowMultipleFamily",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 183,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testFilters\");\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    byte[][] ROWS = makeN(org.apache.hadoop.hbase.client.TestFromClientSide.ROW, 10);\n    byte[][] QUALIFIERS = new byte[][]{ org.apache.hadoop.hbase.util.Bytes.toBytes(\"col0-<d2v1>-<d3v2>\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col1-<d2v1>-<d3v2>\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col2-<d2v1>-<d3v2>\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col3-<d2v1>-<d3v2>\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col4-<d2v1>-<d3v2>\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col5-<d2v1>-<d3v2>\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col6-<d2v1>-<d3v2>\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col7-<d2v1>-<d3v2>\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col8-<d2v1>-<d3v2>\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col9-<d2v1>-<d3v2>\") };\n    for (int i = 0; i < 10; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(ROWS[i]);\n        put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, QUALIFIERS[i], org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n        ht.put(put);\n    }\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.apache.hadoop.hbase.filter.Filter filter = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\"col[1-5]\"));\n    scan.setFilter(filter);\n    org.apache.hadoop.hbase.client.ResultScanner scanner = ht.getScanner(scan);\n    int expectedIndex = 1;\n    for (org.apache.hadoop.hbase.client.Result result : ht.getScanner(scan)) {\n        org.junit.Assert.assertEquals(result.size(), 1);\n        org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(result.raw()[0].getRow(), ROWS[expectedIndex]));\n        org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(result.raw()[0].getQualifier(), QUALIFIERS[expectedIndex]));\n        expectedIndex++;\n    }\n    org.junit.Assert.assertEquals(expectedIndex, 6);\n    scanner.close();\n}",
        "CUT_1": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testIndexesScanWithOneDeletedRow\");\n    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"family\");\n    java.lang.String method = \"testIndexesScanWithOneDeletedRow\";\n    initHRegion(tableName, method, new org.apache.hadoop.hbase.HBaseConfiguration(), family);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(1L));\n    put.add(family, qual1, 1L, org.apache.hadoop.hbase.util.Bytes.toBytes(1L));\n    region.put(put);\n    region.flushcache();\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.util.Bytes.toBytes(1L), 1L, null);\n    region.delete(delete, null, true);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(2L));\n    put.add(family, qual1, 2L, org.apache.hadoop.hbase.util.Bytes.toBytes(2L));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan idxScan = new org.apache.hadoop.hbase.client.Scan();\n    idxScan.addFamily(family);\n    idxScan.setFilter(new org.apache.hadoop.hbase.filter.FilterList(org.apache.hadoop.hbase.filter.FilterList.Operator.MUST_PASS_ALL, java.util.Arrays.<org.apache.hadoop.hbase.filter.Filter>asList(new org.apache.hadoop.hbase.filter.SingleColumnValueFilter(family, qual1, org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(0L))), new org.apache.hadoop.hbase.filter.SingleColumnValueFilter(family, qual1, org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(3L))))));\n    org.apache.hadoop.hbase.regionserver.InternalScanner scanner = region.getScanner(idxScan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> res = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    while (scanner.next(res));\n    junit.framework.Assert.assertEquals(1L, res.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testIndexesScanWithOneDeletedRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(row), org.apache.hadoop.hbase.util.Bytes.toBytes(family), org.apache.hadoop.hbase.util.Bytes.toBytes(qualifier), timestamp, type, org.apache.hadoop.hbase.util.Bytes.toBytes(value));\n}",
            "ClassName": "KeyValueTestUtil",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "create",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"test_table\");\n    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testFamily\");\n    initHRegion(tableName, getName(), family);\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row111\");\n    byte[] row2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row222\");\n    byte[] row3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row333\");\n    byte[] row4 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row444\");\n    byte[] row5 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row555\");\n    byte[] col1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub111\");\n    byte[] col2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub222\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(10L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row2);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(15L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row3);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(20L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row4);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(30L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row5);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(40L));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row3, row4);\n    scan.setMaxVersions();\n    scan.addColumn(family, col1);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    junit.framework.Assert.assertEquals(false, s.next(results));\n    junit.framework.Assert.assertEquals(0, results.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_StopRow1542",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 34,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 2,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testFilters",
        "NumberOfAsserts": 4,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 25,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    final byte[] CONTENTS_FAMILY = org.apache.hadoop.hbase.util.Bytes.toBytes(\"contents\");\n    final byte[] SMALL_FAMILY = org.apache.hadoop.hbase.util.Bytes.toBytes(\"smallfam\");\n    final byte[] value = org.apache.hadoop.hbase.util.Bytes.toBytes(\"abcd\");\n    final int NB_BATCH_ROWS = 10;\n    org.apache.hadoop.hbase.client.HTable table = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowsPutBufferedOneFlush\"), new byte[][]{ CONTENTS_FAMILY, SMALL_FAMILY });\n    table.setAutoFlush(false);\n    java.util.ArrayList<org.apache.hadoop.hbase.client.Put> rowsUpdate = new java.util.ArrayList<org.apache.hadoop.hbase.client.Put>();\n    for (int i = 0; i < (NB_BATCH_ROWS * 10); i++) {\n        byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + i);\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n        put.add(CONTENTS_FAMILY, null, value);\n        rowsUpdate.add(put);\n    }\n    table.put(rowsUpdate);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(CONTENTS_FAMILY);\n    org.apache.hadoop.hbase.client.ResultScanner scanner = table.getScanner(scan);\n    int nbRows = 0;\n    for (@java.lang.SuppressWarnings(\"unused\")\n    org.apache.hadoop.hbase.client.Result row : scanner)\n        nbRows++;\n\n    org.junit.Assert.assertEquals(0, nbRows);\n    scanner.close();\n    table.flushCommits();\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(CONTENTS_FAMILY);\n    scanner = table.getScanner(scan);\n    nbRows = 0;\n    for (@java.lang.SuppressWarnings(\"unused\")\n    org.apache.hadoop.hbase.client.Result row : scanner)\n        nbRows++;\n\n    org.junit.Assert.assertEquals(NB_BATCH_ROWS * 10, nbRows);\n}",
        "CUT_1": {
            "Body": "{\n    java.lang.Runnable runnable = new java.lang.Runnable() {\n        public void run() {\n            try {\n                org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n                org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n                scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n                org.apache.hadoop.hbase.client.ResultScanner s = t.getScanner(scan);\n                s.close();\n            } catch (java.io.IOException e) {\n                LOG.fatal(\"could not re-open meta table because\", e);\n                junit.framework.Assert.fail();\n            }\n            org.apache.hadoop.hbase.client.ResultScanner scanner = null;\n            try {\n                org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n                scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n                scanner = table.getScanner(scan);\n                LOG.info(\"Obtained scanner \" + scanner);\n                for (org.apache.hadoop.hbase.client.Result r : scanner) {\n                    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(r.getRow(), row));\n                    junit.framework.Assert.assertEquals(1, r.size());\n                    byte[] bytes = r.value();\n                    junit.framework.Assert.assertNotNull(bytes);\n                    junit.framework.Assert.assertTrue(tableName.equals(org.apache.hadoop.hbase.util.Bytes.toString(bytes)));\n                }\n                LOG.info(\"Success!\");\n            } catch (java.lang.Exception e) {\n                e.printStackTrace();\n                junit.framework.Assert.fail();\n            } finally {\n                if (scanner != null) {\n                    LOG.info(\"Closing scanner \" + scanner);\n                    scanner.close();\n                }\n            }\n        }\n    };\n    return new java.lang.Thread(runnable);\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startVerificationThread",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 40,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"test_table\");\n    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testFamily\");\n    initHRegion(tableName, getName(), family);\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row111\");\n    byte[] row2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row222\");\n    byte[] row3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row333\");\n    byte[] row4 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row444\");\n    byte[] row5 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row555\");\n    byte[] col1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub111\");\n    byte[] col2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub222\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(10L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row2);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(15L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row3);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(20L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row4);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(30L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row5);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(40L));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row3, row4);\n    scan.setMaxVersions();\n    scan.addColumn(family, col1);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    junit.framework.Assert.assertEquals(false, s.next(results));\n    junit.framework.Assert.assertEquals(0, results.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_StopRow1542",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 34,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(getConfiguration(), tableName);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.ResultScanner resScan = table.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result res : resScan) {\n        org.apache.hadoop.hbase.client.Delete del = new org.apache.hadoop.hbase.client.Delete(res.getRow());\n        table.delete(del);\n    }\n    return table;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "truncateTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 10,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 3,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testRowsPutBufferedOneFlush",
        "NumberOfAsserts": 2,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 36,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    int numRows = 10;\n    int numColsPerRow = 2000;\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testJiraTest867\");\n    byte[][] ROWS = makeN(org.apache.hadoop.hbase.client.TestFromClientSide.ROW, numRows);\n    byte[][] QUALIFIERS = makeN(org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, numColsPerRow);\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    for (int i = 0; i < numRows; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(ROWS[i]);\n        for (int j = 0; j < numColsPerRow; j++) {\n            put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, QUALIFIERS[j], QUALIFIERS[j]);\n        }\n        org.junit.Assert.assertTrue((((\"Put expected to contain \" + numColsPerRow) + \" columns but \") + \"only contains \") + put.size(), put.size() == numColsPerRow);\n        ht.put(put);\n    }\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(ROWS[numRows - 1]);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertNumKeys(result, numColsPerRow);\n    org.apache.hadoop.hbase.KeyValue[] keys = result.sorted();\n    for (int i = 0; i < result.size(); i++) {\n        assertKey(keys[i], ROWS[numRows - 1], org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, QUALIFIERS[i], QUALIFIERS[i]);\n    }\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.ResultScanner scanner = ht.getScanner(scan);\n    int rowCount = 0;\n    while ((result = scanner.next()) != null) {\n        assertNumKeys(result, numColsPerRow);\n        org.apache.hadoop.hbase.KeyValue[] kvs = result.sorted();\n        for (int i = 0; i < numColsPerRow; i++) {\n            assertKey(kvs[i], ROWS[rowCount], org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, QUALIFIERS[i], QUALIFIERS[i]);\n        }\n        rowCount++;\n    } \n    scanner.close();\n    org.junit.Assert.assertTrue((((\"Expected to scan \" + numRows) + \" rows but actually scanned \") + rowCount) + \" rows\", rowCount == numRows);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[numRows - 1]);\n    result = ht.get(get);\n    assertNumKeys(result, numColsPerRow);\n    keys = result.sorted();\n    for (int i = 0; i < result.size(); i++) {\n        assertKey(keys[i], ROWS[numRows - 1], org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, QUALIFIERS[i], QUALIFIERS[i]);\n    }\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scanner = ht.getScanner(scan);\n    rowCount = 0;\n    while ((result = scanner.next()) != null) {\n        assertNumKeys(result, numColsPerRow);\n        org.apache.hadoop.hbase.KeyValue[] kvs = result.sorted();\n        for (int i = 0; i < numColsPerRow; i++) {\n            assertKey(kvs[i], ROWS[rowCount], org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, QUALIFIERS[i], QUALIFIERS[i]);\n        }\n        rowCount++;\n    } \n    scanner.close();\n    org.junit.Assert.assertTrue((((\"Expected to scan \" + numRows) + \" rows but actually scanned \") + rowCount) + \" rows\", rowCount == numRows);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(this.conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    java.util.List<byte[]> rows = new java.util.ArrayList<byte[]>();\n    org.apache.hadoop.hbase.client.ResultScanner s = t.getScanner(new org.apache.hadoop.hbase.client.Scan());\n    for (org.apache.hadoop.hbase.client.Result result : s) {\n        LOG.info(\"getMetaTableRows: row -> \" + org.apache.hadoop.hbase.util.Bytes.toStringBinary(result.getRow()));\n        rows.add(result.getRow());\n    }\n    s.close();\n    return rows;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getMetaTableRows",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 11,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.util.TestMergeTool.FAMILY);\n    org.apache.hadoop.hbase.regionserver.InternalScanner scanner = merged.getScanner(scan);\n    try {\n        java.util.List<org.apache.hadoop.hbase.KeyValue> testRes = null;\n        while (true) {\n            testRes = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n            boolean hasNext = scanner.next(testRes);\n            if (!hasNext) {\n                break;\n            }\n        } \n    } finally {\n        scanner.close();\n    }\n    for (int i = 0; i < upperbound; i++) {\n        for (int j = 0; j < rows[i].length; j++) {\n            org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(rows[i][j]);\n            get.addFamily(org.apache.hadoop.hbase.util.TestMergeTool.FAMILY);\n            org.apache.hadoop.hbase.client.Result result = merged.get(get, null);\n            junit.framework.Assert.assertEquals(1, result.size());\n            byte[] bytes = result.sorted()[0].getValue();\n            junit.framework.Assert.assertNotNull(org.apache.hadoop.hbase.util.Bytes.toStringBinary(rows[i][j]), bytes);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(bytes, rows[i][j]));\n        }\n    }\n}",
            "ClassName": "TestMergeTool",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "verifyMerge",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 28,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.ResultScanner s = t.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        rows++;\n    }\n    s.close();\n    org.apache.hadoop.hbase.master.TestMasterTransitions.LOG.info(\"Counted=\" + rows);\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "count",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        rows++;\n    }\n    s.close();\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "countOfMetaRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 8,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testJiraTest867",
        "NumberOfAsserts": 3,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 57,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    int initialCount = 10;\n    org.apache.hadoop.hbase.client.HTable t = org.apache.hadoop.hbase.client.TestScannerTimeout.TEST_UTIL.createTable(org.apache.hadoop.hbase.util.Bytes.toBytes(\"t\"), someBytes);\n    for (int i = 0; i < initialCount; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(i));\n        put.add(someBytes, someBytes, someBytes);\n        t.put(put);\n    }\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.ResultScanner r = t.getScanner(scan);\n    int count = 0;\n    try {\n        org.apache.hadoop.hbase.client.Result res = r.next();\n        while (res != null) {\n            count++;\n            if (count == 5) {\n                java.lang.Thread.sleep(1500);\n            }\n            res = r.next();\n        } \n    } catch (org.apache.hadoop.hbase.client.ScannerTimeoutException e) {\n        LOG.info(\"Got the timeout \" + e.getMessage(), e);\n        return;\n    }\n    org.junit.Assert.fail(\"We should be timing out\");\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    final int COUNT = 5;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(getName());\n    for (int i = 0; i < COUNT; i++) {\n        byte[] regionName = org.apache.hadoop.hbase.HRegionInfo.createRegionName(tableName, org.apache.hadoop.hbase.util.Bytes.toBytes(i == 0 ? \"\" : java.lang.Integer.toString(i)), java.lang.Long.toString(java.lang.System.currentTimeMillis()));\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(regionName);\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER, org.apache.hadoop.hbase.util.Bytes.toBytes(\"localhost:1234\"));\n        t.put(put);\n    }\n    long sleepTime = conf.getLong(\"hbase.master.meta.thread.rescanfrequency\", 10000);\n    int tries = conf.getInt(\"hbase.client.retries.number\", 5);\n    int count = 0;\n    do {\n        tries -= 1;\n        try {\n            java.lang.Thread.sleep(sleepTime);\n        } catch (java.lang.InterruptedException e) {\n        }\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITA_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITB_QUALIFIER);\n        org.apache.hadoop.hbase.client.ResultScanner scanner = t.getScanner(scan);\n        try {\n            count = 0;\n            org.apache.hadoop.hbase.client.Result r;\n            while ((r = scanner.next()) != null) {\n                if (!r.isEmpty()) {\n                    count += 1;\n                }\n            } \n        } finally {\n            scanner.close();\n        }\n    } while ((count != 0) && (tries >= 0) );\n    junit.framework.Assert.assertTrue(tries >= 0);\n    junit.framework.Assert.assertEquals(0, count);\n}",
            "ClassName": "TestEmptyMetaInfo",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testEmptyMetaInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 41,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(getConfiguration(), tableName);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.ResultScanner resScan = table.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result res : resScan) {\n        org.apache.hadoop.hbase.client.Delete del = new org.apache.hadoop.hbase.client.Delete(res.getRow());\n        table.delete(del);\n    }\n    return table;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "truncateTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 10,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    this.server = cluster.getRegionServerThreads().get(0).getRegionServer();\n    this.log = server.getLog();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    for (int i = 1; i <= 256; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + java.lang.String.format(\"%1$04d\", i)));\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, value);\n        table.put(put);\n        if ((i % 32) == 0) {\n            try {\n                java.lang.Thread.sleep(2000);\n            } catch (java.lang.InterruptedException e) {\n            }\n        }\n    }\n}",
            "ClassName": "TestLogRolling",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startAndWriteData",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        org.apache.hadoop.hbase.HRegionInfo hri = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(b);\n        byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(row);\n        p.add(org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily(), org.apache.hadoop.hbase.master.TestMasterTransitions.getTestQualifier(), row);\n        t.put(p);\n        rows++;\n    }\n    s.close();\n    org.junit.Assert.assertEquals(expected, rows);\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addToEachStartKey",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestScannerTimeout",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 3,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "test2481",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 1,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 26,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 2,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    testScan(\"obb\", \"qpp\", \"qpo\");\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.ipc.HBaseServer.Call call = org.apache.hadoop.hbase.ipc.HBaseServer.CurCall.get();\n    if (call != null) {\n        return call.connection.socket.getInetAddress();\n    }\n    return null;\n}",
            "ClassName": "HBaseServer",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getRemoteIp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.HRegion r = org.apache.hadoop.hbase.regionserver.HRegion.newHRegion(org.apache.hadoop.hbase.HTableDescriptor.getTableDir(rootDir, regionInfo.getTableDesc().getName()), this.hlog, this.fs, conf, regionInfo, this.cacheFlusher);\n    r.initialize(null, new org.apache.hadoop.util.Progressable() {\n        public void progress() {\n            addProcessingMessage(regionInfo);\n        }\n    });\n    return r;\n}",
            "ClassName": "HRegionServer",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "instantiateRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return this.lockId;\n}",
            "ClassName": "Get",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getLockId",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    if (!this.changedReaderObservers.remove(o)) {\n        org.apache.hadoop.hbase.regionserver.Store.LOG.warn(\"Not in set\" + o);\n    }\n}",
            "ClassName": "Store",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "deleteChangedReaderObserver",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_5": {
            "Body": "{\n    this.tables = tables;\n}",
            "ClassName": "TableListModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setTables",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableInputFormatScan",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testScanOBBToQPP",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    final byte[] name = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testTableNotFoundExceptionWithATable\");\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.createTable(name, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.createTable(name, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.msginterval\", 100);\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration().setInt(\"hbase.client.pause\", 250);\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration().setInt(\"hbase.client.retries.number\", 6);\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    this.admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration());\n}",
            "ClassName": "TestAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        rows++;\n    }\n    s.close();\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "countOfMetaRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    final int COUNT = 5;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(getName());\n    for (int i = 0; i < COUNT; i++) {\n        byte[] regionName = org.apache.hadoop.hbase.HRegionInfo.createRegionName(tableName, org.apache.hadoop.hbase.util.Bytes.toBytes(i == 0 ? \"\" : java.lang.Integer.toString(i)), java.lang.Long.toString(java.lang.System.currentTimeMillis()));\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(regionName);\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER, org.apache.hadoop.hbase.util.Bytes.toBytes(\"localhost:1234\"));\n        t.put(put);\n    }\n    long sleepTime = conf.getLong(\"hbase.master.meta.thread.rescanfrequency\", 10000);\n    int tries = conf.getInt(\"hbase.client.retries.number\", 5);\n    int count = 0;\n    do {\n        tries -= 1;\n        try {\n            java.lang.Thread.sleep(sleepTime);\n        } catch (java.lang.InterruptedException e) {\n        }\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITA_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITB_QUALIFIER);\n        org.apache.hadoop.hbase.client.ResultScanner scanner = t.getScanner(scan);\n        try {\n            count = 0;\n            org.apache.hadoop.hbase.client.Result r;\n            while ((r = scanner.next()) != null) {\n                if (!r.isEmpty()) {\n                    count += 1;\n                }\n            } \n        } finally {\n            scanner.close();\n        }\n    } while ((count != 0) && (tries >= 0) );\n    junit.framework.Assert.assertTrue(tries >= 0);\n    junit.framework.Assert.assertEquals(0, count);\n}",
            "ClassName": "TestEmptyMetaInfo",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testEmptyMetaInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 41,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "ClassName": "TestAdmin",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testTableNotFoundExceptionWithATable",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 5,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testJiraTest33\");\n    byte[][] VALUES = makeNAscii(org.apache.hadoop.hbase.client.TestFromClientSide.VALUE, 7);\n    long[] STAMPS = makeStamps(7);\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, 10);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0], VALUES[0]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3], VALUES[3]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    ht.put(put);\n    getVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 5);\n    getVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 2);\n    getVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 4, 5);\n    getVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 2, 3);\n    scanVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 5);\n    scanVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 2);\n    scanVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 4, 5);\n    scanVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 2, 3);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    getVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 5);\n    getVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 2);\n    getVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 4, 5);\n    getVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 2, 3);\n    scanVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 5);\n    scanVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 2);\n    scanVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 4, 5);\n    scanVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 2, 3);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration());\n    admin.split(t.getTableName());\n    java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> regions = waitOnSplit(t);\n    org.junit.Assert.assertTrue(regions.size() > 1);\n    return regions;\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "splitTable",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(family, qualifier);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertNResult(result, row, family, qualifier, stamps, values, start, end);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getAllVersionsAndVerify",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testJiraTest33",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 31,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = new org.apache.hadoop.hbase.client.HTablePool(((org.apache.hadoop.hbase.HBaseConfiguration) (null)), java.lang.Integer.MAX_VALUE);\n    java.lang.String tableName = \"testTable\";\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    junit.framework.Assert.assertNotNull(table);\n    pool.putTable(table);\n    org.apache.hadoop.hbase.client.HTableInterface sameTable = pool.getTable(tableName);\n    junit.framework.Assert.assertSame(table, sameTable);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    try {\n        return ((org.apache.hadoop.hbase.client.HTable) (table)).getRegionsInfo();\n    } finally {\n        pool.putTable(table);\n    }\n}",
            "ClassName": "RegionsResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    try {\n        return table.getTableDescriptor();\n    } finally {\n        pool.putTable(table);\n    }\n}",
            "ClassName": "SchemaResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableSchema",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return pool;\n}",
            "ClassName": "RESTServlet",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTablePool",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    servlet.getMetrics().incrementRequests(1);\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = null;\n    try {\n        java.util.List<org.apache.hadoop.hbase.rest.model.RowModel> rows = model.getRows();\n        table = pool.getTable(tableName);\n        ((org.apache.hadoop.hbase.client.HTable) (table)).setAutoFlush(false);\n        for (org.apache.hadoop.hbase.rest.model.RowModel row : rows) {\n            byte[] key = row.getKey();\n            org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(key);\n            for (org.apache.hadoop.hbase.rest.model.CellModel cell : row.getCells()) {\n                byte[][] parts = org.apache.hadoop.hbase.KeyValue.parseColumn(cell.getColumn());\n                if ((parts.length == 2) && (parts[1].length > 0)) {\n                    put.add(parts[0], parts[1], cell.getTimestamp(), cell.getValue());\n                } else {\n                    put.add(parts[0], null, cell.getTimestamp(), cell.getValue());\n                }\n            }\n            table.put(put);\n            if (org.apache.hadoop.hbase.rest.RowResource.LOG.isDebugEnabled()) {\n                org.apache.hadoop.hbase.rest.RowResource.LOG.debug(\"PUT \" + put.toString());\n            }\n        }\n        ((org.apache.hadoop.hbase.client.HTable) (table)).setAutoFlush(true);\n        table.flushCommits();\n        javax.ws.rs.core.Response.ResponseBuilder response = javax.ws.rs.core.Response.ok();\n        return response.build();\n    } catch (java.io.IOException e) {\n        throw new javax.ws.rs.WebApplicationException(e, Response.Status.SERVICE_UNAVAILABLE);\n    } finally {\n        if (table != null) {\n            pool.putTable(table);\n        }\n    }\n}",
            "ClassName": "RowResource",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "update",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 36,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(getConfiguration(), tableName);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.ResultScanner resScan = table.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result res : resScan) {\n        org.apache.hadoop.hbase.client.Delete del = new org.apache.hadoop.hbase.client.Delete(res.getRow());\n        table.delete(del);\n    }\n    return table;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "truncateTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 10,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHTablePool",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testTableWithStringName",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 9,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.HTable table = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testGet_NonExistentRow\"), org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n    table.put(put);\n    LOG.info(\"Row put\");\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.apache.hadoop.hbase.client.Result r = table.get(get);\n    org.junit.Assert.assertFalse(r.isEmpty());\n    java.lang.System.out.println(\"Row retrieved successfully\");\n    byte[] missingrow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"missingrow\");\n    get = new org.apache.hadoop.hbase.client.Get(missingrow);\n    get.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    r = table.get(get);\n    org.junit.Assert.assertTrue(r.isEmpty());\n    LOG.info(\"Row missing as it should be\");\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(c, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.Result res = t.get(get);\n    org.apache.hadoop.hbase.KeyValue[] kvs = res.raw();\n    if (kvs.length <= 0) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    byte[] value = kvs[0].getValue();\n    if (value == null) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(value);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    info.setOffline(onlineOffline);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(info));\n    t.put(put);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(row);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER);\n    t.delete(delete);\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "changeOnlineStatus",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testGet_NonExistentRow",
        "NumberOfAsserts": 2,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 18,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testCreateTableWithRegions\");\n    byte[][] splitKeys = new byte[][]{ new byte[]{ 1, 1, 1 }, new byte[]{ 2, 2, 2 }, new byte[]{ 3, 3, 3 }, new byte[]{ 4, 4, 4 }, new byte[]{ 5, 5, 5 }, new byte[]{ 6, 6, 6 }, new byte[]{ 7, 7, 7 }, new byte[]{ 8, 8, 8 }, new byte[]{ 9, 9, 9 } };\n    int expectedRegions = splitKeys.length + 1;\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    admin.createTable(desc, splitKeys);\n    org.apache.hadoop.hbase.client.HTable ht = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration(), tableName);\n    java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> regions = ht.getRegionsInfo();\n    org.junit.Assert.assertEquals((((\"Tried to create \" + expectedRegions) + \" regions \") + \"but only found \") + regions.size(), expectedRegions, regions.size());\n    java.lang.System.err.println((\"Found \" + regions.size()) + \" regions\");\n    java.util.Iterator<org.apache.hadoop.hbase.HRegionInfo> hris = regions.keySet().iterator();\n    org.apache.hadoop.hbase.HRegionInfo hri = hris.next();\n    org.junit.Assert.assertTrue((hri.getStartKey() == null) || (hri.getStartKey().length == 0));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), splitKeys[0]));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), splitKeys[0]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), splitKeys[1]));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), splitKeys[1]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), splitKeys[2]));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), splitKeys[2]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), splitKeys[3]));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), splitKeys[3]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), splitKeys[4]));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), splitKeys[4]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), splitKeys[5]));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), splitKeys[5]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), splitKeys[6]));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), splitKeys[6]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), splitKeys[7]));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), splitKeys[7]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), splitKeys[8]));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), splitKeys[8]));\n    org.junit.Assert.assertTrue((hri.getEndKey() == null) || (hri.getEndKey().length == 0));\n    byte[] startKey = new byte[]{ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 };\n    byte[] endKey = new byte[]{ 9, 9, 9, 9, 9, 9, 9, 9, 9, 9 };\n    expectedRegions = 10;\n    byte[] TABLE_2 = org.apache.hadoop.hbase.util.Bytes.add(tableName, org.apache.hadoop.hbase.util.Bytes.toBytes(\"_2\"));\n    desc = new org.apache.hadoop.hbase.HTableDescriptor(TABLE_2);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration());\n    admin.createTable(desc, startKey, endKey, expectedRegions);\n    ht = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration(), TABLE_2);\n    regions = ht.getRegionsInfo();\n    org.junit.Assert.assertEquals((((\"Tried to create \" + expectedRegions) + \" regions \") + \"but only found \") + regions.size(), expectedRegions, regions.size());\n    java.lang.System.err.println((\"Found \" + regions.size()) + \" regions\");\n    hris = regions.keySet().iterator();\n    hri = hris.next();\n    org.junit.Assert.assertTrue((hri.getStartKey() == null) || (hri.getStartKey().length == 0));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), new byte[]{ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 }));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), new byte[]{ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 }));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), new byte[]{ 2, 2, 2, 2, 2, 2, 2, 2, 2, 2 }));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), new byte[]{ 2, 2, 2, 2, 2, 2, 2, 2, 2, 2 }));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), new byte[]{ 3, 3, 3, 3, 3, 3, 3, 3, 3, 3 }));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), new byte[]{ 3, 3, 3, 3, 3, 3, 3, 3, 3, 3 }));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), new byte[]{ 4, 4, 4, 4, 4, 4, 4, 4, 4, 4 }));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), new byte[]{ 4, 4, 4, 4, 4, 4, 4, 4, 4, 4 }));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), new byte[]{ 5, 5, 5, 5, 5, 5, 5, 5, 5, 5 }));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), new byte[]{ 5, 5, 5, 5, 5, 5, 5, 5, 5, 5 }));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), new byte[]{ 6, 6, 6, 6, 6, 6, 6, 6, 6, 6 }));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), new byte[]{ 6, 6, 6, 6, 6, 6, 6, 6, 6, 6 }));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), new byte[]{ 7, 7, 7, 7, 7, 7, 7, 7, 7, 7 }));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), new byte[]{ 7, 7, 7, 7, 7, 7, 7, 7, 7, 7 }));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), new byte[]{ 8, 8, 8, 8, 8, 8, 8, 8, 8, 8 }));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), new byte[]{ 8, 8, 8, 8, 8, 8, 8, 8, 8, 8 }));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), new byte[]{ 9, 9, 9, 9, 9, 9, 9, 9, 9, 9 }));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), new byte[]{ 9, 9, 9, 9, 9, 9, 9, 9, 9, 9 }));\n    org.junit.Assert.assertTrue((hri.getEndKey() == null) || (hri.getEndKey().length == 0));\n    startKey = new byte[]{ 0, 0, 0, 0, 0, 0 };\n    endKey = new byte[]{ 1, 0, 0, 0, 0, 0 };\n    expectedRegions = 5;\n    byte[] TABLE_3 = org.apache.hadoop.hbase.util.Bytes.add(tableName, org.apache.hadoop.hbase.util.Bytes.toBytes(\"_3\"));\n    desc = new org.apache.hadoop.hbase.HTableDescriptor(TABLE_3);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration());\n    admin.createTable(desc, startKey, endKey, expectedRegions);\n    ht = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration(), TABLE_3);\n    regions = ht.getRegionsInfo();\n    org.junit.Assert.assertEquals((((\"Tried to create \" + expectedRegions) + \" regions \") + \"but only found \") + regions.size(), expectedRegions, regions.size());\n    java.lang.System.err.println((\"Found \" + regions.size()) + \" regions\");\n    splitKeys = new byte[][]{ new byte[]{ 1, 1, 1 }, new byte[]{ 2, 2, 2 }, new byte[]{ 3, 3, 3 }, new byte[]{ 2, 2, 2 } };\n    byte[] TABLE_4 = org.apache.hadoop.hbase.util.Bytes.add(tableName, org.apache.hadoop.hbase.util.Bytes.toBytes(\"_4\"));\n    desc = new org.apache.hadoop.hbase.HTableDescriptor(TABLE_4);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration());\n    try {\n        admin.createTable(desc, splitKeys);\n        org.junit.Assert.assertTrue(\"Should not be able to create this table because of \" + \"duplicate split keys\", false);\n    } catch (java.lang.IllegalArgumentException iae) {\n    }\n}",
        "CUT_1": {
            "Body": "{\n    return org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, hri.getStartKey()) ? org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaa\") : hri.getStartKey();\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getStartKey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    final int COL_COUNT = 10;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"tablename\");\n    final byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\");\n    this.conf.setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = null;\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(this.fs, dir, this.oldLogDir, this.conf, null);\n    try {\n        long timestamp = java.lang.System.currentTimeMillis();\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit cols = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        for (int i = 0; i < COL_COUNT; i++) {\n            cols.add(new org.apache.hadoop.hbase.KeyValue(row, org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\"), org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(i)), timestamp, new byte[]{ ((byte) (i + '0')) }));\n        }\n        org.apache.hadoop.hbase.HRegionInfo hri = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(tableName), org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.HConstants.EMPTY_END_ROW);\n        log.append(hri, tableName, cols, java.lang.System.currentTimeMillis());\n        long logSeqId = log.startCacheFlush();\n        log.completeCacheFlush(hri.getRegionName(), tableName, logSeqId, false);\n        log.close();\n        org.apache.hadoop.fs.Path filename = log.computeFilename(log.getFilenum());\n        log = null;\n        reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, filename, conf);\n        org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = reader.next();\n        junit.framework.Assert.assertEquals(COL_COUNT, entry.getEdit().size());\n        int idx = 0;\n        for (org.apache.hadoop.hbase.KeyValue val : entry.getEdit().getKeyValues()) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getRegionName(), entry.getKey().getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, entry.getKey().getTablename()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(row, val.getRow()));\n            junit.framework.Assert.assertEquals(((byte) (idx + '0')), val.getValue()[0]);\n            java.lang.System.out.println((entry.getKey() + \" \") + val);\n            idx++;\n        }\n        entry = reader.next();\n        junit.framework.Assert.assertEquals(1, entry.getEdit().size());\n        for (org.apache.hadoop.hbase.KeyValue val : entry.getEdit().getKeyValues()) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getRegionName(), entry.getKey().getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, entry.getKey().getTablename()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, val.getRow()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, val.getFamily()));\n            junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH, val.getValue()));\n            java.lang.System.out.println((entry.getKey() + \" \") + val);\n        }\n    } finally {\n        if (log != null) {\n            log.closeAndDelete();\n        }\n        if (reader != null) {\n            reader.close();\n        }\n    }\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testAppend",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 51,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n    org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon hri = new org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon(r);\n    try {\n        LOG.info(\"Added: \" + org.apache.hadoop.hbase.HBaseTestCase.addContent(hri, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY), org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER)));\n        int count = count(hri, -1, false);\n        junit.framework.Assert.assertEquals(count, count(hri, 100, true));\n    } catch (java.lang.Exception e) {\n        LOG.error(\"Failed\", e);\n        throw e;\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(cluster);\n    }\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanAndRealConcurrentFlush",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n    org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon hri = new org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon(r);\n    try {\n        LOG.info(\"Added: \" + org.apache.hadoop.hbase.HBaseTestCase.addContent(hri, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY), org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER)));\n        int count = count(hri, -1, false);\n        junit.framework.Assert.assertEquals(count, count(hri, 100, false));\n    } catch (java.lang.Exception e) {\n        LOG.error(\"Failed\", e);\n        throw e;\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(cluster);\n    }\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanAndSyncFlush",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(getName());\n    java.lang.String[] families = new java.lang.String[]{ \"info\", \"anchor\" };\n    for (int i = 0; i < families.length; i++) {\n        htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(families[i]));\n    }\n    org.apache.hadoop.hbase.HRegionInfo hri = new org.apache.hadoop.hbase.HRegionInfo(htd, org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.HConstants.EMPTY_END_ROW);\n    byte[] hrib = org.apache.hadoop.hbase.util.Writables.getBytes(hri);\n    org.apache.hadoop.hbase.HRegionInfo deserializedHri = ((org.apache.hadoop.hbase.HRegionInfo) (org.apache.hadoop.hbase.util.Writables.getWritable(hrib, new org.apache.hadoop.hbase.HRegionInfo())));\n    junit.framework.Assert.assertEquals(hri.getEncodedName(), deserializedHri.getEncodedName());\n    junit.framework.Assert.assertEquals(hri.getTableDesc().getFamilies().size(), deserializedHri.getTableDesc().getFamilies().size());\n}",
            "ClassName": "TestSerialization",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testRegionInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testCreateTableWithRegions",
        "NumberOfAsserts": 44,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 108,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.master.TestMasterTransitions.LOG.info(\"Running testAddingServerBeforeOldIsDead2413\");\n    org.apache.hadoop.hbase.MiniHBaseCluster cluster = org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getHBaseCluster();\n    int count = org.apache.hadoop.hbase.master.TestMasterTransitions.count();\n    int metaIndex = cluster.getServerWithMeta();\n    org.apache.hadoop.hbase.MiniHBaseCluster.MiniHBaseClusterRegionServer metaHRS = ((org.apache.hadoop.hbase.MiniHBaseCluster.MiniHBaseClusterRegionServer) (cluster.getRegionServer(metaIndex)));\n    int port = metaHRS.getServerInfo().getServerAddress().getPort();\n    org.apache.hadoop.conf.Configuration c = org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration();\n    java.lang.String oldPort = c.get(org.apache.hadoop.hbase.HConstants.REGIONSERVER_PORT, \"0\");\n    try {\n        org.apache.hadoop.hbase.master.TestMasterTransitions.LOG.info(\"KILLED=\" + metaHRS);\n        metaHRS.kill();\n        c.set(org.apache.hadoop.hbase.HConstants.REGIONSERVER_PORT, java.lang.Integer.toString(port));\n        org.apache.hadoop.hbase.regionserver.HRegionServer hrs = null;\n        while (true) {\n            try {\n                hrs = cluster.startRegionServer().getRegionServer();\n                break;\n            } catch (java.io.IOException e) {\n                if ((e.getCause() != null) && (e.getCause() instanceof java.lang.reflect.InvocationTargetException)) {\n                    java.lang.reflect.InvocationTargetException ee = ((java.lang.reflect.InvocationTargetException) (e.getCause()));\n                    if ((ee.getCause() != null) && (ee.getCause() instanceof java.net.BindException)) {\n                        org.apache.hadoop.hbase.master.TestMasterTransitions.LOG.info(\"BindException; retrying: \" + e.toString());\n                    }\n                }\n            }\n        } \n        org.apache.hadoop.hbase.master.TestMasterTransitions.LOG.info(\"STARTED=\" + hrs);\n        while (hrs.getOnlineRegions().size() < 3)\n            org.apache.hadoop.hbase.util.Threads.sleep(100);\n\n        org.apache.hadoop.hbase.master.TestMasterTransitions.LOG.info(((hrs.toString() + \" has \") + hrs.getOnlineRegions().size()) + \" regions\");\n        org.junit.Assert.assertEquals(count, org.apache.hadoop.hbase.master.TestMasterTransitions.count());\n    } finally {\n        c.set(org.apache.hadoop.hbase.HConstants.REGIONSERVER_PORT, oldPort);\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.startMiniCluster(2);\n    org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.createTable(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME), org.apache.hadoop.hbase.master.TestMasterTransitions.FAMILIES);\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    int countOfRegions = org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.createMultiRegions(t, org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily());\n    org.apache.hadoop.hbase.master.TestMasterTransitions.waitUntilAllRegionsAssigned(countOfRegions);\n    org.apache.hadoop.hbase.master.TestMasterTransitions.addToEachStartKey(countOfRegions);\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "beforeAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    if (org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getHBaseCluster().getLiveRegionServerThreads().size() < 2) {\n        org.apache.hadoop.hbase.master.TestMasterTransitions.LOG.info(\"Started new server=\" + org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getHBaseCluster().startRegionServer());\n    }\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setup",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.Get g = new org.apache.hadoop.hbase.client.Get(row);\n    org.junit.Assert.assertTrue(t.get(g).size() > 0);\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "assertRegionIsBackOnline",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return org.apache.hadoop.hbase.master.TestMasterTransitions.FAMILIES[0];\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTestFamily",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        org.apache.hadoop.hbase.HRegionInfo hri = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(b);\n        byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(row);\n        p.add(org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily(), org.apache.hadoop.hbase.master.TestMasterTransitions.getTestQualifier(), row);\n        t.put(p);\n        rows++;\n    }\n    s.close();\n    org.junit.Assert.assertEquals(expected, rows);\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addToEachStartKey",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestMasterTransitions",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 4,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 1,
        "Label": 0,
        "MethodName": "testAddingServerBeforeOldIsDead2413",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 37,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testJiraTest52\");\n    byte[][] VALUES = makeNAscii(org.apache.hadoop.hbase.client.TestFromClientSide.VALUE, 7);\n    long[] STAMPS = makeStamps(7);\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, 10);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0], VALUES[0]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3], VALUES[3]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    ht.put(put);\n    getAllVersionsAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 5);\n    scanAllVersionsAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 5);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    getAllVersionsAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 5);\n    scanAllVersionsAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 5);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration());\n    admin.split(t.getTableName());\n    java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> regions = waitOnSplit(t);\n    org.junit.Assert.assertTrue(regions.size() > 1);\n    return regions;\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "splitTable",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testJiraTest52",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 19,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.DummyLogActionsListener list = new org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.DummyLogActionsListener();\n    org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.DummyLogActionsListener laterList = new org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.DummyLogActionsListener();\n    org.apache.hadoop.hbase.regionserver.wal.HLog hlog = new org.apache.hadoop.hbase.regionserver.wal.HLog(org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.fs, org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.logDir, org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.oldLogDir, org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.conf, null, list);\n    org.apache.hadoop.hbase.HRegionInfo hri = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.SOME_BYTES), org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.SOME_BYTES, org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.SOME_BYTES, false);\n    for (int i = 0; i < 20; i++) {\n        byte[] b = org.apache.hadoop.hbase.util.Bytes.toBytes(i + \"\");\n        org.apache.hadoop.hbase.KeyValue kv = new org.apache.hadoop.hbase.KeyValue(b, b, b);\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit edit = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        edit.add(kv);\n        org.apache.hadoop.hbase.regionserver.wal.HLogKey key = new org.apache.hadoop.hbase.regionserver.wal.HLogKey(b, b, 0, 0);\n        hlog.append(hri, key, edit);\n        if (i == 10) {\n            hlog.addLogActionsListerner(laterList);\n        }\n        if ((i % 2) == 0) {\n            hlog.rollWriter();\n        }\n    }\n    org.junit.Assert.assertEquals(11, list.logRollCounter);\n    org.junit.Assert.assertEquals(5, laterList.logRollCounter);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.conf = org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.TEST_UTIL.getConfiguration();\n    org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.conf.setInt(\"hbase.regionserver.maxlogs\", 5);\n    org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.fs = org.apache.hadoop.fs.FileSystem.get(org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.conf);\n    org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.oldLogDir = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.TEST_UTIL.getTestDir(), org.apache.hadoop.hbase.HConstants.HREGION_OLDLOGDIR_NAME);\n    org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.logDir = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.TEST_UTIL.getTestDir(), org.apache.hadoop.hbase.HConstants.HREGION_LOGDIR_NAME);\n}",
            "ClassName": "TestLogActionsListener",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.fs.delete(org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.logDir, true);\n    org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.fs.delete(org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.oldLogDir, true);\n}",
            "ClassName": "TestLogActionsListener",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.KeyValue kv = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, null, java.lang.System.currentTimeMillis(), org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH);\n    org.apache.hadoop.hbase.regionserver.wal.WALEdit e = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n    e.add(kv);\n    return e;\n}",
            "ClassName": "HLog",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "completeCacheFlushLogEdit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    final int COL_COUNT = 10;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"tablename\");\n    final byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\");\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = null;\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(fs, dir, this.oldLogDir, this.conf, null);\n    try {\n        long timestamp = java.lang.System.currentTimeMillis();\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit cols = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        for (int i = 0; i < COL_COUNT; i++) {\n            cols.add(new org.apache.hadoop.hbase.KeyValue(row, org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\"), org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(i)), timestamp, new byte[]{ ((byte) (i + '0')) }));\n        }\n        org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(tableName), row, org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.util.Bytes.toString(row) + \"1\"), false);\n        final byte[] regionName = info.getRegionName();\n        log.append(info, tableName, cols, java.lang.System.currentTimeMillis());\n        long logSeqId = log.startCacheFlush();\n        log.completeCacheFlush(regionName, tableName, logSeqId, info.isMetaRegion());\n        log.close();\n        org.apache.hadoop.fs.Path filename = log.computeFilename(log.getFilenum());\n        log = null;\n        reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, filename, conf);\n        for (int i = 0; i < 1; i++) {\n            org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = reader.next(null);\n            if (entry == null)\n                break;\n\n            org.apache.hadoop.hbase.regionserver.wal.HLogKey key = entry.getKey();\n            org.apache.hadoop.hbase.regionserver.wal.WALEdit val = entry.getEdit();\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(regionName, key.getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, key.getTablename()));\n            org.apache.hadoop.hbase.KeyValue kv = val.getKeyValues().get(0);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(row, kv.getRow()));\n            junit.framework.Assert.assertEquals(((byte) (i + '0')), kv.getValue()[0]);\n            java.lang.System.out.println((key + \" \") + val);\n        }\n        org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = null;\n        while ((entry = reader.next(null)) != null) {\n            org.apache.hadoop.hbase.regionserver.wal.HLogKey key = entry.getKey();\n            org.apache.hadoop.hbase.regionserver.wal.WALEdit val = entry.getEdit();\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(regionName, key.getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, key.getTablename()));\n            org.apache.hadoop.hbase.KeyValue kv = val.getKeyValues().get(0);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, kv.getRow()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, kv.getFamily()));\n            junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH, val.getKeyValues().get(0).getValue()));\n            java.lang.System.out.println((key + \" \") + val);\n        } \n    } finally {\n        if (log != null) {\n            log.closeAndDelete();\n        }\n        if (reader != null) {\n            reader.close();\n        }\n    }\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 6,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testEditAdd",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 56,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    byte[] bytes = org.apache.hadoop.hbase.util.Bytes.toBytes(getName());\n    org.apache.hadoop.fs.Path p = new org.apache.hadoop.fs.Path(this.dir, getName() + \".fsdos\");\n    org.apache.hadoop.fs.FSDataOutputStream out = fs.create(p);\n    out.write(bytes);\n    out.sync();\n    org.apache.hadoop.fs.FSDataInputStream in = fs.open(p);\n    junit.framework.Assert.assertTrue(in.available() > 0);\n    byte[] buffer = new byte[1024];\n    int read = in.read(buffer);\n    junit.framework.Assert.assertEquals(bytes.length, read);\n    out.close();\n    in.close();\n    org.apache.hadoop.fs.Path subdir = new org.apache.hadoop.fs.Path(this.dir, \"hlogdir\");\n    org.apache.hadoop.hbase.regionserver.wal.HLog wal = new org.apache.hadoop.hbase.regionserver.wal.HLog(this.fs, subdir, this.oldLogDir, this.conf, null);\n    final int total = 20;\n    org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(bytes), null, null, false);\n    for (int i = 0; i < total; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit kvs = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        kvs.add(new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(i), bytes, bytes));\n        wal.append(info, bytes, kvs, java.lang.System.currentTimeMillis());\n    }\n    wal.sync();\n    org.apache.hadoop.fs.Path walPath = wal.computeFilename(wal.getFilenum());\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, walPath, conf);\n    int count = 0;\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = new org.apache.hadoop.hbase.regionserver.wal.HLog.Entry();\n    while ((entry = reader.next(entry)) != null)\n        count++;\n\n    junit.framework.Assert.assertEquals(total, count);\n    reader.close();\n    for (int i = 0; i < total; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit kvs = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        kvs.add(new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(i), bytes, bytes));\n        wal.append(info, bytes, kvs, java.lang.System.currentTimeMillis());\n    }\n    reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, walPath, conf);\n    count = 0;\n    while ((entry = reader.next(entry)) != null)\n        count++;\n\n    junit.framework.Assert.assertTrue(count >= total);\n    reader.close();\n    wal.sync();\n    reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, walPath, conf);\n    count = 0;\n    while ((entry = reader.next(entry)) != null)\n        count++;\n\n    junit.framework.Assert.assertEquals(total * 2, count);\n    final byte[] value = new byte[1025 * 1024];\n    for (int i = 0; i < total; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit kvs = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        kvs.add(new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(i), bytes, value));\n        wal.append(info, bytes, kvs, java.lang.System.currentTimeMillis());\n    }\n    wal.sync();\n    reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, walPath, conf);\n    count = 0;\n    while ((entry = reader.next(entry)) != null)\n        count++;\n\n    junit.framework.Assert.assertEquals(total * 3, count);\n    reader.close();\n    wal.close();\n    reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, walPath, conf);\n    count = 0;\n    while ((entry = reader.next(entry)) != null)\n        count++;\n\n    junit.framework.Assert.assertEquals(total * 3, count);\n    reader.close();\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 8,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "Broken_testSync",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 74,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestLogActionsListener",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 3,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testActionListener",
        "NumberOfAsserts": 2,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 22,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    final byte[] CONTENTS_FAMILY = org.apache.hadoop.hbase.util.Bytes.toBytes(\"contents\");\n    final byte[] SMALL_FAMILY = org.apache.hadoop.hbase.util.Bytes.toBytes(\"smallfam\");\n    final byte[] value = org.apache.hadoop.hbase.util.Bytes.toBytes(\"abcd\");\n    final int NB_BATCH_ROWS = 10;\n    org.apache.hadoop.hbase.client.HTable table = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowsPutBufferedManyManyFlushes\"), new byte[][]{ CONTENTS_FAMILY, SMALL_FAMILY });\n    table.setAutoFlush(false);\n    table.setWriteBufferSize(10);\n    java.util.ArrayList<org.apache.hadoop.hbase.client.Put> rowsUpdate = new java.util.ArrayList<org.apache.hadoop.hbase.client.Put>();\n    for (int i = 0; i < (NB_BATCH_ROWS * 10); i++) {\n        byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + i);\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n        put.add(CONTENTS_FAMILY, null, value);\n        rowsUpdate.add(put);\n    }\n    table.put(rowsUpdate);\n    table.flushCommits();\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(CONTENTS_FAMILY);\n    org.apache.hadoop.hbase.client.ResultScanner scanner = table.getScanner(scan);\n    int nbRows = 0;\n    for (@java.lang.SuppressWarnings(\"unused\")\n    org.apache.hadoop.hbase.client.Result row : scanner)\n        nbRows++;\n\n    org.junit.Assert.assertEquals(NB_BATCH_ROWS * 10, nbRows);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"test_table\");\n    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testFamily\");\n    initHRegion(tableName, getName(), family);\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row111\");\n    byte[] row2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row222\");\n    byte[] row3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row333\");\n    byte[] row4 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row444\");\n    byte[] row5 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row555\");\n    byte[] col1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub111\");\n    byte[] col2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub222\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(10L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row2);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(15L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row3);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(20L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row4);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(30L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row5);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(40L));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row3, row4);\n    scan.setMaxVersions();\n    scan.addColumn(family, col1);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    junit.framework.Assert.assertEquals(false, s.next(results));\n    junit.framework.Assert.assertEquals(0, results.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_StopRow1542",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 34,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    this.server = cluster.getRegionServerThreads().get(0).getRegionServer();\n    this.log = server.getLog();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    for (int i = 1; i <= 256; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + java.lang.String.format(\"%1$04d\", i)));\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, value);\n        table.put(put);\n        if ((i % 32) == 0) {\n            try {\n                java.lang.Thread.sleep(2000);\n            } catch (java.lang.InterruptedException e) {\n            }\n        }\n    }\n}",
            "ClassName": "TestLogRolling",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startAndWriteData",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_5": {
            "Body": "{\n    int count = 0;\n    for (char c = 'a'; c <= 'c'; c++) {\n        byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"ab\" + c);\n        int i;\n        for (i = 0; i < 2500; i++) {\n            byte[] b = org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.String.format(\"%10d\", i));\n            org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n            put.add(family, b, b);\n            region.put(put);\n            count++;\n        }\n    }\n    {\n        byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"abf\");\n        int i;\n        for (i = 0; i < 100000; i++) {\n            byte[] b = org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.String.format(\"%10d\", i));\n            org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n            put.add(family, b, b);\n            region.put(put);\n            count++;\n        }\n    }\n    return count;\n}",
            "ClassName": "TestWideScanner",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addWideContent",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 26,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 2,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testRowsPutBufferedManyManyFlushes",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 27,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    java.lang.String msg = null;\n    try {\n        this.admin.createTable(org.apache.hadoop.hbase.HTableDescriptor.ROOT_TABLEDESC);\n    } catch (java.lang.IllegalArgumentException e) {\n        msg = e.toString();\n    }\n    org.junit.Assert.assertTrue(\"Unexcepted exception message \" + msg, ((msg != null) && msg.startsWith(java.lang.IllegalArgumentException.class.getName())) && msg.contains(org.apache.hadoop.hbase.HTableDescriptor.ROOT_TABLEDESC.getNameAsString()));\n    msg = null;\n    try {\n        this.admin.createTable(org.apache.hadoop.hbase.HTableDescriptor.META_TABLEDESC);\n    } catch (java.lang.IllegalArgumentException e) {\n        msg = e.toString();\n    }\n    org.junit.Assert.assertTrue(\"Unexcepted exception message \" + msg, ((msg != null) && msg.startsWith(java.lang.IllegalArgumentException.class.getName())) && msg.contains(org.apache.hadoop.hbase.HTableDescriptor.META_TABLEDESC.getNameAsString()));\n    final org.apache.hadoop.hbase.HTableDescriptor threadDesc = new org.apache.hadoop.hbase.HTableDescriptor(\"threaded_testCreateBadTables\");\n    threadDesc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    int count = 10;\n    java.lang.Thread[] threads = new java.lang.Thread[count];\n    final java.util.concurrent.atomic.AtomicInteger successes = new java.util.concurrent.atomic.AtomicInteger(0);\n    final java.util.concurrent.atomic.AtomicInteger failures = new java.util.concurrent.atomic.AtomicInteger(0);\n    final org.apache.hadoop.hbase.client.HBaseAdmin localAdmin = this.admin;\n    for (int i = 0; i < count; i++) {\n        threads[i] = new java.lang.Thread(java.lang.Integer.toString(i)) {\n            @java.lang.Override\n            public void run() {\n                try {\n                    localAdmin.createTable(threadDesc);\n                    successes.incrementAndGet();\n                } catch (org.apache.hadoop.hbase.TableExistsException e) {\n                    failures.incrementAndGet();\n                } catch (java.io.IOException e) {\n                    throw new java.lang.RuntimeException(\"Failed threaded create\" + getName(), e);\n                }\n            }\n        };\n    }\n    for (int i = 0; i < count; i++) {\n        threads[i].start();\n    }\n    for (int i = 0; i < count; i++) {\n        while (threads[i].isAlive()) {\n            try {\n                java.lang.Thread.sleep(1000);\n            } catch (java.lang.InterruptedException e) {\n            }\n        } \n    }\n    org.junit.Assert.assertEquals(1, successes.get());\n    org.junit.Assert.assertEquals(count - 1, failures.get());\n}",
        "CUT_1": {
            "Body": "{\n    return t instanceof java.io.IOException ? ((java.io.IOException) (t)) : (msg == null) || (msg.length() == 0) ? new java.io.IOException(t) : new java.io.IOException(msg, t);\n}",
            "ClassName": "HRegionServer",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "convertThrowableToIOE",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    final java.util.List<java.lang.Thread> threads = new java.util.ArrayList<java.lang.Thread>(N);\n    final int perClientRows = R / N;\n    for (int i = 0; i < N; i++) {\n        java.lang.Thread t = new java.lang.Thread(java.lang.Integer.toString(i)) {\n            @java.lang.Override\n            public void run() {\n                super.run();\n                org.apache.hadoop.hbase.rest.PerformanceEvaluation pe = new org.apache.hadoop.hbase.rest.PerformanceEvaluation(conf);\n                int index = java.lang.Integer.parseInt(getName());\n                try {\n                    long elapsedTime = pe.runOneClient(cmd, index * perClientRows, perClientRows, R, B, new org.apache.hadoop.hbase.rest.PerformanceEvaluation.Status() {\n                        public void setStatus(final java.lang.String msg) throws java.io.IOException {\n                            org.apache.hadoop.hbase.rest.PerformanceEvaluation.LOG.info(((\"client-\" + getName()) + \" \") + msg);\n                        }\n                    });\n                    org.apache.hadoop.hbase.rest.PerformanceEvaluation.LOG.info((((((\"Finished \" + getName()) + \" in \") + elapsedTime) + \"ms writing \") + perClientRows) + \" rows\");\n                } catch (java.io.IOException e) {\n                    throw new java.lang.RuntimeException(e);\n                }\n            }\n        };\n        threads.add(t);\n    }\n    for (java.lang.Thread t : threads) {\n        t.start();\n    }\n    for (java.lang.Thread t : threads) {\n        while (t.isAlive()) {\n            try {\n                t.join();\n            } catch (java.lang.InterruptedException e) {\n                org.apache.hadoop.hbase.rest.PerformanceEvaluation.LOG.debug(\"Interrupted, continuing\" + e.toString());\n            }\n        } \n    }\n}",
            "ClassName": "PerformanceEvaluation",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "doMultipleClients",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 37,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 28
        },
        "CUT_3": {
            "Body": "{\n    final int count = 1;\n    long now = java.lang.System.currentTimeMillis();\n    java.util.List<java.lang.Thread> threads = new java.util.ArrayList<java.lang.Thread>(count);\n    for (int i = 0; i < count; i++) {\n        java.lang.Thread t = new java.lang.Thread(r);\n        t.setName(\"\" + i);\n        threads.add(t);\n    }\n    for (java.lang.Thread t : threads) {\n        t.start();\n    }\n    for (java.lang.Thread t : threads) {\n        try {\n            t.join();\n        } catch (java.lang.InterruptedException e) {\n            e.printStackTrace();\n        }\n    }\n    org.apache.hadoop.hbase.PerformanceEvaluationCommons.LOG.info(\"Test took \" + (java.lang.System.currentTimeMillis() - now));\n}",
            "ClassName": "PerformanceEvaluationCommons",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "concurrentReads",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 22
        },
        "CUT_4": {
            "Body": "{\n    if (this.toDo.isEmpty()) {\n        return;\n    }\n    for (org.apache.hadoop.hbase.regionserver.HRegionServer.ToDoEntry e : this.toDo) {\n        if (e == null) {\n            org.apache.hadoop.hbase.regionserver.HRegionServer.LOG.warn(\"toDo gave a null entry during iteration\");\n            break;\n        }\n        org.apache.hadoop.hbase.HMsg msg = e.msg;\n        if (msg != null) {\n            if (msg.isType(org.apache.hadoop.hbase.HMsg.Type.MSG_REGION_OPEN)) {\n                addProcessingMessage(msg.getRegionInfo());\n            }\n        } else {\n            org.apache.hadoop.hbase.regionserver.HRegionServer.LOG.warn(\"Message is empty: \" + e);\n        }\n    }\n}",
            "ClassName": "HRegionServer",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "housekeeping",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 19,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_5": {
            "Body": "{\n    java.util.Properties properties = new java.util.Properties();\n    try {\n        properties.load(inputStream);\n    } catch (java.io.IOException e) {\n        java.lang.String msg = \"fail to read properties from \" + org.apache.hadoop.hbase.HConstants.ZOOKEEPER_CONFIG_NAME;\n        org.apache.hadoop.hbase.zookeeper.HQuorumPeer.LOG.fatal(msg);\n        throw new java.io.IOException(msg, e);\n    }\n    for (java.util.Map.Entry<java.lang.Object, java.lang.Object> entry : properties.entrySet()) {\n        java.lang.String value = entry.getValue().toString().trim();\n        java.lang.String key = entry.getKey().toString().trim();\n        java.lang.StringBuilder newValue = new java.lang.StringBuilder();\n        int varStart = value.indexOf(org.apache.hadoop.hbase.zookeeper.HQuorumPeer.VARIABLE_START);\n        int varEnd = 0;\n        while (varStart != (-1)) {\n            varEnd = value.indexOf(org.apache.hadoop.hbase.zookeeper.HQuorumPeer.VARIABLE_END, varStart);\n            if (varEnd == (-1)) {\n                java.lang.String msg = (\"variable at \" + varStart) + \" has no end marker\";\n                org.apache.hadoop.hbase.zookeeper.HQuorumPeer.LOG.fatal(msg);\n                throw new java.io.IOException(msg);\n            }\n            java.lang.String variable = value.substring(varStart + org.apache.hadoop.hbase.zookeeper.HQuorumPeer.VARIABLE_START_LENGTH, varEnd);\n            java.lang.String substituteValue = java.lang.System.getProperty(variable);\n            if (substituteValue == null) {\n                substituteValue = conf.get(variable);\n            }\n            if (substituteValue == null) {\n                java.lang.String msg = ((\"variable \" + variable) + \" not set in system property \") + \"or hbase configs\";\n                org.apache.hadoop.hbase.zookeeper.HQuorumPeer.LOG.fatal(msg);\n                throw new java.io.IOException(msg);\n            }\n            newValue.append(substituteValue);\n            varEnd += org.apache.hadoop.hbase.zookeeper.HQuorumPeer.VARIABLE_END_LENGTH;\n            varStart = value.indexOf(org.apache.hadoop.hbase.zookeeper.HQuorumPeer.VARIABLE_START, varEnd);\n        } \n        if (key.startsWith(\"server.\")) {\n            if (conf.get(org.apache.hadoop.hbase.HConstants.CLUSTER_DISTRIBUTED).equals(org.apache.hadoop.hbase.HConstants.CLUSTER_IS_DISTRIBUTED) && value.startsWith(\"localhost\")) {\n                java.lang.String msg = \"The server in zoo.cfg cannot be set to localhost \" + (\"in a fully-distributed setup because it won't be reachable. \" + \"See \\\"Getting Started\\\" for more information.\");\n                org.apache.hadoop.hbase.zookeeper.HQuorumPeer.LOG.fatal(msg);\n                throw new java.io.IOException(msg);\n            }\n        }\n        newValue.append(value.substring(varEnd));\n        properties.setProperty(key, newValue.toString());\n    }\n    return properties;\n}",
            "ClassName": "HQuorumPeer",
            "CyclomaticComplexity": 7,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "parseZooCfg",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 48,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 4,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testCreateBadTables",
        "NumberOfAsserts": 4,
        "NumberOfAsynchronousWaits": 1,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 51,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 36,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[] name = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testFilterAcrossMutlipleRegions\");\n    org.apache.hadoop.hbase.client.HTable t = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(name, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    int rowCount = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.loadTable(t, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    assertRowCount(t, rowCount);\n    java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> regions = splitTable(t);\n    assertRowCount(t, rowCount);\n    byte[] endKey = regions.keySet().iterator().next().getEndKey();\n    int endKeyCount = countRows(t, createScanWithRowFilter(endKey));\n    org.junit.Assert.assertTrue(endKeyCount < rowCount);\n    byte[] key = new byte[]{ endKey[0], endKey[1], ((byte) (endKey[2] + 1)) };\n    int plusOneCount = countRows(t, createScanWithRowFilter(key));\n    org.junit.Assert.assertEquals(endKeyCount + 1, plusOneCount);\n    key = new byte[]{ endKey[0], endKey[1], ((byte) (endKey[2] + 2)) };\n    int plusTwoCount = countRows(t, createScanWithRowFilter(key));\n    org.junit.Assert.assertEquals(endKeyCount + 2, plusTwoCount);\n    key = new byte[]{ endKey[0], endKey[1], ((byte) (endKey[2] - 1)) };\n    int minusOneCount = countRows(t, createScanWithRowFilter(key));\n    org.junit.Assert.assertEquals(endKeyCount - 1, minusOneCount);\n    key = new byte[]{ 'a', 'a', 'a' };\n    int countBBB = countRows(t, createScanWithRowFilter(key, null, org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL));\n    org.junit.Assert.assertEquals(1, countBBB);\n    int countGreater = countRows(t, createScanWithRowFilter(endKey, null, org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER_OR_EQUAL));\n    org.junit.Assert.assertEquals(0, countGreater);\n    countGreater = countRows(t, createScanWithRowFilter(endKey, endKey, org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER_OR_EQUAL));\n    org.junit.Assert.assertEquals(rowCount - endKeyCount, countGreater);\n}",
        "CUT_1": {
            "Body": "{\n    this.endKey = endKey;\n}",
            "ClassName": "TableRegionModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setEndKey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return endKey;\n}",
            "ClassName": "HRegionInfo",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getEndKey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return endKey;\n}",
            "ClassName": "TableRegionModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getEndKey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    this.endKey = endKey;\n    return this;\n}",
            "ClassName": "TRegionInfo",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setEndKey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return this.endKey;\n}",
            "ClassName": "TRegionInfo",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getEndKey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testFilterAcrossMutlipleRegions",
        "NumberOfAsserts": 7,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 27,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    testScan(null, \"bbb\", \"bba\");\n}",
        "CUT_1": {
            "Body": "{\n    final byte[] a = org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaa\");\n    final byte[] b = org.apache.hadoop.hbase.util.Bytes.toBytes(\"bbb\");\n    final byte[] fam = org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\");\n    final byte[] qf = org.apache.hadoop.hbase.util.Bytes.toBytes(\"umn\");\n    org.apache.hadoop.hbase.KeyValue aaa = new org.apache.hadoop.hbase.KeyValue(a, fam, qf, a);\n    org.apache.hadoop.hbase.KeyValue bbb = new org.apache.hadoop.hbase.KeyValue(b, fam, qf, b);\n    byte[] keyabb = aaa.getKey();\n    byte[] keybbb = bbb.getKey();\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compare(aaa, bbb) < 0);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.KEY_COMPARATOR.compare(keyabb, 0, keyabb.length, keybbb, 0, keybbb.length) < 0);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compare(bbb, aaa) > 0);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.KEY_COMPARATOR.compare(keybbb, 0, keybbb.length, keyabb, 0, keyabb.length) > 0);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compare(bbb, bbb) == 0);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.KEY_COMPARATOR.compare(keybbb, 0, keybbb.length, keybbb, 0, keybbb.length) == 0);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compare(aaa, aaa) == 0);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.KEY_COMPARATOR.compare(keyabb, 0, keyabb.length, keyabb, 0, keyabb.length) == 0);\n    aaa = new org.apache.hadoop.hbase.KeyValue(a, fam, qf, 1, a);\n    bbb = new org.apache.hadoop.hbase.KeyValue(a, fam, qf, 2, a);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compare(aaa, bbb) > 0);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compare(bbb, aaa) < 0);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compare(aaa, aaa) == 0);\n    aaa = new org.apache.hadoop.hbase.KeyValue(a, fam, qf, 1, org.apache.hadoop.hbase.KeyValue.Type.Delete, a);\n    bbb = new org.apache.hadoop.hbase.KeyValue(a, fam, qf, 1, a);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compare(aaa, bbb) < 0);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compare(bbb, aaa) > 0);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compare(aaa, aaa) == 0);\n}",
            "ClassName": "TestKeyValue",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testPlainCompare",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 28,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return ((x == null) && (y == null)) || ((x != null) && x.equals(y));\n}",
            "ClassName": "Pair",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "equals",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HalfStoreFileReader",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "midkey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "internalPreFlushcacheCommit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return null;\n}",
            "ClassName": "ScanWildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableInputFormatScan",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testScanEmptyToBBB",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    byte[][] illegalNames = new byte[][]{ org.apache.hadoop.hbase.util.Bytes.toBytes(\"-bad\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\".bad\"), org.apache.hadoop.hbase.HConstants.ROOT_TABLE_NAME, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME };\n    for (int i = 0; i < illegalNames.length; i++) {\n        try {\n            new org.apache.hadoop.hbase.HTableDescriptor(illegalNames[i]);\n            throw new java.io.IOException((\"Did not detect '\" + org.apache.hadoop.hbase.util.Bytes.toString(illegalNames[i])) + \"' as an illegal user table name\");\n        } catch (java.lang.IllegalArgumentException e) {\n        }\n    }\n    byte[] legalName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"g-oo.d\");\n    try {\n        new org.apache.hadoop.hbase.HTableDescriptor(legalName);\n    } catch (java.lang.IllegalArgumentException e) {\n        throw new java.io.IOException(((\"Legal user table name: '\" + org.apache.hadoop.hbase.util.Bytes.toString(legalName)) + \"' caused IllegalArgumentException: \") + e.getMessage());\n    }\n}",
        "CUT_1": {
            "Body": "{\n    return new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(row), org.apache.hadoop.hbase.util.Bytes.toBytes(family), org.apache.hadoop.hbase.util.Bytes.toBytes(qualifier), timestamp, type, org.apache.hadoop.hbase.util.Bytes.toBytes(value));\n}",
            "ClassName": "KeyValueTestUtil",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "create",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return org.apache.hadoop.hbase.util.Bytes.equals(n, org.apache.hadoop.hbase.HConstants.ROOT_TABLE_NAME) || org.apache.hadoop.hbase.util.Bytes.equals(n, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "isMetaTableName",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.HRegionInfo a = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(\"a\"), null, null);\n    org.apache.hadoop.hbase.HRegionInfo b = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(\"b\"), null, null);\n    junit.framework.Assert.assertTrue(a.compareTo(b) != 0);\n    org.apache.hadoop.hbase.HTableDescriptor t = new org.apache.hadoop.hbase.HTableDescriptor(\"t\");\n    byte[] midway = org.apache.hadoop.hbase.util.Bytes.toBytes(\"midway\");\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, null, midway);\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, midway, null);\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n    junit.framework.Assert.assertTrue(b.compareTo(a) > 0);\n    junit.framework.Assert.assertEquals(a, a);\n    junit.framework.Assert.assertTrue(a.compareTo(a) == 0);\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"a\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"d\"));\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"e\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"g\"));\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"dddd\"));\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"e\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"g\"));\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"dddd\"));\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"eeee\"));\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n}",
            "ClassName": "TestCompare",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testHRegionInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    long now = java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.KeyValue a = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\".META.,,99999999999999\"), now);\n    org.apache.hadoop.hbase.KeyValue b = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\".META.,,1\"), now);\n    org.apache.hadoop.hbase.KeyValue.KVComparator c = new org.apache.hadoop.hbase.KeyValue.RootComparator();\n    junit.framework.Assert.assertTrue(c.compare(b, a) < 0);\n    org.apache.hadoop.hbase.KeyValue aa = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\".META.,,1\"), now);\n    org.apache.hadoop.hbase.KeyValue bb = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\".META.,,1\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"info\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"regioninfo\"), 1235943454602L, ((byte[]) (null)));\n    junit.framework.Assert.assertTrue(c.compare(aa, bb) < 0);\n    org.apache.hadoop.hbase.KeyValue aaa = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"TestScanMultipleVersions,row_0500,1236020145502\"), now);\n    org.apache.hadoop.hbase.KeyValue bbb = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"TestScanMultipleVersions,,99999999999999\"), now);\n    c = new org.apache.hadoop.hbase.KeyValue.MetaComparator();\n    junit.framework.Assert.assertTrue(c.compare(bbb, aaa) < 0);\n    org.apache.hadoop.hbase.KeyValue aaaa = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"TestScanMultipleVersions,,1236023996656\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"info\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"regioninfo\"), 1236024396271L, ((byte[]) (null)));\n    junit.framework.Assert.assertTrue(c.compare(aaaa, bbb) < 0);\n    org.apache.hadoop.hbase.KeyValue x = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"TestScanMultipleVersions,row_0500,1236034574162\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"info\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), 9223372036854775807L, ((byte[]) (null)));\n    org.apache.hadoop.hbase.KeyValue y = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"TestScanMultipleVersions,row_0500,1236034574162\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"info\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"regioninfo\"), 1236034574912L, ((byte[]) (null)));\n    junit.framework.Assert.assertTrue(c.compare(x, y) < 0);\n    comparisons(new org.apache.hadoop.hbase.KeyValue.MetaComparator());\n    comparisons(new org.apache.hadoop.hbase.KeyValue.KVComparator());\n    metacomparisons(new org.apache.hadoop.hbase.KeyValue.RootComparator());\n    metacomparisons(new org.apache.hadoop.hbase.KeyValue.MetaComparator());\n}",
            "ClassName": "TestKeyValue",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testMoreComparisons",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "a309d26e8c6636b968f6beb8dab6510a6972f76c",
        "CyclomaticComplexity": 1,
        "Date": "Thu, 20 May 2010 02:31:11 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testTableNames",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 16,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274322671"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response;\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteValue(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response;\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteValue(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDelete",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 20,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSingleCellGetPutXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = getValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = putValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSingleCellGetPutPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 15,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    java.lang.String path = (\"/\" + org.apache.hadoop.hbase.rest.TestRowResource.TABLE) + \"/fakerow\";\n    org.apache.hadoop.hbase.rest.model.CellSetModel cellSetModel = new org.apache.hadoop.hbase.rest.model.CellSetModel();\n    org.apache.hadoop.hbase.rest.model.RowModel rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2)));\n    cellSetModel.addRow(rowModel);\n    rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4)));\n    cellSetModel.addRow(rowModel);\n    java.io.StringWriter writer = new java.io.StringWriter();\n    marshaller.marshal(cellSetModel, writer);\n    org.apache.hadoop.hbase.rest.client.Response response = client.put(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML, org.apache.hadoop.hbase.util.Bytes.toBytes(writer.toString()));\n    java.lang.Thread.yield();\n    response = client.get(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testMultiCellGetPutXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 26,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_5": {
            "Body": "{\n    java.lang.String path = (\"/\" + org.apache.hadoop.hbase.rest.TestRowResource.TABLE) + \"/fakerow\";\n    org.apache.hadoop.hbase.rest.model.CellSetModel cellSetModel = new org.apache.hadoop.hbase.rest.model.CellSetModel();\n    org.apache.hadoop.hbase.rest.model.RowModel rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2)));\n    cellSetModel.addRow(rowModel);\n    rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4)));\n    cellSetModel.addRow(rowModel);\n    org.apache.hadoop.hbase.rest.client.Response response = client.put(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF, cellSetModel.createProtobufOutput());\n    java.lang.Thread.yield();\n    response = client.get(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testMultiCellGetPutPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 24,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "ClassName": "TestRowResource",
        "Commit": "49aee06f9311f28bd2510fb36b2b97a86b4c4333",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 05:24:32 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testDelete",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 20,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274333072"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSingleCellGetPutXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response;\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteValue(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDelete",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 20,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = getValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = putValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSingleCellGetPutPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 15,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    java.lang.String path = (\"/\" + org.apache.hadoop.hbase.rest.TestRowResource.TABLE) + \"/fakerow\";\n    org.apache.hadoop.hbase.rest.model.CellSetModel cellSetModel = new org.apache.hadoop.hbase.rest.model.CellSetModel();\n    org.apache.hadoop.hbase.rest.model.RowModel rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2)));\n    cellSetModel.addRow(rowModel);\n    rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4)));\n    cellSetModel.addRow(rowModel);\n    java.io.StringWriter writer = new java.io.StringWriter();\n    marshaller.marshal(cellSetModel, writer);\n    org.apache.hadoop.hbase.rest.client.Response response = client.put(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML, org.apache.hadoop.hbase.util.Bytes.toBytes(writer.toString()));\n    java.lang.Thread.yield();\n    response = client.get(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testMultiCellGetPutXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 26,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_5": {
            "Body": "{\n    java.lang.String path = (\"/\" + org.apache.hadoop.hbase.rest.TestRowResource.TABLE) + \"/fakerow\";\n    org.apache.hadoop.hbase.rest.model.CellSetModel cellSetModel = new org.apache.hadoop.hbase.rest.model.CellSetModel();\n    org.apache.hadoop.hbase.rest.model.RowModel rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2)));\n    cellSetModel.addRow(rowModel);\n    rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4)));\n    cellSetModel.addRow(rowModel);\n    org.apache.hadoop.hbase.rest.client.Response response = client.put(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF, cellSetModel.createProtobufOutput());\n    java.lang.Thread.yield();\n    response = client.get(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testMultiCellGetPutPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 24,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "ClassName": "TestRowResource",
        "Commit": "49aee06f9311f28bd2510fb36b2b97a86b4c4333",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 05:24:32 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testSingleCellGetPutXML",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 12,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274333072"
    },
    {
        "Body": "{\n    final java.lang.String path = ((((\"/\" + org.apache.hadoop.hbase.rest.TestRowResource.TABLE) + \"/\") + org.apache.hadoop.hbase.rest.TestRowResource.ROW_4) + \"/\") + org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1;\n    org.apache.hadoop.hbase.rest.client.Response response = client.put(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_BINARY, org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4));\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    java.lang.Thread.yield();\n    response = client.get(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_JSON);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_4);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
        "CUT_1": {
            "Body": "{\n    final java.lang.String path = ((((\"/\" + org.apache.hadoop.hbase.rest.TestRowResource.TABLE) + \"/\") + org.apache.hadoop.hbase.rest.TestRowResource.ROW_4) + \"/\") + org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1;\n    org.apache.hadoop.hbase.rest.client.Response response = client.put(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_BINARY, org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4));\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    java.lang.Thread.yield();\n    response = client.get(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_JSON);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_4);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSingleCellGetJSON",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 10,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    final java.lang.String path = ((((\"/\" + org.apache.hadoop.hbase.rest.TestRowResource.TABLE) + \"/\") + org.apache.hadoop.hbase.rest.TestRowResource.ROW_3) + \"/\") + org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1;\n    final byte[] body = org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3);\n    org.apache.hadoop.hbase.rest.client.Response response = client.put(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_BINARY, body);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    java.lang.Thread.yield();\n    response = client.get(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_BINARY);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(response.getBody(), body));\n    boolean foundTimestampHeader = false;\n    for (org.apache.commons.httpclient.Header header : response.getHeaders()) {\n        if (header.getName().equals(\"X-Timestamp\")) {\n            foundTimestampHeader = true;\n            break;\n        }\n    }\n    junit.framework.Assert.assertTrue(foundTimestampHeader);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_3);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSingleCellGetPutBinary",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 20,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response;\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteValue(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDelete",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 20,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSingleCellGetPutXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = getValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = putValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSingleCellGetPutPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 15,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestRowResource",
        "Commit": "49aee06f9311f28bd2510fb36b2b97a86b4c4333",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 05:24:32 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testSingleCellGetJSON",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 10,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 2,
        "ProjectName": "hbase",
        "Timestamp": "1274333072"
    },
    {
        "Body": "{\n    createStoreFile(r);\n    for (int i = 0; i < org.apache.hadoop.hbase.regionserver.TestCompaction.COMPACTION_THRESHOLD; i++) {\n        createStoreFile(r);\n    }\n    org.apache.hadoop.hbase.HBaseTestCase.addContent(new org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon(r), org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.TestCompaction.COLUMN_FAMILY));\n    org.apache.hadoop.hbase.client.Result result = r.get(new org.apache.hadoop.hbase.client.Get(STARTROW).addFamily(org.apache.hadoop.hbase.regionserver.TestCompaction.COLUMN_FAMILY_TEXT).setMaxVersions(100), null);\n    junit.framework.Assert.assertEquals(org.apache.hadoop.hbase.regionserver.TestCompaction.COMPACTION_THRESHOLD, result.size());\n    r.flushcache();\n    r.compactStores();\n    junit.framework.Assert.assertTrue(this.cluster.getFileSystem().exists(this.compactionDir));\n    junit.framework.Assert.assertTrue(!this.cluster.getFileSystem().exists(this.regionCompactionDir));\n    byte[] secondRowBytes = START_KEY.getBytes(org.apache.hadoop.hbase.HConstants.UTF8_ENCODING);\n    secondRowBytes[org.apache.hadoop.hbase.regionserver.TestCompaction.START_KEY_BYTES.length - 1]++;\n    result = r.get(new org.apache.hadoop.hbase.client.Get(secondRowBytes).addFamily(org.apache.hadoop.hbase.regionserver.TestCompaction.COLUMN_FAMILY_TEXT).setMaxVersions(100), null);\n    junit.framework.Assert.assertEquals(3, result.size());\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(secondRowBytes, java.lang.System.currentTimeMillis(), null);\n    byte[][] famAndQf = new byte[][]{ org.apache.hadoop.hbase.regionserver.TestCompaction.COLUMN_FAMILY, null };\n    delete.deleteFamily(famAndQf[0]);\n    r.delete(delete, null, true);\n    result = r.get(new org.apache.hadoop.hbase.client.Get(secondRowBytes).addFamily(org.apache.hadoop.hbase.regionserver.TestCompaction.COLUMN_FAMILY_TEXT).setMaxVersions(100), null);\n    junit.framework.Assert.assertTrue(result.isEmpty());\n    r.flushcache();\n    result = r.get(new org.apache.hadoop.hbase.client.Get(secondRowBytes).addFamily(org.apache.hadoop.hbase.regionserver.TestCompaction.COLUMN_FAMILY_TEXT).setMaxVersions(100), null);\n    junit.framework.Assert.assertTrue(result.isEmpty());\n    createSmallerStoreFile(this.r);\n    r.flushcache();\n    result = r.get(new org.apache.hadoop.hbase.client.Get(secondRowBytes).addFamily(org.apache.hadoop.hbase.regionserver.TestCompaction.COLUMN_FAMILY_TEXT).setMaxVersions(100), null);\n    junit.framework.Assert.assertTrue(result.isEmpty());\n    r.compactStores(true);\n    junit.framework.Assert.assertEquals(r.getStore(org.apache.hadoop.hbase.regionserver.TestCompaction.COLUMN_FAMILY_TEXT).getStorefiles().size(), 1);\n    result = r.get(new org.apache.hadoop.hbase.client.Get(secondRowBytes).addFamily(org.apache.hadoop.hbase.regionserver.TestCompaction.COLUMN_FAMILY_TEXT).setMaxVersions(100), null);\n    junit.framework.Assert.assertTrue(result.isEmpty());\n    int count = 0;\n    boolean containsStartRow = false;\n    for (org.apache.hadoop.hbase.regionserver.StoreFile f : this.r.stores.get(org.apache.hadoop.hbase.regionserver.TestCompaction.COLUMN_FAMILY_TEXT).getStorefiles().values()) {\n        org.apache.hadoop.hbase.io.hfile.HFileScanner scanner = f.getReader().getScanner(false, false);\n        scanner.seekTo();\n        do {\n            byte[] row = scanner.getKeyValue().getRow();\n            if (org.apache.hadoop.hbase.util.Bytes.equals(row, STARTROW)) {\n                containsStartRow = true;\n                count++;\n            } else {\n                junit.framework.Assert.assertFalse(org.apache.hadoop.hbase.util.Bytes.equals(row, secondRowBytes));\n            }\n        } while (scanner.next() );\n    }\n    junit.framework.Assert.assertTrue(containsStartRow);\n    junit.framework.Assert.assertTrue(count == 3);\n    final int ttlInSeconds = 1;\n    for (org.apache.hadoop.hbase.regionserver.Store store : this.r.stores.values()) {\n        store.ttl = ttlInSeconds * 1000;\n    }\n    java.lang.Thread.sleep(ttlInSeconds * 1000);\n    r.compactStores(true);\n    count = count();\n    junit.framework.Assert.assertTrue(count == 0);\n}",
        "CUT_1": {
            "Body": "{\n    createStoreFile(r);\n    for (int i = 0; i < org.apache.hadoop.hbase.regionserver.TestCompaction.COMPACTION_THRESHOLD; i++) {\n        createStoreFile(r);\n    }\n    org.apache.hadoop.hbase.HBaseTestCase.addContent(new org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon(r), org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.TestCompaction.COLUMN_FAMILY));\n    org.apache.hadoop.hbase.client.Result result = r.get(new org.apache.hadoop.hbase.client.Get(STARTROW).addFamily(org.apache.hadoop.hbase.regionserver.TestCompaction.COLUMN_FAMILY_TEXT).setMaxVersions(100), null);\n    junit.framework.Assert.assertEquals(org.apache.hadoop.hbase.regionserver.TestCompaction.COMPACTION_THRESHOLD, result.size());\n    r.flushcache();\n    r.compactStores();\n    junit.framework.Assert.assertTrue(this.cluster.getFileSystem().exists(this.compactionDir));\n    junit.framework.Assert.assertTrue(!this.cluster.getFileSystem().exists(this.regionCompactionDir));\n    byte[] secondRowBytes = START_KEY.getBytes(org.apache.hadoop.hbase.HConstants.UTF8_ENCODING);\n    secondRowBytes[org.apache.hadoop.hbase.regionserver.TestCompaction.START_KEY_BYTES.length - 1]++;\n    result = r.get(new org.apache.hadoop.hbase.client.Get(secondRowBytes).addFamily(org.apache.hadoop.hbase.regionserver.TestCompaction.COLUMN_FAMILY_TEXT).setMaxVersions(100), null);\n    junit.framework.Assert.assertEquals(3, result.size());\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(secondRowBytes, java.lang.System.currentTimeMillis(), null);\n    byte[][] famAndQf = new byte[][]{ org.apache.hadoop.hbase.regionserver.TestCompaction.COLUMN_FAMILY, null };\n    delete.deleteFamily(famAndQf[0]);\n    r.delete(delete, null, true);\n    result = r.get(new org.apache.hadoop.hbase.client.Get(secondRowBytes).addFamily(org.apache.hadoop.hbase.regionserver.TestCompaction.COLUMN_FAMILY_TEXT).setMaxVersions(100), null);\n    junit.framework.Assert.assertTrue(result.isEmpty());\n    r.flushcache();\n    result = r.get(new org.apache.hadoop.hbase.client.Get(secondRowBytes).addFamily(org.apache.hadoop.hbase.regionserver.TestCompaction.COLUMN_FAMILY_TEXT).setMaxVersions(100), null);\n    junit.framework.Assert.assertTrue(result.isEmpty());\n    createSmallerStoreFile(this.r);\n    r.flushcache();\n    result = r.get(new org.apache.hadoop.hbase.client.Get(secondRowBytes).addFamily(org.apache.hadoop.hbase.regionserver.TestCompaction.COLUMN_FAMILY_TEXT).setMaxVersions(100), null);\n    junit.framework.Assert.assertTrue(result.isEmpty());\n    r.compactStores(true);\n    junit.framework.Assert.assertEquals(r.getStore(org.apache.hadoop.hbase.regionserver.TestCompaction.COLUMN_FAMILY_TEXT).getStorefiles().size(), 1);\n    result = r.get(new org.apache.hadoop.hbase.client.Get(secondRowBytes).addFamily(org.apache.hadoop.hbase.regionserver.TestCompaction.COLUMN_FAMILY_TEXT).setMaxVersions(100), null);\n    junit.framework.Assert.assertTrue(result.isEmpty());\n    int count = 0;\n    boolean containsStartRow = false;\n    for (org.apache.hadoop.hbase.regionserver.StoreFile f : this.r.stores.get(org.apache.hadoop.hbase.regionserver.TestCompaction.COLUMN_FAMILY_TEXT).getStorefiles().values()) {\n        org.apache.hadoop.hbase.io.hfile.HFileScanner scanner = f.getReader().getScanner(false, false);\n        scanner.seekTo();\n        do {\n            byte[] row = scanner.getKeyValue().getRow();\n            if (org.apache.hadoop.hbase.util.Bytes.equals(row, STARTROW)) {\n                containsStartRow = true;\n                count++;\n            } else {\n                junit.framework.Assert.assertFalse(org.apache.hadoop.hbase.util.Bytes.equals(row, secondRowBytes));\n            }\n        } while (scanner.next() );\n    }\n    junit.framework.Assert.assertTrue(containsStartRow);\n    junit.framework.Assert.assertTrue(count == 3);\n    final int ttlInSeconds = 1;\n    for (org.apache.hadoop.hbase.regionserver.Store store : this.r.stores.values()) {\n        store.ttl = ttlInSeconds * 1000;\n    }\n    java.lang.Thread.sleep(ttlInSeconds * 1000);\n    r.compactStores(true);\n    count = count();\n    junit.framework.Assert.assertTrue(count == 0);\n}",
            "ClassName": "TestCompaction",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testCompaction",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 59,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(familiy, qualifier);\n    org.apache.hadoop.hbase.client.Result result = region.get(get, null);\n    junit.framework.Assert.assertEquals(1, result.size());\n    org.apache.hadoop.hbase.KeyValue kv = result.raw()[0];\n    long r = org.apache.hadoop.hbase.util.Bytes.toLong(kv.getValue());\n    junit.framework.Assert.assertEquals(amount, r);\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "assertICV",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    int count = 0;\n    for (org.apache.hadoop.hbase.regionserver.StoreFile f : this.r.stores.get(org.apache.hadoop.hbase.regionserver.TestCompaction.COLUMN_FAMILY_TEXT).getStorefiles().values()) {\n        org.apache.hadoop.hbase.io.hfile.HFileScanner scanner = f.getReader().getScanner(false, false);\n        if (!scanner.seekTo()) {\n            continue;\n        }\n        do {\n            count++;\n        } while (scanner.next() );\n    }\n    return count;\n}",
            "ClassName": "TestCompaction",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "count",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 13,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    java.util.List<org.apache.hadoop.hbase.KeyValue> keys = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    org.apache.hadoop.hbase.client.Result r = new org.apache.hadoop.hbase.client.Result(keys);\n    junit.framework.Assert.assertTrue(r.isEmpty());\n    byte[] rb = org.apache.hadoop.hbase.util.Writables.getBytes(r);\n    org.apache.hadoop.hbase.client.Result deserializedR = ((org.apache.hadoop.hbase.client.Result) (org.apache.hadoop.hbase.util.Writables.getWritable(rb, new org.apache.hadoop.hbase.client.Result())));\n    junit.framework.Assert.assertTrue(deserializedR.isEmpty());\n}",
            "ClassName": "TestSerialization",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testResultEmpty",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.util.TestMergeTool.FAMILY);\n    org.apache.hadoop.hbase.regionserver.InternalScanner scanner = merged.getScanner(scan);\n    try {\n        java.util.List<org.apache.hadoop.hbase.KeyValue> testRes = null;\n        while (true) {\n            testRes = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n            boolean hasNext = scanner.next(testRes);\n            if (!hasNext) {\n                break;\n            }\n        } \n    } finally {\n        scanner.close();\n    }\n    for (int i = 0; i < upperbound; i++) {\n        for (int j = 0; j < rows[i].length; j++) {\n            org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(rows[i][j]);\n            get.addFamily(org.apache.hadoop.hbase.util.TestMergeTool.FAMILY);\n            org.apache.hadoop.hbase.client.Result result = merged.get(get, null);\n            junit.framework.Assert.assertEquals(1, result.size());\n            byte[] bytes = result.sorted()[0].getValue();\n            junit.framework.Assert.assertNotNull(org.apache.hadoop.hbase.util.Bytes.toStringBinary(rows[i][j]), bytes);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(bytes, rows[i][j]));\n        }\n    }\n}",
            "ClassName": "TestMergeTool",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "verifyMerge",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 28,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestCompaction",
        "Commit": "49aee06f9311f28bd2510fb36b2b97a86b4c4333",
        "CyclomaticComplexity": 5,
        "Date": "Thu, 20 May 2010 05:24:32 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testCompaction",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 1,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 59,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 2,
        "ProjectName": "hbase",
        "Timestamp": "1274333072"
    },
    {
        "Body": "{\n    final java.lang.String path = ((((\"/\" + org.apache.hadoop.hbase.rest.TestRowResource.TABLE) + \"/\") + org.apache.hadoop.hbase.rest.TestRowResource.ROW_3) + \"/\") + org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1;\n    final byte[] body = org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3);\n    org.apache.hadoop.hbase.rest.client.Response response = client.put(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_BINARY, body);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    java.lang.Thread.yield();\n    response = client.get(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_BINARY);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(response.getBody(), body));\n    boolean foundTimestampHeader = false;\n    for (org.apache.commons.httpclient.Header header : response.getHeaders()) {\n        if (header.getName().equals(\"X-Timestamp\")) {\n            foundTimestampHeader = true;\n            break;\n        }\n    }\n    junit.framework.Assert.assertTrue(foundTimestampHeader);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_3);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
        "CUT_1": {
            "Body": "{\n    final java.lang.String path = ((((\"/\" + org.apache.hadoop.hbase.rest.TestRowResource.TABLE) + \"/\") + org.apache.hadoop.hbase.rest.TestRowResource.ROW_3) + \"/\") + org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1;\n    final byte[] body = org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3);\n    org.apache.hadoop.hbase.rest.client.Response response = client.put(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_BINARY, body);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    java.lang.Thread.yield();\n    response = client.get(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_BINARY);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(response.getBody(), body));\n    boolean foundTimestampHeader = false;\n    for (org.apache.commons.httpclient.Header header : response.getHeaders()) {\n        if (header.getName().equals(\"X-Timestamp\")) {\n            foundTimestampHeader = true;\n            break;\n        }\n    }\n    junit.framework.Assert.assertTrue(foundTimestampHeader);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_3);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSingleCellGetPutBinary",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 20,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    final java.lang.String path = ((((\"/\" + org.apache.hadoop.hbase.rest.TestRowResource.TABLE) + \"/\") + org.apache.hadoop.hbase.rest.TestRowResource.ROW_4) + \"/\") + org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1;\n    org.apache.hadoop.hbase.rest.client.Response response = client.put(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_BINARY, org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4));\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    java.lang.Thread.yield();\n    response = client.get(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_JSON);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_4);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSingleCellGetJSON",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 10,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response;\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteValue(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDelete",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 20,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSingleCellGetPutXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = getValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = putValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSingleCellGetPutPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 15,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestRowResource",
        "Commit": "49aee06f9311f28bd2510fb36b2b97a86b4c4333",
        "CyclomaticComplexity": 2,
        "Date": "Thu, 20 May 2010 05:24:32 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testSingleCellGetPutBinary",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 20,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 2,
        "ProjectName": "hbase",
        "Timestamp": "1274333072"
    },
    {
        "Body": "{\n    doTestTableCreateDrop();\n    doTestTableMutations();\n    doTestTableTimestampsAndColumns();\n    doTestTableScanners();\n}",
        "CUT_1": {
            "Body": "{\n    doTestTableCreateDrop();\n    doTestTableMutations();\n    doTestTableTimestampsAndColumns();\n    doTestTableScanners();\n}",
            "ClassName": "TestThriftServer",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testAll",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.ipc.HBaseServer.Call call = org.apache.hadoop.hbase.ipc.HBaseServer.CurCall.get();\n    if (call != null) {\n        return call.connection.socket.getInetAddress();\n    }\n    return null;\n}",
            "ClassName": "HBaseServer",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getRemoteIp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_3": {
            "Body": "{\n    final org.apache.hadoop.hbase.HRegionInfo oldRegionInfo = region.getRegionInfo();\n    final long startTime = java.lang.System.currentTimeMillis();\n    final org.apache.hadoop.hbase.regionserver.HRegion[] newRegions = region.splitRegion(midKey);\n    if (newRegions == null) {\n        return;\n    }\n    org.apache.hadoop.hbase.client.HTable t = null;\n    if (region.getRegionInfo().isMetaTable()) {\n        if (this.root == null) {\n            this.root = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.ROOT_TABLE_NAME);\n        }\n        t = root;\n    } else {\n        if (meta == null) {\n            meta = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n        }\n        t = meta;\n    }\n    oldRegionInfo.setOffline(true);\n    oldRegionInfo.setSplit(true);\n    this.server.removeFromOnlineRegions(oldRegionInfo);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(oldRegionInfo.getRegionName());\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(oldRegionInfo));\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITA_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(newRegions[0].getRegionInfo()));\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITB_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(newRegions[1].getRegionInfo()));\n    t.put(put);\n    for (int i = 0; i < newRegions.length; i++) {\n        put = new org.apache.hadoop.hbase.client.Put(newRegions[i].getRegionName());\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(newRegions[i].getRegionInfo()));\n        t.put(put);\n    }\n    server.reportSplit(oldRegionInfo, newRegions[0].getRegionInfo(), newRegions[1].getRegionInfo());\n    org.apache.hadoop.hbase.regionserver.CompactSplitThread.LOG.info((((((((\"region split, META updated, and report to master all\" + \" successful. Old region=\") + oldRegionInfo.toString()) + \", new regions: \") + newRegions[0].toString()) + \", \") + newRegions[1].toString()) + \". Split took \") + org.apache.hadoop.util.StringUtils.formatTimeDiff(java.lang.System.currentTimeMillis(), startTime));\n}",
            "ClassName": "CompactSplitThread",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "split",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 37,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    if (!this.changedReaderObservers.remove(o)) {\n        org.apache.hadoop.hbase.regionserver.Store.LOG.warn(\"Not in set\" + o);\n    }\n}",
            "ClassName": "Store",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "deleteChangedReaderObserver",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_5": {
            "Body": "{\n    this.tables = tables;\n}",
            "ClassName": "TableListModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setTables",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestThriftServer",
        "Commit": "49aee06f9311f28bd2510fb36b2b97a86b4c4333",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 05:24:32 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testAll",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 6,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274333072"
    },
    {
        "Body": "{\n    java.lang.String encodedKey = java.net.URLEncoder.encode(\"http://www.google.com/\", org.apache.hadoop.hbase.HConstants.UTF8_ENCODING);\n    org.apache.hadoop.hbase.rest.client.Response response;\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, encodedKey, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = putValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, encodedKey, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, encodedKey, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, encodedKey, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n}",
        "CUT_1": {
            "Body": "{\n    java.lang.String encodedKey = java.net.URLEncoder.encode(\"http://www.google.com/\", org.apache.hadoop.hbase.HConstants.UTF8_ENCODING);\n    org.apache.hadoop.hbase.rest.client.Response response;\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, encodedKey, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = putValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, encodedKey, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, encodedKey, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, encodedKey, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testURLEncodedKey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 10,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = getValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = putValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSingleCellGetPutPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 15,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSingleCellGetPutXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response;\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteValue(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDelete",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 20,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    java.lang.String path = (\"/\" + org.apache.hadoop.hbase.rest.TestRowResource.TABLE) + \"/fakerow\";\n    org.apache.hadoop.hbase.rest.model.CellSetModel cellSetModel = new org.apache.hadoop.hbase.rest.model.CellSetModel();\n    org.apache.hadoop.hbase.rest.model.RowModel rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2)));\n    cellSetModel.addRow(rowModel);\n    rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4)));\n    cellSetModel.addRow(rowModel);\n    org.apache.hadoop.hbase.rest.client.Response response = client.put(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF, cellSetModel.createProtobufOutput());\n    java.lang.Thread.yield();\n    response = client.get(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testMultiCellGetPutPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 24,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "ClassName": "TestRowResource",
        "Commit": "49aee06f9311f28bd2510fb36b2b97a86b4c4333",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 05:24:32 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testURLEncodedKey",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 10,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274333072"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = getValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = putValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = getValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = putValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSingleCellGetPutPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 15,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSingleCellGetPutXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response;\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteValue(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDelete",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 20,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    java.lang.String path = (\"/\" + org.apache.hadoop.hbase.rest.TestRowResource.TABLE) + \"/fakerow\";\n    org.apache.hadoop.hbase.rest.model.CellSetModel cellSetModel = new org.apache.hadoop.hbase.rest.model.CellSetModel();\n    org.apache.hadoop.hbase.rest.model.RowModel rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2)));\n    cellSetModel.addRow(rowModel);\n    rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4)));\n    cellSetModel.addRow(rowModel);\n    org.apache.hadoop.hbase.rest.client.Response response = client.put(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF, cellSetModel.createProtobufOutput());\n    java.lang.Thread.yield();\n    response = client.get(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testMultiCellGetPutPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 24,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_5": {
            "Body": "{\n    java.lang.String path = (\"/\" + org.apache.hadoop.hbase.rest.TestRowResource.TABLE) + \"/fakerow\";\n    org.apache.hadoop.hbase.rest.model.CellSetModel cellSetModel = new org.apache.hadoop.hbase.rest.model.CellSetModel();\n    org.apache.hadoop.hbase.rest.model.RowModel rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2)));\n    cellSetModel.addRow(rowModel);\n    rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4)));\n    cellSetModel.addRow(rowModel);\n    java.io.StringWriter writer = new java.io.StringWriter();\n    marshaller.marshal(cellSetModel, writer);\n    org.apache.hadoop.hbase.rest.client.Response response = client.put(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML, org.apache.hadoop.hbase.util.Bytes.toBytes(writer.toString()));\n    java.lang.Thread.yield();\n    response = client.get(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testMultiCellGetPutXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 26,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "ClassName": "TestRowResource",
        "Commit": "49aee06f9311f28bd2510fb36b2b97a86b4c4333",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 05:24:32 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testSingleCellGetPutPB",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 15,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274333072"
    },
    {
        "Body": "{\n    doATest(false);\n}",
        "CUT_1": {
            "Body": "{\n    doATest(false);\n}",
            "ClassName": "TestMultiParallelPut",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testParallelPut",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    doATest(true);\n}",
            "ClassName": "TestMultiParallelPut",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testParallelPutWithRSAbort",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return false;\n}",
            "ClassName": "WildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "done",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return false;\n}",
            "ClassName": "DependentColumnFilter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "filterAllRemaining",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return false;\n}",
            "ClassName": "FilterBase",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "hasFilterRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestMultiParallelPut",
        "Commit": "3d5c3a66b330ccdacfdb80758177b9f936fccb35",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 20 May 2010 09:02:18 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testParallelPut",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274346138"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.regionserver.HRegion region = null;\n    byte[] c0 = org.apache.hadoop.hbase.HBaseTestCase.COLUMNS[0];\n    try {\n        org.apache.hadoop.hbase.HTableDescriptor htd = createTableDescriptor(getName());\n        region = createNewHRegion(htd, null, null);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10);\n        region.put(p);\n        p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30);\n        region.put(p);\n        p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T40);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T40);\n        region.put(p);\n        org.apache.hadoop.hbase.client.Result r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T35, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, r.getRow()));\n        region.flushcache();\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T35, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, r.getRow()));\n        p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20);\n        region.put(p);\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T35, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, r.getRow()));\n        region.flushcache();\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T35, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, r.getRow()));\n    } finally {\n        if (region != null) {\n            try {\n                region.close();\n            } catch (java.lang.Exception e) {\n                e.printStackTrace();\n            }\n            region.getLog().closeAndDelete();\n        }\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.HRegion region = null;\n    byte[] c0 = org.apache.hadoop.hbase.HBaseTestCase.COLUMNS[0];\n    try {\n        org.apache.hadoop.hbase.HTableDescriptor htd = createTableDescriptor(getName());\n        region = createNewHRegion(htd, null, null);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10);\n        region.put(p);\n        p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30);\n        region.put(p);\n        p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T40);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T40);\n        region.put(p);\n        org.apache.hadoop.hbase.client.Result r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T35, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, r.getRow()));\n        region.flushcache();\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T35, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, r.getRow()));\n        p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20);\n        region.put(p);\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T35, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, r.getRow()));\n        region.flushcache();\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T35, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, r.getRow()));\n    } finally {\n        if (region != null) {\n            try {\n                region.close();\n            } catch (java.lang.Exception e) {\n                e.printStackTrace();\n            }\n            region.getLog().closeAndDelete();\n        }\n    }\n}",
            "ClassName": "TestGetClosestAtOrBefore",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetClosestRowBefore2",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 39,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.HRegion region = null;\n    byte[] c0 = org.apache.hadoop.hbase.HBaseTestCase.COLUMNS[0];\n    byte[] c1 = org.apache.hadoop.hbase.HBaseTestCase.COLUMNS[1];\n    try {\n        org.apache.hadoop.hbase.HTableDescriptor htd = createTableDescriptor(getName());\n        region = createNewHRegion(htd, null, null);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T00);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T00);\n        region.put(p);\n        p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10);\n        region.put(p);\n        p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20);\n        region.put(p);\n        org.apache.hadoop.hbase.client.Result r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20, r.getRow()));\n        org.apache.hadoop.hbase.client.Delete d = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20);\n        d.deleteColumn(c0, c0);\n        region.delete(d, null, false);\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30);\n        region.put(p);\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, r.getRow()));\n        d = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30);\n        d.deleteColumn(c0, c0);\n        region.delete(d, null, false);\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T31, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        region.flushcache();\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T31, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20);\n        p.add(c1, c1, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20);\n        region.put(p);\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T31, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        region.flushcache();\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T31, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        d = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20);\n        d.deleteColumn(c1, c1);\n        region.delete(d, null, false);\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T31, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        region.flushcache();\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T31, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T11);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T11);\n        region.put(p);\n        d = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10);\n        d.deleteColumn(c1, c1);\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T12, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T11, r.getRow()));\n    } finally {\n        if (region != null) {\n            try {\n                region.close();\n            } catch (java.lang.Exception e) {\n                e.printStackTrace();\n            }\n            region.getLog().closeAndDelete();\n        }\n    }\n}",
            "ClassName": "TestGetClosestAtOrBefore",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetClosestRowBefore3",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 80,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.HRegion region = org.apache.hadoop.hbase.regionserver.HRegion.createHRegion(newRegion, this.master.getRootDir(), master.getConfiguration());\n    org.apache.hadoop.hbase.HRegionInfo info = region.getRegionInfo();\n    byte[] regionName = region.getRegionName();\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(regionName);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(info));\n    server.put(metaRegionName, put);\n    region.close();\n    region.getLog().closeAndDelete();\n    setUnassigned(info, true);\n}",
            "ClassName": "RegionManager",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 11,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.HRegion region = createNewHRegion(desc, startKey, endKey);\n    java.lang.System.out.println(\"created region \" + org.apache.hadoop.hbase.util.Bytes.toString(region.getRegionName()));\n    org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon r = new org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon(region);\n    for (int i = firstRow; i < (firstRow + nrows); i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row_\" + java.lang.String.format(\"%1$05d\", i)));\n        put.add(org.apache.hadoop.hbase.AbstractMergeTestBase.COLUMN_NAME, null, value.get());\n        region.put(put);\n        if ((i % 10000) == 0) {\n            java.lang.System.out.println(\"Flushing write #\" + i);\n            r.flushcache();\n        }\n    }\n    region.close();\n    region.getLog().closeAndDelete();\n    region.getRegionInfo().setOffline(true);\n    return region;\n}",
            "ClassName": "AbstractMergeTestBase",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createAregion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 18,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.HRegion region = createNewHRegion(desc, startKey, endKey);\n    byte[] keyToWrite = (startKey == null) ? org.apache.hadoop.hbase.util.Bytes.toBytes(\"row_000\") : startKey;\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(keyToWrite);\n    put.add(FAMILY_NAME, null, org.apache.hadoop.hbase.util.Bytes.toBytes(\"test\"));\n    region.put(put);\n    region.close();\n    region.getLog().closeAndDelete();\n    return region;\n}",
            "ClassName": "TestRegionRebalancing",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createAregion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 10,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestGetClosestAtOrBefore",
        "Commit": "897e903d90c11999a9e9b4ed9fb46b473309af09",
        "CyclomaticComplexity": 1,
        "Date": "Thu, 20 May 2010 18:10:57 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testGetClosestRowBefore2",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 39,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274379057"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.regionserver.HRegion region = null;\n    byte[] c0 = org.apache.hadoop.hbase.HBaseTestCase.COLUMNS[0];\n    byte[] c1 = org.apache.hadoop.hbase.HBaseTestCase.COLUMNS[1];\n    try {\n        org.apache.hadoop.hbase.HTableDescriptor htd = createTableDescriptor(getName());\n        region = createNewHRegion(htd, null, null);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T00);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T00);\n        region.put(p);\n        p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10);\n        region.put(p);\n        p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20);\n        region.put(p);\n        org.apache.hadoop.hbase.client.Result r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20, r.getRow()));\n        org.apache.hadoop.hbase.client.Delete d = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20);\n        d.deleteColumn(c0, c0);\n        region.delete(d, null, false);\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30);\n        region.put(p);\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, r.getRow()));\n        d = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30);\n        d.deleteColumn(c0, c0);\n        region.delete(d, null, false);\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T31, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        region.flushcache();\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T31, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20);\n        p.add(c1, c1, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20);\n        region.put(p);\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T31, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        region.flushcache();\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T31, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        d = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20);\n        d.deleteColumn(c1, c1);\n        region.delete(d, null, false);\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T31, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        region.flushcache();\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T31, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T11);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T11);\n        region.put(p);\n        d = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10);\n        d.deleteColumn(c1, c1);\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T12, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T11, r.getRow()));\n    } finally {\n        if (region != null) {\n            try {\n                region.close();\n            } catch (java.lang.Exception e) {\n                e.printStackTrace();\n            }\n            region.getLog().closeAndDelete();\n        }\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.HRegion region = null;\n    byte[] c0 = org.apache.hadoop.hbase.HBaseTestCase.COLUMNS[0];\n    byte[] c1 = org.apache.hadoop.hbase.HBaseTestCase.COLUMNS[1];\n    try {\n        org.apache.hadoop.hbase.HTableDescriptor htd = createTableDescriptor(getName());\n        region = createNewHRegion(htd, null, null);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T00);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T00);\n        region.put(p);\n        p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10);\n        region.put(p);\n        p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20);\n        region.put(p);\n        org.apache.hadoop.hbase.client.Result r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20, r.getRow()));\n        org.apache.hadoop.hbase.client.Delete d = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20);\n        d.deleteColumn(c0, c0);\n        region.delete(d, null, false);\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30);\n        region.put(p);\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, r.getRow()));\n        d = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30);\n        d.deleteColumn(c0, c0);\n        region.delete(d, null, false);\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T31, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        region.flushcache();\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T31, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20);\n        p.add(c1, c1, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20);\n        region.put(p);\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T31, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        region.flushcache();\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T31, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        d = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20);\n        d.deleteColumn(c1, c1);\n        region.delete(d, null, false);\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T31, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        region.flushcache();\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T31, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10, r.getRow()));\n        p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T11);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T11);\n        region.put(p);\n        d = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10);\n        d.deleteColumn(c1, c1);\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T12, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T11, r.getRow()));\n    } finally {\n        if (region != null) {\n            try {\n                region.close();\n            } catch (java.lang.Exception e) {\n                e.printStackTrace();\n            }\n            region.getLog().closeAndDelete();\n        }\n    }\n}",
            "ClassName": "TestGetClosestAtOrBefore",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetClosestRowBefore3",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 80,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.HRegion region = null;\n    byte[] c0 = org.apache.hadoop.hbase.HBaseTestCase.COLUMNS[0];\n    try {\n        org.apache.hadoop.hbase.HTableDescriptor htd = createTableDescriptor(getName());\n        region = createNewHRegion(htd, null, null);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T10);\n        region.put(p);\n        p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30);\n        region.put(p);\n        p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T40);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T40);\n        region.put(p);\n        org.apache.hadoop.hbase.client.Result r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T35, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, r.getRow()));\n        region.flushcache();\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T35, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, r.getRow()));\n        p = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20);\n        p.add(c0, c0, org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T20);\n        region.put(p);\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T35, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, r.getRow()));\n        region.flushcache();\n        r = region.getClosestRowBefore(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T35, c0);\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.T30, r.getRow()));\n    } finally {\n        if (region != null) {\n            try {\n                region.close();\n            } catch (java.lang.Exception e) {\n                e.printStackTrace();\n            }\n            region.getLog().closeAndDelete();\n        }\n    }\n}",
            "ClassName": "TestGetClosestAtOrBefore",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetClosestRowBefore2",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 39,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.HRegion region = org.apache.hadoop.hbase.regionserver.HRegion.createHRegion(newRegion, this.master.getRootDir(), master.getConfiguration());\n    org.apache.hadoop.hbase.HRegionInfo info = region.getRegionInfo();\n    byte[] regionName = region.getRegionName();\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(regionName);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(info));\n    server.put(metaRegionName, put);\n    region.close();\n    region.getLog().closeAndDelete();\n    setUnassigned(info, true);\n}",
            "ClassName": "RegionManager",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 11,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    final int COL_COUNT = 10;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"tablename\");\n    final byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\");\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = null;\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(fs, dir, this.oldLogDir, this.conf, null);\n    try {\n        long timestamp = java.lang.System.currentTimeMillis();\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit cols = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        for (int i = 0; i < COL_COUNT; i++) {\n            cols.add(new org.apache.hadoop.hbase.KeyValue(row, org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\"), org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(i)), timestamp, new byte[]{ ((byte) (i + '0')) }));\n        }\n        org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(tableName), row, org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.util.Bytes.toString(row) + \"1\"), false);\n        final byte[] regionName = info.getRegionName();\n        log.append(info, tableName, cols, java.lang.System.currentTimeMillis());\n        long logSeqId = log.startCacheFlush();\n        log.completeCacheFlush(regionName, tableName, logSeqId, info.isMetaRegion());\n        log.close();\n        org.apache.hadoop.fs.Path filename = log.computeFilename(log.getFilenum());\n        log = null;\n        reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, filename, conf);\n        for (int i = 0; i < 1; i++) {\n            org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = reader.next(null);\n            if (entry == null)\n                break;\n\n            org.apache.hadoop.hbase.regionserver.wal.HLogKey key = entry.getKey();\n            org.apache.hadoop.hbase.regionserver.wal.WALEdit val = entry.getEdit();\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(regionName, key.getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, key.getTablename()));\n            org.apache.hadoop.hbase.KeyValue kv = val.getKeyValues().get(0);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(row, kv.getRow()));\n            junit.framework.Assert.assertEquals(((byte) (i + '0')), kv.getValue()[0]);\n            java.lang.System.out.println((key + \" \") + val);\n        }\n        org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = null;\n        while ((entry = reader.next(null)) != null) {\n            org.apache.hadoop.hbase.regionserver.wal.HLogKey key = entry.getKey();\n            org.apache.hadoop.hbase.regionserver.wal.WALEdit val = entry.getEdit();\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(regionName, key.getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, key.getTablename()));\n            org.apache.hadoop.hbase.KeyValue kv = val.getKeyValues().get(0);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, kv.getRow()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, kv.getFamily()));\n            junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH, val.getKeyValues().get(0).getValue()));\n            java.lang.System.out.println((key + \" \") + val);\n        } \n    } finally {\n        if (log != null) {\n            log.closeAndDelete();\n        }\n        if (reader != null) {\n            reader.close();\n        }\n    }\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 6,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testEditAdd",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 56,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    byte[] tableb = org.apache.hadoop.hbase.util.Bytes.toBytes(\"\" + table);\n    byte[] tofindBytes = org.apache.hadoop.hbase.util.Bytes.toBytes(((short) (rowToFind)));\n    byte[] metaKey = org.apache.hadoop.hbase.HRegionInfo.createRegionName(tableb, tofindBytes, org.apache.hadoop.hbase.HConstants.NINES);\n    org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.LOG.info(\"find=\" + new java.lang.String(metaKey));\n    org.apache.hadoop.hbase.client.Result r = mr.getClosestRowBefore(metaKey);\n    if (answer == (-1)) {\n        junit.framework.Assert.assertNull(r);\n        return null;\n    }\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.compareTo(org.apache.hadoop.hbase.util.Bytes.toBytes(((short) (answer))), extractRowFromMetaRow(r.getRow())) == 0);\n    return r.getRow();\n}",
            "ClassName": "TestGetClosestAtOrBefore",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "findRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 13,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestGetClosestAtOrBefore",
        "Commit": "897e903d90c11999a9e9b4ed9fb46b473309af09",
        "CyclomaticComplexity": 1,
        "Date": "Thu, 20 May 2010 18:10:57 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testGetClosestRowBefore3",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 80,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274379057"
    },
    {
        "Body": "{\n    org.apache.hadoop.fs.FileSystem filesystem = org.apache.hadoop.fs.FileSystem.get(conf);\n    org.apache.hadoop.fs.Path rootdir = filesystem.makeQualified(new org.apache.hadoop.fs.Path(conf.get(org.apache.hadoop.hbase.HConstants.HBASE_DIR)));\n    filesystem.mkdirs(rootdir);\n    org.apache.hadoop.hbase.HRegionInfo.FIRST_META_REGIONINFO.getTableDesc().setMemStoreFlushSize((64 * 1024) * 1024);\n    org.apache.hadoop.hbase.regionserver.HRegion mr = org.apache.hadoop.hbase.regionserver.HRegion.createHRegion(org.apache.hadoop.hbase.HRegionInfo.FIRST_META_REGIONINFO, rootdir, this.conf);\n    for (char c = 'A'; c < 'D'; c++) {\n        org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(\"\" + c);\n        final int last = 128;\n        final int interval = 2;\n        for (int i = 0; i <= last; i += interval) {\n            org.apache.hadoop.hbase.HRegionInfo hri = new org.apache.hadoop.hbase.HRegionInfo(htd, i == 0 ? org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY : org.apache.hadoop.hbase.util.Bytes.toBytes(((byte) (i))), i == last ? org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY : org.apache.hadoop.hbase.util.Bytes.toBytes(((byte) (i)) + interval));\n            org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(hri.getRegionName());\n            put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(hri));\n            mr.put(put, false);\n        }\n    }\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = mr.getScanner(new org.apache.hadoop.hbase.client.Scan());\n    try {\n        java.util.List<org.apache.hadoop.hbase.KeyValue> keys = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        while (s.next(keys)) {\n            org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.LOG.info(keys);\n            keys.clear();\n        } \n    } finally {\n        s.close();\n    }\n    findRow(mr, 'C', 44, 44);\n    findRow(mr, 'C', 45, 44);\n    findRow(mr, 'C', 46, 46);\n    findRow(mr, 'C', 43, 42);\n    mr.flushcache();\n    findRow(mr, 'C', 44, 44);\n    findRow(mr, 'C', 45, 44);\n    findRow(mr, 'C', 46, 46);\n    findRow(mr, 'C', 43, 42);\n    byte[] firstRowInC = org.apache.hadoop.hbase.HRegionInfo.createRegionName(org.apache.hadoop.hbase.util.Bytes.toBytes(\"\" + 'C'), org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY, org.apache.hadoop.hbase.HConstants.ZEROES);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(firstRowInC);\n    s = mr.getScanner(scan);\n    try {\n        java.util.List<org.apache.hadoop.hbase.KeyValue> keys = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        while (s.next(keys)) {\n            mr.delete(new org.apache.hadoop.hbase.client.Delete(keys.get(0).getRow()), null, false);\n            keys.clear();\n        } \n    } finally {\n        s.close();\n    }\n    findRow(mr, 'C', 44, -1);\n    findRow(mr, 'C', 45, -1);\n    findRow(mr, 'C', 46, -1);\n    findRow(mr, 'C', 43, -1);\n    mr.flushcache();\n    findRow(mr, 'C', 44, -1);\n    findRow(mr, 'C', 45, -1);\n    findRow(mr, 'C', 46, -1);\n    findRow(mr, 'C', 43, -1);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.fs.FileSystem filesystem = org.apache.hadoop.fs.FileSystem.get(conf);\n    org.apache.hadoop.fs.Path rootdir = filesystem.makeQualified(new org.apache.hadoop.fs.Path(conf.get(org.apache.hadoop.hbase.HConstants.HBASE_DIR)));\n    filesystem.mkdirs(rootdir);\n    org.apache.hadoop.hbase.HRegionInfo.FIRST_META_REGIONINFO.getTableDesc().setMemStoreFlushSize((64 * 1024) * 1024);\n    org.apache.hadoop.hbase.regionserver.HRegion mr = org.apache.hadoop.hbase.regionserver.HRegion.createHRegion(org.apache.hadoop.hbase.HRegionInfo.FIRST_META_REGIONINFO, rootdir, this.conf);\n    for (char c = 'A'; c < 'D'; c++) {\n        org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(\"\" + c);\n        final int last = 128;\n        final int interval = 2;\n        for (int i = 0; i <= last; i += interval) {\n            org.apache.hadoop.hbase.HRegionInfo hri = new org.apache.hadoop.hbase.HRegionInfo(htd, i == 0 ? org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY : org.apache.hadoop.hbase.util.Bytes.toBytes(((byte) (i))), i == last ? org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY : org.apache.hadoop.hbase.util.Bytes.toBytes(((byte) (i)) + interval));\n            org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(hri.getRegionName());\n            put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(hri));\n            mr.put(put, false);\n        }\n    }\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = mr.getScanner(new org.apache.hadoop.hbase.client.Scan());\n    try {\n        java.util.List<org.apache.hadoop.hbase.KeyValue> keys = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        while (s.next(keys)) {\n            org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.LOG.info(keys);\n            keys.clear();\n        } \n    } finally {\n        s.close();\n    }\n    findRow(mr, 'C', 44, 44);\n    findRow(mr, 'C', 45, 44);\n    findRow(mr, 'C', 46, 46);\n    findRow(mr, 'C', 43, 42);\n    mr.flushcache();\n    findRow(mr, 'C', 44, 44);\n    findRow(mr, 'C', 45, 44);\n    findRow(mr, 'C', 46, 46);\n    findRow(mr, 'C', 43, 42);\n    byte[] firstRowInC = org.apache.hadoop.hbase.HRegionInfo.createRegionName(org.apache.hadoop.hbase.util.Bytes.toBytes(\"\" + 'C'), org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY, org.apache.hadoop.hbase.HConstants.ZEROES);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(firstRowInC);\n    s = mr.getScanner(scan);\n    try {\n        java.util.List<org.apache.hadoop.hbase.KeyValue> keys = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        while (s.next(keys)) {\n            mr.delete(new org.apache.hadoop.hbase.client.Delete(keys.get(0).getRow()), null, false);\n            keys.clear();\n        } \n    } finally {\n        s.close();\n    }\n    findRow(mr, 'C', 44, -1);\n    findRow(mr, 'C', 45, -1);\n    findRow(mr, 'C', 46, -1);\n    findRow(mr, 'C', 43, -1);\n    mr.flushcache();\n    findRow(mr, 'C', 44, -1);\n    findRow(mr, 'C', 45, -1);\n    findRow(mr, 'C', 46, -1);\n    findRow(mr, 'C', 43, -1);\n}",
            "ClassName": "TestGetClosestAtOrBefore",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testUsingMetaAndBinary",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 58,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.master.MetaRegion mr = this.master.getRegionManager().getFirstMetaRegionForRegion(hri);\n    org.apache.hadoop.hbase.client.Get g = new org.apache.hadoop.hbase.client.Get(hri.getRegionName());\n    g.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    try {\n        org.apache.hadoop.hbase.ipc.HRegionInterface server = this.master.getServerConnection().getHRegionConnection(mr.getServer());\n        org.apache.hadoop.hbase.client.Result r = server.get(mr.getRegionName(), g);\n        if (r.size() >= 3)\n            return;\n\n    } catch (java.io.IOException e) {\n        org.apache.hadoop.hbase.master.ServerManager.LOG.warn((\"Failed get on \" + org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY_STR) + \"; possible double-assignment?\", e);\n    }\n    this.master.getRegionManager().setUnassigned(hri, false);\n}",
            "ClassName": "ServerManager",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "assignSplitDaughter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 15,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress>> result = new java.util.ArrayList<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress>>();\n    java.util.Set<org.apache.hadoop.hbase.master.MetaRegion> regions = this.regionManager.getMetaRegionsForTable(tableName);\n    byte[] firstRowInTable = org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.util.Bytes.toString(tableName) + \",,\");\n    for (org.apache.hadoop.hbase.master.MetaRegion m : regions) {\n        byte[] metaRegionName = m.getRegionName();\n        org.apache.hadoop.hbase.ipc.HRegionInterface srvr = this.connection.getHRegionConnection(m.getServer());\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(firstRowInTable);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        long scannerid = srvr.openScanner(metaRegionName, scan);\n        try {\n            while (true) {\n                org.apache.hadoop.hbase.client.Result data = srvr.next(scannerid);\n                if ((data == null) || (data.size() <= 0))\n                    break;\n\n                org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(data.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER));\n                if (org.apache.hadoop.hbase.util.Bytes.equals(info.getTableDesc().getName(), tableName)) {\n                    byte[] value = data.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n                    if (value != null) {\n                        org.apache.hadoop.hbase.HServerAddress server = new org.apache.hadoop.hbase.HServerAddress(org.apache.hadoop.hbase.util.Bytes.toString(value));\n                        result.add(new org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress>(info, server));\n                    }\n                } else {\n                    break;\n                }\n            } \n        } finally {\n            srvr.close(scannerid);\n        }\n    }\n    return result;\n}",
            "ClassName": "HMaster",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 34,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    if (org.apache.hadoop.hbase.util.MetaUtils.LOG.isDebugEnabled()) {\n        org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(hri.getRegionName());\n        get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        org.apache.hadoop.hbase.client.Result res = r.get(get, null);\n        org.apache.hadoop.hbase.KeyValue[] kvs = res.raw();\n        if (kvs.length <= 0) {\n            return;\n        }\n        byte[] value = kvs[0].getValue();\n        if (value == null) {\n            return;\n        }\n        org.apache.hadoop.hbase.HRegionInfo h = org.apache.hadoop.hbase.util.Writables.getHRegionInfoOrNull(value);\n        org.apache.hadoop.hbase.util.MetaUtils.LOG.debug(((((((((\"Old \" + org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY)) + \":\") + org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER)) + \" for \") + hri.toString()) + \" in \") + r.toString()) + \" is: \") + h.toString());\n    }\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(hri.getRegionName());\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(hri));\n    r.put(put);\n    if (org.apache.hadoop.hbase.util.MetaUtils.LOG.isDebugEnabled()) {\n        org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(hri.getRegionName());\n        get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        org.apache.hadoop.hbase.client.Result res = r.get(get, null);\n        org.apache.hadoop.hbase.KeyValue[] kvs = res.raw();\n        if (kvs.length <= 0) {\n            return;\n        }\n        byte[] value = kvs[0].getValue();\n        if (value == null) {\n            return;\n        }\n        org.apache.hadoop.hbase.HRegionInfo h = org.apache.hadoop.hbase.util.Writables.getHRegionInfoOrNull(value);\n        org.apache.hadoop.hbase.util.MetaUtils.LOG.debug(((((((((\"New \" + org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY)) + \":\") + org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER)) + \" for \") + hri.toString()) + \" in \") + r.toString()) + \" is: \") + h.toString());\n    }\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 6,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "updateMETARegionInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 35,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(c, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.Result res = t.get(get);\n    org.apache.hadoop.hbase.KeyValue[] kvs = res.raw();\n    if (kvs.length <= 0) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    byte[] value = kvs[0].getValue();\n    if (value == null) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(value);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    info.setOffline(onlineOffline);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(info));\n    t.put(put);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(row);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER);\n    t.delete(delete);\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "changeOnlineStatus",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestGetClosestAtOrBefore",
        "Commit": "897e903d90c11999a9e9b4ed9fb46b473309af09",
        "CyclomaticComplexity": 4,
        "Date": "Thu, 20 May 2010 18:10:57 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testUsingMetaAndBinary",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 58,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274379057"
    },
    {
        "Body": "{\n    runTestOnTable(new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.mapred.TestTableMapReduce.MULTI_REGION_TABLE_NAME));\n}",
        "CUT_1": {
            "Body": "{\n    runTestOnTable(new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.MULTI_REGION_TABLE_NAME));\n}",
            "ClassName": "TestTableMapReduce",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testMultiRegionTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    this.server = cluster.getRegionServerThreads().get(0).getRegionServer();\n    this.log = server.getLog();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    for (int i = 1; i <= 256; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + java.lang.String.format(\"%1$04d\", i)));\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, value);\n        table.put(put);\n        if ((i % 32) == 0) {\n            try {\n                java.lang.Thread.sleep(2000);\n            } catch (java.lang.InterruptedException e) {\n            }\n        }\n    }\n}",
            "ClassName": "TestLogRolling",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startAndWriteData",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_3": {
            "Body": "{\n    return org.apache.hadoop.hbase.client.HTable.isTableEnabled(conf, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n}",
            "ClassName": "HTable",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "isTableEnabled",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        org.apache.hadoop.hbase.HRegionInfo hri = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(b);\n        byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(row);\n        p.add(org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily(), org.apache.hadoop.hbase.master.TestMasterTransitions.getTestQualifier(), row);\n        t.put(p);\n        rows++;\n    }\n    s.close();\n    org.junit.Assert.assertEquals(expected, rows);\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addToEachStartKey",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.mapred.MiniMRCluster mrCluster = new org.apache.hadoop.mapred.MiniMRCluster(2, fs.getUri().toString(), 1);\n    org.apache.hadoop.mapreduce.Job job = null;\n    try {\n        org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.LOG.info(\"Before map/reduce startup\");\n        job = new org.apache.hadoop.mapreduce.Job(conf, \"process column contents\");\n        job.setNumReduceTasks(1);\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.addFamily(org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.INPUT_FAMILY);\n        org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(org.apache.hadoop.hbase.util.Bytes.toString(table.getTableName()), scan, org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.ProcessContentsMapper.class, org.apache.hadoop.hbase.io.ImmutableBytesWritable.class, org.apache.hadoop.hbase.client.Put.class, job);\n        org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob(org.apache.hadoop.hbase.util.Bytes.toString(table.getTableName()), org.apache.hadoop.hbase.mapreduce.IdentityTableReducer.class, job);\n        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new org.apache.hadoop.fs.Path(\"test\"));\n        org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.LOG.info(\"Started \" + org.apache.hadoop.hbase.util.Bytes.toString(table.getTableName()));\n        job.waitForCompletion(true);\n        org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.LOG.info(\"After map/reduce completion\");\n        verify(org.apache.hadoop.hbase.util.Bytes.toString(table.getTableName()));\n    } finally {\n        mrCluster.shutdown();\n        if (job != null) {\n            org.apache.hadoop.fs.FileUtil.fullyDelete(new java.io.File(job.getConfiguration().get(\"hadoop.tmp.dir\")));\n        }\n    }\n}",
            "ClassName": "TestTableMapReduce",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "runTestOnTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 3,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableMapReduce",
        "Commit": "b2d7b19bc324c8d234bbcad7ef6bf1683182fbbb",
        "CyclomaticComplexity": 0,
        "Date": "Fri, 21 May 2010 05:54:17 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testMultiRegionTable",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274421257"
    },
    {
        "Body": "{\n    prepareTest();\n    runTestOnTable();\n    verify();\n}",
        "CUT_1": {
            "Body": "{\n    prepareTest();\n    runTestOnTable();\n    verify();\n}",
            "ClassName": "TestTimeRangeMapRed",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTimeRangeMapRed",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    runTestOnTable(new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.MULTI_REGION_TABLE_NAME));\n}",
            "ClassName": "TestTableMapReduce",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testMultiRegionTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    java.util.Map<java.lang.Long, org.apache.hadoop.hbase.regionserver.StoreFile> results = new java.util.HashMap<java.lang.Long, org.apache.hadoop.hbase.regionserver.StoreFile>();\n    org.apache.hadoop.fs.FileStatus[] files = this.fs.listStatus(this.homedir);\n    for (int i = 0; (files != null) && (i < files.length); i++) {\n        if (files[i].isDir()) {\n            continue;\n        }\n        org.apache.hadoop.fs.Path p = files[i].getPath();\n        if (this.fs.getFileStatus(p).getLen() <= 0) {\n            org.apache.hadoop.hbase.regionserver.Store.LOG.warn((\"Skipping \" + p) + \" because its empty. HBASE-646 DATA LOSS?\");\n            continue;\n        }\n        org.apache.hadoop.hbase.regionserver.StoreFile curfile = null;\n        try {\n            curfile = new org.apache.hadoop.hbase.regionserver.StoreFile(fs, p, blockcache, this.conf, this.family.getBloomFilterType(), this.inMemory);\n            curfile.createReader();\n        } catch (java.io.IOException ioe) {\n            org.apache.hadoop.hbase.regionserver.Store.LOG.warn((((\"Failed open of \" + p) + \"; presumption is that file was \") + \"corrupted at flush and lost edits picked up by commit log replay. \") + \"Verify!\", ioe);\n            continue;\n        }\n        long storeSeqId = curfile.getMaxSequenceId();\n        if (storeSeqId > this.maxSeqId) {\n            this.maxSeqId = storeSeqId;\n        }\n        long length = curfile.getReader().length();\n        this.storeSize += length;\n        if (org.apache.hadoop.hbase.regionserver.Store.LOG.isDebugEnabled()) {\n            org.apache.hadoop.hbase.regionserver.Store.LOG.debug(((((((((\"loaded \" + org.apache.hadoop.hbase.util.FSUtils.getPath(p)) + \", isReference=\") + curfile.isReference()) + \", sequence id=\") + storeSeqId) + \", length=\") + length) + \", majorCompaction=\") + curfile.isMajorCompaction());\n        }\n        results.put(java.lang.Long.valueOf(storeSeqId), curfile);\n    }\n    return results;\n}",
            "ClassName": "Store",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "loadStoreFiles",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 33,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.mapred.MiniMRCluster mrCluster = new org.apache.hadoop.mapred.MiniMRCluster(2, fs.getUri().toString(), 1);\n    org.apache.hadoop.mapreduce.Job job = null;\n    try {\n        org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.LOG.info(\"Before map/reduce startup\");\n        job = new org.apache.hadoop.mapreduce.Job(conf, \"process column contents\");\n        job.setNumReduceTasks(1);\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.addFamily(org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.INPUT_FAMILY);\n        org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(org.apache.hadoop.hbase.util.Bytes.toString(table.getTableName()), scan, org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.ProcessContentsMapper.class, org.apache.hadoop.hbase.io.ImmutableBytesWritable.class, org.apache.hadoop.hbase.client.Put.class, job);\n        org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob(org.apache.hadoop.hbase.util.Bytes.toString(table.getTableName()), org.apache.hadoop.hbase.mapreduce.IdentityTableReducer.class, job);\n        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new org.apache.hadoop.fs.Path(\"test\"));\n        org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.LOG.info(\"Started \" + org.apache.hadoop.hbase.util.Bytes.toString(table.getTableName()));\n        job.waitForCompletion(true);\n        org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.LOG.info(\"After map/reduce completion\");\n        verify(org.apache.hadoop.hbase.util.Bytes.toString(table.getTableName()));\n    } finally {\n        mrCluster.shutdown();\n        if (job != null) {\n            org.apache.hadoop.fs.FileUtil.fullyDelete(new java.io.File(job.getConfiguration().get(\"hadoop.tmp.dir\")));\n        }\n    }\n}",
            "ClassName": "TestTableMapReduce",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "runTestOnTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 3,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    final org.apache.hadoop.hbase.HRegionInfo oldRegionInfo = region.getRegionInfo();\n    final long startTime = java.lang.System.currentTimeMillis();\n    final org.apache.hadoop.hbase.regionserver.HRegion[] newRegions = region.splitRegion(midKey);\n    if (newRegions == null) {\n        return;\n    }\n    org.apache.hadoop.hbase.client.HTable t = null;\n    if (region.getRegionInfo().isMetaTable()) {\n        if (this.root == null) {\n            this.root = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.ROOT_TABLE_NAME);\n        }\n        t = root;\n    } else {\n        if (meta == null) {\n            meta = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n        }\n        t = meta;\n    }\n    oldRegionInfo.setOffline(true);\n    oldRegionInfo.setSplit(true);\n    this.server.removeFromOnlineRegions(oldRegionInfo);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(oldRegionInfo.getRegionName());\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(oldRegionInfo));\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITA_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(newRegions[0].getRegionInfo()));\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITB_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(newRegions[1].getRegionInfo()));\n    t.put(put);\n    for (int i = 0; i < newRegions.length; i++) {\n        put = new org.apache.hadoop.hbase.client.Put(newRegions[i].getRegionName());\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(newRegions[i].getRegionInfo()));\n        t.put(put);\n    }\n    server.reportSplit(oldRegionInfo, newRegions[0].getRegionInfo(), newRegions[1].getRegionInfo());\n    org.apache.hadoop.hbase.regionserver.CompactSplitThread.LOG.info((((((((\"region split, META updated, and report to master all\" + \" successful. Old region=\") + oldRegionInfo.toString()) + \", new regions: \") + newRegions[0].toString()) + \", \") + newRegions[1].toString()) + \". Split took \") + org.apache.hadoop.util.StringUtils.formatTimeDiff(java.lang.System.currentTimeMillis(), startTime));\n}",
            "ClassName": "CompactSplitThread",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "split",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 37,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTimeRangeMapRed",
        "Commit": "b2d7b19bc324c8d234bbcad7ef6bf1683182fbbb",
        "CyclomaticComplexity": 0,
        "Date": "Fri, 21 May 2010 05:54:17 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testTimeRangeMapRed",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 5,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274421257"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(createTableDescriptor(org.apache.hadoop.hbase.master.TestMinimumServerCount.TABLE_NAME));\n    admin.disableTable(org.apache.hadoop.hbase.master.TestMinimumServerCount.TABLE_NAME);\n    junit.framework.Assert.assertFalse(admin.isTableEnabled(org.apache.hadoop.hbase.master.TestMinimumServerCount.TABLE_NAME));\n    cluster.hbaseCluster.getMaster().getServerManager().setMinimumServerCount(2);\n    try {\n        admin.enableTable(org.apache.hadoop.hbase.master.TestMinimumServerCount.TABLE_NAME);\n    } catch (java.io.IOException ex) {\n    }\n    java.lang.Thread.sleep(10 * 1000);\n    junit.framework.Assert.assertFalse(admin.isTableAvailable(org.apache.hadoop.hbase.master.TestMinimumServerCount.TABLE_NAME));\n    cluster.startRegionServer();\n    java.lang.Thread.sleep(10 * 1000);\n    junit.framework.Assert.assertTrue(admin.isTableAvailable(org.apache.hadoop.hbase.master.TestMinimumServerCount.TABLE_NAME));\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(createTableDescriptor(org.apache.hadoop.hbase.master.TestMinimumServerCount.TABLE_NAME));\n    admin.disableTable(org.apache.hadoop.hbase.master.TestMinimumServerCount.TABLE_NAME);\n    junit.framework.Assert.assertFalse(admin.isTableEnabled(org.apache.hadoop.hbase.master.TestMinimumServerCount.TABLE_NAME));\n    cluster.hbaseCluster.getMaster().getServerManager().setMinimumServerCount(2);\n    try {\n        admin.enableTable(org.apache.hadoop.hbase.master.TestMinimumServerCount.TABLE_NAME);\n    } catch (java.io.IOException ex) {\n    }\n    java.lang.Thread.sleep(10 * 1000);\n    junit.framework.Assert.assertFalse(admin.isTableAvailable(org.apache.hadoop.hbase.master.TestMinimumServerCount.TABLE_NAME));\n    cluster.startRegionServer();\n    java.lang.Thread.sleep(10 * 1000);\n    junit.framework.Assert.assertTrue(admin.isTableAvailable(org.apache.hadoop.hbase.master.TestMinimumServerCount.TABLE_NAME));\n}",
            "ClassName": "TestMinimumServerCount",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testMinimumServerCount",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 2,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 4
        },
        "CUT_2": {
            "Body": "{\n    super.setUp();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.client.TestGetRowVersions.TABLE_NAME);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.client.TestGetRowVersions.CONTENTS));\n    this.admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    this.admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.client.TestGetRowVersions.TABLE_NAME);\n}",
            "ClassName": "TestGetRowVersions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(this.configuration);\n    return admin.getClusterStatus().getServers();\n}",
            "ClassName": "HTable",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getCurrentNrHRS",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration());\n    admin.split(t.getTableName());\n    java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> regions = waitOnSplit(t);\n    org.junit.Assert.assertTrue(regions.size() > 1);\n    return regions;\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "splitTable",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.HBaseConfiguration.create();\n    org.apache.hadoop.hbase.LocalHBaseCluster cluster = new org.apache.hadoop.hbase.LocalHBaseCluster(conf);\n    cluster.startup();\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.util.Bytes.toBytes(cluster.getClass().getName()));\n    admin.createTable(htd);\n    cluster.shutdown();\n}",
            "ClassName": "LocalHBaseCluster",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "main",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestMinimumServerCount",
        "Commit": "b2d7b19bc324c8d234bbcad7ef6bf1683182fbbb",
        "CyclomaticComplexity": 0,
        "Date": "Fri, 21 May 2010 05:54:17 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testMinimumServerCount",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 2,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 16,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 4,
        "ProjectName": "hbase",
        "Timestamp": "1274421257"
    },
    {
        "Body": "{\n    try {\n        this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestWideScanner.REGION_INFO.getTableDesc(), null, null);\n        int inserted = addWideContent(this.r, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        scan.setBatch(org.apache.hadoop.hbase.regionserver.TestWideScanner.BATCH);\n        org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n        int total = 0;\n        int i = 0;\n        boolean more;\n        do {\n            more = s.next(results);\n            i++;\n            LOG.info(((\"iteration #\" + i) + \", results.size=\") + results.size());\n            junit.framework.Assert.assertTrue(results.size() <= org.apache.hadoop.hbase.regionserver.TestWideScanner.BATCH);\n            total += results.size();\n            if (results.size() > 0) {\n                byte[] row = results.get(0).getRow();\n                for (org.apache.hadoop.hbase.KeyValue kv : results) {\n                    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(row, kv.getRow()));\n                }\n            }\n            results.clear();\n        } while (more );\n        LOG.info(((\"inserted \" + inserted) + \", scanned \") + total);\n        junit.framework.Assert.assertTrue(total == inserted);\n        s.close();\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(this.cluster);\n    }\n}",
        "CUT_1": {
            "Body": "{\n    try {\n        this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestWideScanner.REGION_INFO.getTableDesc(), null, null);\n        int inserted = addWideContent(this.r, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        scan.setBatch(org.apache.hadoop.hbase.regionserver.TestWideScanner.BATCH);\n        org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n        int total = 0;\n        int i = 0;\n        boolean more;\n        do {\n            more = s.next(results);\n            i++;\n            LOG.info(((\"iteration #\" + i) + \", results.size=\") + results.size());\n            junit.framework.Assert.assertTrue(results.size() <= org.apache.hadoop.hbase.regionserver.TestWideScanner.BATCH);\n            total += results.size();\n            if (results.size() > 0) {\n                byte[] row = results.get(0).getRow();\n                for (org.apache.hadoop.hbase.KeyValue kv : results) {\n                    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(row, kv.getRow()));\n                }\n            }\n            results.clear();\n        } while (more );\n        LOG.info(((\"inserted \" + inserted) + \", scanned \") + total);\n        junit.framework.Assert.assertTrue(total == inserted);\n        s.close();\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(this.cluster);\n    }\n}",
            "ClassName": "TestWideScanner",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testWideScanBatching",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 35,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    byte[] startrow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"bbb\");\n    byte[] stoprow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"ccc\");\n    try {\n        this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n        org.apache.hadoop.hbase.HBaseTestCase.addContent(this.r, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"abc\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"abd\"));\n        scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n        int count = 0;\n        while (s.next(results)) {\n            count++;\n        } \n        s.close();\n        junit.framework.Assert.assertEquals(0, count);\n        scan = new org.apache.hadoop.hbase.client.Scan(startrow, stoprow);\n        scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        s = r.getScanner(scan);\n        count = 0;\n        org.apache.hadoop.hbase.KeyValue kv = null;\n        results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        for (boolean first = true; s.next(results);) {\n            kv = results.get(0);\n            if (first) {\n                junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.BYTES_COMPARATOR.compare(startrow, kv.getRow()) == 0);\n                first = false;\n            }\n            count++;\n        }\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.BYTES_COMPARATOR.compare(stoprow, kv.getRow()) > 0);\n        junit.framework.Assert.assertTrue(count > 10);\n        s.close();\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(this.cluster);\n    }\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testStopRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 39,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    try {\n        this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n        org.apache.hadoop.hbase.HBaseTestCase.addContent(this.r, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        try {\n            s.next(results);\n            s.close();\n            s.next(results);\n            junit.framework.Assert.fail(\"We don't want anything more, we should be failing\");\n        } catch (org.apache.hadoop.hbase.UnknownScannerException ex) {\n            return;\n        }\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(this.cluster);\n    }\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testRaceBetweenClientAndTimeout",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n    try {\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        boolean hasNext = true;\n        do {\n            hasNext = s.next(results);\n            org.apache.hadoop.hbase.HRegionInfo info = null;\n            for (org.apache.hadoop.hbase.KeyValue kv : results) {\n                info = org.apache.hadoop.hbase.util.Writables.getHRegionInfoOrNull(kv.getValue());\n                if (info == null) {\n                    org.apache.hadoop.hbase.util.MetaUtils.LOG.warn(((\"Region info is null for row \" + org.apache.hadoop.hbase.util.Bytes.toString(kv.getRow())) + \" in table \") + r.getTableDesc().getNameAsString());\n                }\n                continue;\n            }\n            if (!listener.processRow(info)) {\n                break;\n            }\n            results.clear();\n        } while (hasNext );\n    } finally {\n        s.close();\n    }\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "scanMetaRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 26,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ org.apache.hadoop.hbase.KeyValueTestUtil.create(\"R1\", \"cf\", \"a\", 2, org.apache.hadoop.hbase.KeyValue.Type.Put, \"dont-care\"), org.apache.hadoop.hbase.KeyValueTestUtil.create(\"R1\", \"cf\", \"a\", 1, org.apache.hadoop.hbase.KeyValue.Type.Put, \"dont-care\"), org.apache.hadoop.hbase.KeyValueTestUtil.create(\"R2\", \"cf\", \"a\", 1, org.apache.hadoop.hbase.KeyValue.Type.Put, \"dont-care\") };\n    java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner> scanners = org.apache.hadoop.hbase.regionserver.KeyValueScanFixture.scanFixture(kvs);\n    org.apache.hadoop.hbase.client.Scan scanSpec = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"R1\"));\n    org.apache.hadoop.hbase.regionserver.StoreScanner scan = new org.apache.hadoop.hbase.regionserver.StoreScanner(scanSpec, CF, java.lang.Long.MAX_VALUE, org.apache.hadoop.hbase.KeyValue.COMPARATOR, getCols(\"a\"), scanners);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    scan.next(results);\n    junit.framework.Assert.assertEquals(1, results.size());\n    junit.framework.Assert.assertEquals(kvs[0], results.get(0));\n    results.clear();\n    scan.next(results);\n    junit.framework.Assert.assertEquals(1, results.size());\n    junit.framework.Assert.assertEquals(kvs[2], results.get(0));\n    results.clear();\n    scan.next(results);\n    junit.framework.Assert.assertEquals(0, results.size());\n}",
            "ClassName": "TestStoreScanner",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testWontNextToNext",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 17,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestWideScanner",
        "Commit": "b2d7b19bc324c8d234bbcad7ef6bf1683182fbbb",
        "CyclomaticComplexity": 3,
        "Date": "Fri, 21 May 2010 05:54:17 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testWideScanBatching",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 35,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274421257"
    },
    {
        "Body": "{\n    try {\n        r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.TESTTABLEDESC, null, null);\n        region = new org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon(r);\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestScanner.ROW_KEY, java.lang.System.currentTimeMillis(), null);\n        java.io.ByteArrayOutputStream byteStream = new java.io.ByteArrayOutputStream();\n        java.io.DataOutputStream s = new java.io.DataOutputStream(byteStream);\n        org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.write(s);\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, byteStream.toByteArray());\n        region.put(put);\n        scan(false, null);\n        getRegionInfo();\n        r.close();\n        r = openClosedRegion(r);\n        region = new org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon(r);\n        scan(false, null);\n        getRegionInfo();\n        org.apache.hadoop.hbase.HServerAddress address = new org.apache.hadoop.hbase.HServerAddress(\"foo.bar.com:1234\");\n        put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestScanner.ROW_KEY, java.lang.System.currentTimeMillis(), null);\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER, org.apache.hadoop.hbase.util.Bytes.toBytes(address.toString()));\n        region.put(put);\n        scan(true, address.toString());\n        getRegionInfo();\n        region.flushcache();\n        scan(true, address.toString());\n        getRegionInfo();\n        r.close();\n        r = openClosedRegion(r);\n        region = new org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon(r);\n        scan(true, address.toString());\n        getRegionInfo();\n        address = new org.apache.hadoop.hbase.HServerAddress(\"bar.foo.com:4321\");\n        put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestScanner.ROW_KEY, java.lang.System.currentTimeMillis(), null);\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER, org.apache.hadoop.hbase.util.Bytes.toBytes(address.toString()));\n        region.put(put);\n        scan(true, address.toString());\n        getRegionInfo();\n        region.flushcache();\n        scan(true, address.toString());\n        getRegionInfo();\n        r.close();\n        r = openClosedRegion(r);\n        region = new org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon(r);\n        scan(true, address.toString());\n        getRegionInfo();\n        r.close();\n        r.getLog().closeAndDelete();\n    } finally {\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(cluster);\n    }\n}",
        "CUT_1": {
            "Body": "{\n    try {\n        r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.TESTTABLEDESC, null, null);\n        region = new org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon(r);\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestScanner.ROW_KEY, java.lang.System.currentTimeMillis(), null);\n        java.io.ByteArrayOutputStream byteStream = new java.io.ByteArrayOutputStream();\n        java.io.DataOutputStream s = new java.io.DataOutputStream(byteStream);\n        org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.write(s);\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, byteStream.toByteArray());\n        region.put(put);\n        scan(false, null);\n        getRegionInfo();\n        r.close();\n        r = openClosedRegion(r);\n        region = new org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon(r);\n        scan(false, null);\n        getRegionInfo();\n        org.apache.hadoop.hbase.HServerAddress address = new org.apache.hadoop.hbase.HServerAddress(\"foo.bar.com:1234\");\n        put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestScanner.ROW_KEY, java.lang.System.currentTimeMillis(), null);\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER, org.apache.hadoop.hbase.util.Bytes.toBytes(address.toString()));\n        region.put(put);\n        scan(true, address.toString());\n        getRegionInfo();\n        region.flushcache();\n        scan(true, address.toString());\n        getRegionInfo();\n        r.close();\n        r = openClosedRegion(r);\n        region = new org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon(r);\n        scan(true, address.toString());\n        getRegionInfo();\n        address = new org.apache.hadoop.hbase.HServerAddress(\"bar.foo.com:4321\");\n        put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.regionserver.TestScanner.ROW_KEY, java.lang.System.currentTimeMillis(), null);\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER, org.apache.hadoop.hbase.util.Bytes.toBytes(address.toString()));\n        region.put(put);\n        scan(true, address.toString());\n        getRegionInfo();\n        region.flushcache();\n        scan(true, address.toString());\n        getRegionInfo();\n        r.close();\n        r = openClosedRegion(r);\n        region = new org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon(r);\n        scan(true, address.toString());\n        getRegionInfo();\n        r.close();\n        r.getLog().closeAndDelete();\n    } finally {\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(cluster);\n    }\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 51,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.HRegion region = createNewHRegion(desc, startKey, endKey);\n    java.lang.System.out.println(\"created region \" + org.apache.hadoop.hbase.util.Bytes.toString(region.getRegionName()));\n    org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon r = new org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon(region);\n    for (int i = firstRow; i < (firstRow + nrows); i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row_\" + java.lang.String.format(\"%1$05d\", i)));\n        put.add(org.apache.hadoop.hbase.AbstractMergeTestBase.COLUMN_NAME, null, value.get());\n        region.put(put);\n        if ((i % 10000) == 0) {\n            java.lang.System.out.println(\"Flushing write #\" + i);\n            r.flushcache();\n        }\n    }\n    region.close();\n    region.getLog().closeAndDelete();\n    region.getRegionInfo().setOffline(true);\n    return region;\n}",
            "ClassName": "AbstractMergeTestBase",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createAregion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 18,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.HRegion region = org.apache.hadoop.hbase.regionserver.HRegion.createHRegion(newRegion, this.master.getRootDir(), master.getConfiguration());\n    org.apache.hadoop.hbase.HRegionInfo info = region.getRegionInfo();\n    byte[] regionName = region.getRegionName();\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(regionName);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(info));\n    server.put(metaRegionName, put);\n    region.close();\n    region.getLog().closeAndDelete();\n    setUnassigned(info, true);\n}",
            "ClassName": "RegionManager",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 11,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(i.getRegionName());\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(i));\n    server.put(regionName, put);\n    org.apache.hadoop.hbase.master.ModifyTableMeta.LOG.debug(\"updated HTableDescriptor for region \" + i.getRegionNameAsString());\n}",
            "ClassName": "ModifyTableMeta",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "updateRegionInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    i.setOffline(!online);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(i.getRegionName());\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(i));\n    return put;\n}",
            "ClassName": "ChangeTableState",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "updateRegionInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestScanner",
        "Commit": "2d6804b03ee4b4ed6ddef4ebd9e2474d1c1e530a",
        "CyclomaticComplexity": 0,
        "Date": "Fri, 21 May 2010 17:31:37 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testScanner",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 51,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274463097"
    },
    {
        "Body": "{\n    org.apache.hadoop.fs.Path storedir = new org.apache.hadoop.fs.Path(new org.apache.hadoop.fs.Path(this.testDir, \"regionname\"), \"familyname\");\n    org.apache.hadoop.fs.Path dir = new org.apache.hadoop.fs.Path(storedir, \"1234567890\");\n    org.apache.hadoop.hbase.io.hfile.HFile.Writer writer = org.apache.hadoop.hbase.regionserver.StoreFile.createWriter(this.fs, dir, 8 * 1024);\n    writeStoreFile(writer);\n    org.apache.hadoop.hbase.regionserver.StoreFile hsf = new org.apache.hadoop.hbase.regionserver.StoreFile(this.fs, writer.getPath(), true, conf, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE, false);\n    org.apache.hadoop.hbase.io.hfile.HFile.Reader reader = hsf.createReader();\n    org.apache.hadoop.hbase.KeyValue kv = org.apache.hadoop.hbase.KeyValue.createKeyValueFromKey(reader.midkey());\n    byte[] midRow = kv.getRow();\n    kv = org.apache.hadoop.hbase.KeyValue.createKeyValueFromKey(reader.getLastKey());\n    byte[] finalRow = kv.getRow();\n    org.apache.hadoop.fs.Path refPath = org.apache.hadoop.hbase.regionserver.StoreFile.split(fs, dir, hsf, midRow, org.apache.hadoop.hbase.io.Reference.Range.top);\n    org.apache.hadoop.hbase.regionserver.StoreFile refHsf = new org.apache.hadoop.hbase.regionserver.StoreFile(this.fs, refPath, true, conf, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE, false);\n    org.apache.hadoop.hbase.io.hfile.HFileScanner s = refHsf.createReader().getScanner(false, false);\n    for (boolean first = true; ((!s.isSeeked()) && s.seekTo()) || s.next();) {\n        java.nio.ByteBuffer bb = s.getKey();\n        kv = org.apache.hadoop.hbase.KeyValue.createKeyValueFromKey(bb);\n        if (first) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(kv.getRow(), midRow));\n            first = false;\n        }\n    }\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(kv.getRow(), finalRow));\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.fs.Path storedir = new org.apache.hadoop.fs.Path(new org.apache.hadoop.fs.Path(this.testDir, \"regionname\"), \"familyname\");\n    org.apache.hadoop.fs.Path dir = new org.apache.hadoop.fs.Path(storedir, \"1234567890\");\n    org.apache.hadoop.hbase.io.hfile.HFile.Writer writer = org.apache.hadoop.hbase.regionserver.StoreFile.createWriter(this.fs, dir, 8 * 1024);\n    writeStoreFile(writer);\n    org.apache.hadoop.hbase.regionserver.StoreFile hsf = new org.apache.hadoop.hbase.regionserver.StoreFile(this.fs, writer.getPath(), true, conf, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE, false);\n    org.apache.hadoop.hbase.io.hfile.HFile.Reader reader = hsf.createReader();\n    org.apache.hadoop.hbase.KeyValue kv = org.apache.hadoop.hbase.KeyValue.createKeyValueFromKey(reader.midkey());\n    byte[] midRow = kv.getRow();\n    kv = org.apache.hadoop.hbase.KeyValue.createKeyValueFromKey(reader.getLastKey());\n    byte[] finalRow = kv.getRow();\n    org.apache.hadoop.fs.Path refPath = org.apache.hadoop.hbase.regionserver.StoreFile.split(fs, dir, hsf, midRow, org.apache.hadoop.hbase.io.Reference.Range.top);\n    org.apache.hadoop.hbase.regionserver.StoreFile refHsf = new org.apache.hadoop.hbase.regionserver.StoreFile(this.fs, refPath, true, conf, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE, false);\n    org.apache.hadoop.hbase.io.hfile.HFileScanner s = refHsf.createReader().getScanner(false, false);\n    for (boolean first = true; ((!s.isSeeked()) && s.seekTo()) || s.next();) {\n        java.nio.ByteBuffer bb = s.getKey();\n        kv = org.apache.hadoop.hbase.KeyValue.createKeyValueFromKey(bb);\n        if (first) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(kv.getRow(), midRow));\n            first = false;\n        }\n    }\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(kv.getRow(), finalRow));\n}",
            "ClassName": "TestStoreFile",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testReference",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 24,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.io.hfile.HFile.Writer writer = org.apache.hadoop.hbase.regionserver.StoreFile.createWriter(this.fs, new org.apache.hadoop.fs.Path(new org.apache.hadoop.fs.Path(this.testDir, \"regionname\"), \"familyname\"), 2 * 1024);\n    writeStoreFile(writer);\n    checkHalfHFile(new org.apache.hadoop.hbase.regionserver.StoreFile(this.fs, writer.getPath(), true, conf, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE, false));\n}",
            "ClassName": "TestStoreFile",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testBasicHalfMapFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    if (!fs.exists(dir)) {\n        fs.mkdirs(dir);\n    }\n    org.apache.hadoop.fs.Path path = org.apache.hadoop.hbase.regionserver.StoreFile.getUniqueFile(fs, dir);\n    if ((conf == null) || (!conf.getBoolean(\"io.hfile.bloom.enabled\", true))) {\n        bloomType = org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE;\n    }\n    return new org.apache.hadoop.hbase.regionserver.StoreFile.Writer(fs, path, blocksize, algorithm == null ? org.apache.hadoop.hbase.io.hfile.HFile.DEFAULT_COMPRESSION_ALGORITHM : algorithm, conf, c == null ? org.apache.hadoop.hbase.KeyValue.COMPARATOR : c, bloomType, maxKeySize);\n}",
            "ClassName": "StoreFile",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createWriter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 10,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.fs.FileSystem fs = org.apache.hadoop.fs.FileSystem.getLocal(conf);\n    conf.setFloat(\"io.hfile.bloom.error.rate\", ((float) (0.01)));\n    conf.setBoolean(\"io.hfile.bloom.enabled\", true);\n    org.apache.hadoop.fs.Path f = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.TestStoreFile.ROOT_DIR, getName());\n    org.apache.hadoop.hbase.regionserver.StoreFile.Writer writer = new org.apache.hadoop.hbase.regionserver.StoreFile.Writer(fs, f, org.apache.hadoop.hbase.regionserver.StoreFile.DEFAULT_BLOCKSIZE_SMALL, org.apache.hadoop.hbase.io.hfile.HFile.DEFAULT_COMPRESSION_ALGORITHM, conf, org.apache.hadoop.hbase.KeyValue.COMPARATOR, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.ROW, 2000);\n    long now = java.lang.System.currentTimeMillis();\n    for (int i = 0; i < 2000; i += 2) {\n        java.lang.String row = java.lang.String.format(org.apache.hadoop.hbase.regionserver.TestStoreFile.localFormatter, java.lang.Integer.valueOf(i));\n        org.apache.hadoop.hbase.KeyValue kv = new org.apache.hadoop.hbase.KeyValue(row.getBytes(), \"family\".getBytes(), \"col\".getBytes(), now, \"value\".getBytes());\n        writer.append(kv);\n    }\n    writer.close();\n    org.apache.hadoop.hbase.regionserver.StoreFile.Reader reader = new org.apache.hadoop.hbase.regionserver.StoreFile.Reader(fs, f, null, false);\n    reader.loadFileInfo();\n    reader.loadBloomfilter();\n    org.apache.hadoop.hbase.io.hfile.HFileScanner scanner = reader.getScanner(false, false);\n    int falsePos = 0;\n    int falseNeg = 0;\n    for (int i = 0; i < 2000; i++) {\n        java.lang.String row = java.lang.String.format(org.apache.hadoop.hbase.regionserver.TestStoreFile.localFormatter, java.lang.Integer.valueOf(i));\n        java.util.TreeSet<byte[]> columns = new java.util.TreeSet<byte[]>();\n        columns.add(\"family:col\".getBytes());\n        boolean exists = scanner.shouldSeek(row.getBytes(), columns);\n        if ((i % 2) == 0) {\n            if (!exists)\n                falseNeg++;\n\n        } else {\n            if (exists)\n                falsePos++;\n\n        }\n    }\n    reader.close();\n    fs.delete(f, true);\n    java.lang.System.out.println(\"False negatives: \" + falseNeg);\n    junit.framework.Assert.assertEquals(0, falseNeg);\n    java.lang.System.out.println(\"False positives: \" + falsePos);\n    junit.framework.Assert.assertTrue(falsePos < 2);\n}",
            "ClassName": "TestStoreFile",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testBloomFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 41,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    byte[] midkey = f.createReader().midkey();\n    org.apache.hadoop.hbase.KeyValue midKV = org.apache.hadoop.hbase.KeyValue.createKeyValueFromKey(midkey);\n    byte[] midRow = midKV.getRow();\n    org.apache.hadoop.fs.Path topDir = org.apache.hadoop.hbase.regionserver.Store.getStoreHomedir(this.testDir, 1, org.apache.hadoop.hbase.util.Bytes.toBytes(f.getPath().getParent().getName()));\n    if (this.fs.exists(topDir)) {\n        this.fs.delete(topDir, true);\n    }\n    org.apache.hadoop.fs.Path topPath = org.apache.hadoop.hbase.regionserver.StoreFile.split(this.fs, topDir, f, midRow, org.apache.hadoop.hbase.io.Reference.Range.top);\n    org.apache.hadoop.fs.Path bottomDir = org.apache.hadoop.hbase.regionserver.Store.getStoreHomedir(this.testDir, 2, org.apache.hadoop.hbase.util.Bytes.toBytes(f.getPath().getParent().getName()));\n    if (this.fs.exists(bottomDir)) {\n        this.fs.delete(bottomDir, true);\n    }\n    org.apache.hadoop.fs.Path bottomPath = org.apache.hadoop.hbase.regionserver.StoreFile.split(this.fs, bottomDir, f, midRow, org.apache.hadoop.hbase.io.Reference.Range.bottom);\n    org.apache.hadoop.hbase.io.hfile.HFile.Reader top = new org.apache.hadoop.hbase.regionserver.StoreFile(this.fs, topPath, true, conf, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE, false).createReader();\n    org.apache.hadoop.hbase.io.hfile.HFile.Reader bottom = new org.apache.hadoop.hbase.regionserver.StoreFile(this.fs, bottomPath, true, conf, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE, false).createReader();\n    java.nio.ByteBuffer previous = null;\n    org.apache.hadoop.hbase.regionserver.TestStoreFile.LOG.info(\"Midkey: \" + midKV.toString());\n    java.nio.ByteBuffer bbMidkeyBytes = java.nio.ByteBuffer.wrap(midkey);\n    try {\n        boolean first = true;\n        java.nio.ByteBuffer key = null;\n        org.apache.hadoop.hbase.io.hfile.HFileScanner topScanner = top.getScanner(false, false);\n        while (((!topScanner.isSeeked()) && topScanner.seekTo()) || (topScanner.isSeeked() && topScanner.next())) {\n            key = topScanner.getKey();\n            junit.framework.Assert.assertTrue(topScanner.getReader().getComparator().compare(key.array(), key.arrayOffset(), key.limit(), midkey, 0, midkey.length) >= 0);\n            if (first) {\n                first = false;\n                org.apache.hadoop.hbase.regionserver.TestStoreFile.LOG.info(\"First in top: \" + org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.util.Bytes.toBytes(key)));\n            }\n        } \n        org.apache.hadoop.hbase.regionserver.TestStoreFile.LOG.info(\"Last in top: \" + org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.util.Bytes.toBytes(key)));\n        first = true;\n        org.apache.hadoop.hbase.io.hfile.HFileScanner bottomScanner = bottom.getScanner(false, false);\n        while (((!bottomScanner.isSeeked()) && bottomScanner.seekTo()) || bottomScanner.next()) {\n            previous = bottomScanner.getKey();\n            key = bottomScanner.getKey();\n            if (first) {\n                first = false;\n                org.apache.hadoop.hbase.regionserver.TestStoreFile.LOG.info(\"First in bottom: \" + org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.util.Bytes.toBytes(previous)));\n            }\n            junit.framework.Assert.assertTrue(key.compareTo(bbMidkeyBytes) < 0);\n        } \n        if (previous != null) {\n            org.apache.hadoop.hbase.regionserver.TestStoreFile.LOG.info(\"Last in bottom: \" + org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.util.Bytes.toBytes(previous)));\n        }\n        this.fs.delete(topPath, false);\n        this.fs.delete(bottomPath, false);\n        byte[] badmidkey = org.apache.hadoop.hbase.util.Bytes.toBytes(\"  .\");\n        topPath = org.apache.hadoop.hbase.regionserver.StoreFile.split(this.fs, topDir, f, badmidkey, org.apache.hadoop.hbase.io.Reference.Range.top);\n        bottomPath = org.apache.hadoop.hbase.regionserver.StoreFile.split(this.fs, bottomDir, f, badmidkey, org.apache.hadoop.hbase.io.Reference.Range.bottom);\n        top = new org.apache.hadoop.hbase.regionserver.StoreFile(this.fs, topPath, true, conf, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE, false).createReader();\n        bottom = new org.apache.hadoop.hbase.regionserver.StoreFile(this.fs, bottomPath, true, conf, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE, false).createReader();\n        bottomScanner = bottom.getScanner(false, false);\n        int count = 0;\n        while (((!bottomScanner.isSeeked()) && bottomScanner.seekTo()) || bottomScanner.next()) {\n            count++;\n        } \n        junit.framework.Assert.assertTrue(count == 0);\n        first = true;\n        topScanner = top.getScanner(false, false);\n        while (((!topScanner.isSeeked()) && topScanner.seekTo()) || topScanner.next()) {\n            key = topScanner.getKey();\n            junit.framework.Assert.assertTrue(topScanner.getReader().getComparator().compare(key.array(), key.arrayOffset(), key.limit(), badmidkey, 0, badmidkey.length) >= 0);\n            if (first) {\n                first = false;\n                first = false;\n                org.apache.hadoop.hbase.KeyValue keyKV = org.apache.hadoop.hbase.KeyValue.createKeyValueFromKey(key);\n                org.apache.hadoop.hbase.regionserver.TestStoreFile.LOG.info(\"First top when key < bottom: \" + keyKV);\n                java.lang.String tmp = org.apache.hadoop.hbase.util.Bytes.toString(keyKV.getRow());\n                for (int i = 0; i < tmp.length(); i++) {\n                    junit.framework.Assert.assertTrue(tmp.charAt(i) == 'a');\n                }\n            }\n        } \n        org.apache.hadoop.hbase.KeyValue keyKV = org.apache.hadoop.hbase.KeyValue.createKeyValueFromKey(key);\n        org.apache.hadoop.hbase.regionserver.TestStoreFile.LOG.info(\"Last top when key < bottom: \" + keyKV);\n        java.lang.String tmp = org.apache.hadoop.hbase.util.Bytes.toString(keyKV.getRow());\n        for (int i = 0; i < tmp.length(); i++) {\n            junit.framework.Assert.assertTrue(tmp.charAt(i) == 'z');\n        }\n        this.fs.delete(topPath, false);\n        this.fs.delete(bottomPath, false);\n        badmidkey = org.apache.hadoop.hbase.util.Bytes.toBytes(\"|||\");\n        topPath = org.apache.hadoop.hbase.regionserver.StoreFile.split(this.fs, topDir, f, badmidkey, org.apache.hadoop.hbase.io.Reference.Range.top);\n        bottomPath = org.apache.hadoop.hbase.regionserver.StoreFile.split(this.fs, bottomDir, f, badmidkey, org.apache.hadoop.hbase.io.Reference.Range.bottom);\n        top = new org.apache.hadoop.hbase.regionserver.StoreFile(this.fs, topPath, true, conf, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE, false).createReader();\n        bottom = new org.apache.hadoop.hbase.regionserver.StoreFile(this.fs, bottomPath, true, conf, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE, false).createReader();\n        first = true;\n        bottomScanner = bottom.getScanner(false, false);\n        while (((!bottomScanner.isSeeked()) && bottomScanner.seekTo()) || bottomScanner.next()) {\n            key = bottomScanner.getKey();\n            if (first) {\n                first = false;\n                keyKV = org.apache.hadoop.hbase.KeyValue.createKeyValueFromKey(key);\n                org.apache.hadoop.hbase.regionserver.TestStoreFile.LOG.info(\"First bottom when key > top: \" + keyKV);\n                tmp = org.apache.hadoop.hbase.util.Bytes.toString(keyKV.getRow());\n                for (int i = 0; i < tmp.length(); i++) {\n                    junit.framework.Assert.assertTrue(tmp.charAt(i) == 'a');\n                }\n            }\n        } \n        keyKV = org.apache.hadoop.hbase.KeyValue.createKeyValueFromKey(key);\n        org.apache.hadoop.hbase.regionserver.TestStoreFile.LOG.info(\"Last bottom when key > top: \" + keyKV);\n        for (int i = 0; i < tmp.length(); i++) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.toString(keyKV.getRow()).charAt(i) == 'z');\n        }\n        count = 0;\n        topScanner = top.getScanner(false, false);\n        while (((!topScanner.isSeeked()) && topScanner.seekTo()) || (topScanner.isSeeked() && topScanner.next())) {\n            count++;\n        } \n        junit.framework.Assert.assertTrue(count == 0);\n    } finally {\n        if (top != null) {\n            top.close();\n        }\n        if (bottom != null) {\n            bottom.close();\n        }\n        fs.delete(f.getPath(), true);\n    }\n}",
            "ClassName": "TestStoreFile",
            "CyclomaticComplexity": 19,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "checkHalfHFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 123,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestStoreFile",
        "Commit": "2d6804b03ee4b4ed6ddef4ebd9e2474d1c1e530a",
        "CyclomaticComplexity": 2,
        "Date": "Fri, 21 May 2010 17:31:37 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testReference",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 24,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274463097"
    },
    {
        "Body": "{\n    long expectedRows = this.numRows;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    verifyScan(s, expectedRows, expectedKeys);\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.addFamily(org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0]);\n    verifyScan(s, expectedRows, expectedKeys / 2);\n}",
        "CUT_1": {
            "Body": "{\n    long expectedRows = this.numRows;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    verifyScan(s, expectedRows, expectedKeys);\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.addFamily(org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0]);\n    verifyScan(s, expectedRows, expectedKeys / 2);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testNoFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    long expectedRows = this.numRows;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    verifyScan(s, expectedRows, expectedKeys);\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.addFamily(org.apache.hadoop.hbase.filter.TestFilter.FAMILIES[0]);\n    verifyScan(s, expectedRows, expectedKeys / 2);\n}",
            "ClassName": "TestFilter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testNoFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    long expectedRows = this.numRows / 2;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PrefixFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne\")));\n    verifyScan(s, expectedRows, expectedKeys);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testPrefixFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    long expectedRows = this.numRows / 2;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PrefixFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne\")));\n    verifyScan(s, expectedRows, expectedKeys);\n}",
            "ClassName": "TestFilter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testPrefixFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    long expectedRows = (this.numRows / 2) - 1;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-0\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-3\"));\n    verifyScan(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-0\"));\n    s.setFilter(new org.apache.hadoop.hbase.filter.InclusiveStopFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-3\")));\n    verifyScan(s, expectedRows, expectedKeys);\n    expectedRows = (this.numRows / 2) - 1;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo-0\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo-3\"));\n    verifyScan(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo-0\"));\n    s.setFilter(new org.apache.hadoop.hbase.filter.InclusiveStopFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo-3\")));\n    verifyScan(s, expectedRows, expectedKeys);\n}",
            "ClassName": "TestFilter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testInclusiveStopFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 18,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestScannersWithFilters",
        "Commit": "2d6804b03ee4b4ed6ddef4ebd9e2474d1c1e530a",
        "CyclomaticComplexity": 0,
        "Date": "Fri, 21 May 2010 17:31:37 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testNoFilter",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 9,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274463097"
    },
    {
        "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testWritesWhileScanning\");\n    int testCount = 100;\n    int numRows = 1;\n    int numFamilies = 10;\n    int numQualifiers = 100;\n    int flushInterval = 10;\n    int compactInterval = 10 * flushInterval;\n    byte[][] families = new byte[numFamilies][];\n    for (int i = 0; i < numFamilies; i++) {\n        families[i] = org.apache.hadoop.hbase.util.Bytes.toBytes(\"family\" + i);\n    }\n    byte[][] qualifiers = new byte[numQualifiers][];\n    for (int i = 0; i < numQualifiers; i++) {\n        qualifiers[i] = org.apache.hadoop.hbase.util.Bytes.toBytes(\"qual\" + i);\n    }\n    java.lang.String method = \"testWritesWhileScanning\";\n    initHRegion(tableName, method, families);\n    org.apache.hadoop.hbase.regionserver.TestHRegion.PutThread putThread = new org.apache.hadoop.hbase.regionserver.TestHRegion.PutThread(numRows, families, qualifiers);\n    putThread.start();\n    org.apache.hadoop.hbase.regionserver.TestHRegion.FlushThread flushThread = new org.apache.hadoop.hbase.regionserver.TestHRegion.FlushThread();\n    flushThread.start();\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row0\"));\n    org.apache.hadoop.hbase.client.Result result = null;\n    int expectedCount = numFamilies * numQualifiers;\n    long prevTimestamp = 0L;\n    for (int i = 0; i < testCount; i++) {\n        if ((i != 0) && ((i % compactInterval) == 0)) {\n            region.compactStores(true);\n        }\n        if ((i != 0) && ((i % flushInterval) == 0)) {\n            flushThread.flush();\n        }\n        boolean previousEmpty = (result == null) || result.isEmpty();\n        result = region.get(get, null);\n        if (((!result.isEmpty()) || (!previousEmpty)) || (i > compactInterval)) {\n            junit.framework.Assert.assertEquals(\"i=\" + i, expectedCount, result.size());\n            long timestamp = 0;\n            for (org.apache.hadoop.hbase.KeyValue kv : result.sorted()) {\n                if (org.apache.hadoop.hbase.util.Bytes.equals(kv.getFamily(), families[0]) && org.apache.hadoop.hbase.util.Bytes.equals(kv.getQualifier(), qualifiers[0])) {\n                    timestamp = kv.getTimestamp();\n                }\n            }\n            junit.framework.Assert.assertTrue(timestamp >= prevTimestamp);\n            prevTimestamp = timestamp;\n            byte[] gotValue = null;\n            for (org.apache.hadoop.hbase.KeyValue kv : result.raw()) {\n                byte[] thisValue = kv.getValue();\n                if (gotValue != null) {\n                    org.apache.hadoop.hbase.HBaseTestCase.assertEquals(gotValue, thisValue);\n                }\n                gotValue = thisValue;\n            }\n        }\n    }\n    putThread.done();\n    region.flushcache();\n    putThread.join();\n    putThread.checkNoError();\n    flushThread.done();\n    flushThread.join();\n    flushThread.checkNoError();\n}",
        "CUT_1": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testWritesWhileScanning\");\n    int testCount = 100;\n    int numRows = 1;\n    int numFamilies = 10;\n    int numQualifiers = 100;\n    int flushInterval = 10;\n    int compactInterval = 10 * flushInterval;\n    byte[][] families = new byte[numFamilies][];\n    for (int i = 0; i < numFamilies; i++) {\n        families[i] = org.apache.hadoop.hbase.util.Bytes.toBytes(\"family\" + i);\n    }\n    byte[][] qualifiers = new byte[numQualifiers][];\n    for (int i = 0; i < numQualifiers; i++) {\n        qualifiers[i] = org.apache.hadoop.hbase.util.Bytes.toBytes(\"qual\" + i);\n    }\n    java.lang.String method = \"testWritesWhileScanning\";\n    initHRegion(tableName, method, families);\n    org.apache.hadoop.hbase.regionserver.TestHRegion.PutThread putThread = new org.apache.hadoop.hbase.regionserver.TestHRegion.PutThread(numRows, families, qualifiers);\n    putThread.start();\n    org.apache.hadoop.hbase.regionserver.TestHRegion.FlushThread flushThread = new org.apache.hadoop.hbase.regionserver.TestHRegion.FlushThread();\n    flushThread.start();\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row0\"));\n    org.apache.hadoop.hbase.client.Result result = null;\n    int expectedCount = numFamilies * numQualifiers;\n    long prevTimestamp = 0L;\n    for (int i = 0; i < testCount; i++) {\n        if ((i != 0) && ((i % compactInterval) == 0)) {\n            region.compactStores(true);\n        }\n        if ((i != 0) && ((i % flushInterval) == 0)) {\n            flushThread.flush();\n        }\n        boolean previousEmpty = (result == null) || result.isEmpty();\n        result = region.get(get, null);\n        if (((!result.isEmpty()) || (!previousEmpty)) || (i > compactInterval)) {\n            junit.framework.Assert.assertEquals(\"i=\" + i, expectedCount, result.size());\n            long timestamp = 0;\n            for (org.apache.hadoop.hbase.KeyValue kv : result.sorted()) {\n                if (org.apache.hadoop.hbase.util.Bytes.equals(kv.getFamily(), families[0]) && org.apache.hadoop.hbase.util.Bytes.equals(kv.getQualifier(), qualifiers[0])) {\n                    timestamp = kv.getTimestamp();\n                }\n            }\n            junit.framework.Assert.assertTrue(timestamp >= prevTimestamp);\n            prevTimestamp = timestamp;\n            byte[] gotValue = null;\n            for (org.apache.hadoop.hbase.KeyValue kv : result.raw()) {\n                byte[] thisValue = kv.getValue();\n                if (gotValue != null) {\n                    org.apache.hadoop.hbase.HBaseTestCase.assertEquals(gotValue, thisValue);\n                }\n                gotValue = thisValue;\n            }\n        }\n    }\n    putThread.done();\n    region.flushcache();\n    putThread.join();\n    putThread.checkNoError();\n    flushThread.done();\n    flushThread.join();\n    flushThread.checkNoError();\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 10,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testWritesWhileGetting",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 63,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 4
        },
        "CUT_2": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testWritesWhileScanning\");\n    int testCount = 100;\n    int numRows = 1;\n    int numFamilies = 10;\n    int numQualifiers = 100;\n    int flushInterval = 7;\n    int compactInterval = 5 * flushInterval;\n    byte[][] families = new byte[numFamilies][];\n    for (int i = 0; i < numFamilies; i++) {\n        families[i] = org.apache.hadoop.hbase.util.Bytes.toBytes(\"family\" + i);\n    }\n    byte[][] qualifiers = new byte[numQualifiers][];\n    for (int i = 0; i < numQualifiers; i++) {\n        qualifiers[i] = org.apache.hadoop.hbase.util.Bytes.toBytes(\"qual\" + i);\n    }\n    java.lang.String method = \"testWritesWhileScanning\";\n    initHRegion(tableName, method, families);\n    org.apache.hadoop.hbase.regionserver.TestHRegion.PutThread putThread = new org.apache.hadoop.hbase.regionserver.TestHRegion.PutThread(numRows, families, qualifiers);\n    putThread.start();\n    org.apache.hadoop.hbase.regionserver.TestHRegion.FlushThread flushThread = new org.apache.hadoop.hbase.regionserver.TestHRegion.FlushThread();\n    flushThread.start();\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row0\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\"));\n    int expectedCount = numFamilies * numQualifiers;\n    java.util.List<org.apache.hadoop.hbase.KeyValue> res = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    long prevTimestamp = 0L;\n    for (int i = 0; i < testCount; i++) {\n        if ((i != 0) && ((i % compactInterval) == 0)) {\n            region.compactStores(true);\n        }\n        if ((i != 0) && ((i % flushInterval) == 0)) {\n            flushThread.flush();\n        }\n        boolean previousEmpty = res.isEmpty();\n        res.clear();\n        org.apache.hadoop.hbase.regionserver.InternalScanner scanner = region.getScanner(scan);\n        while (scanner.next(res));\n        if (((!res.isEmpty()) || (!previousEmpty)) || (i > compactInterval)) {\n            junit.framework.Assert.assertEquals(\"i=\" + i, expectedCount, res.size());\n            long timestamp = res.get(0).getTimestamp();\n            junit.framework.Assert.assertTrue(((\"Timestamps were broke: \" + timestamp) + \" prev: \") + prevTimestamp, timestamp >= prevTimestamp);\n            prevTimestamp = timestamp;\n        }\n    }\n    putThread.done();\n    region.flushcache();\n    putThread.join();\n    putThread.checkNoError();\n    flushThread.done();\n    flushThread.join();\n    flushThread.checkNoError();\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 7,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testWritesWhileScanning",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 52,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 4
        },
        "CUT_3": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testFlushCacheWhileScanning\");\n    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"family\");\n    int numRows = 1000;\n    int flushAndScanInterval = 10;\n    int compactInterval = 10 * flushAndScanInterval;\n    java.lang.String method = \"testFlushCacheWhileScanning\";\n    initHRegion(tableName, method, family);\n    org.apache.hadoop.hbase.regionserver.TestHRegion.FlushThread flushThread = new org.apache.hadoop.hbase.regionserver.TestHRegion.FlushThread();\n    flushThread.start();\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(family);\n    scan.setFilter(new org.apache.hadoop.hbase.filter.SingleColumnValueFilter(family, qual1, org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(5L))));\n    int expectedCount = 0;\n    java.util.List<org.apache.hadoop.hbase.KeyValue> res = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    boolean toggle = true;\n    for (long i = 0; i < numRows; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(i));\n        put.add(family, qual1, org.apache.hadoop.hbase.util.Bytes.toBytes(i % 10));\n        region.put(put);\n        if ((i != 0) && ((i % compactInterval) == 0)) {\n            region.compactStores(true);\n        }\n        if ((i % 10) == 5L) {\n            expectedCount++;\n        }\n        if ((i != 0) && ((i % flushAndScanInterval) == 0)) {\n            res.clear();\n            org.apache.hadoop.hbase.regionserver.InternalScanner scanner = region.getScanner(scan);\n            if (toggle) {\n                flushThread.flush();\n            }\n            while (scanner.next(res));\n            if (!toggle) {\n                flushThread.flush();\n            }\n            junit.framework.Assert.assertEquals(\"i=\" + i, expectedCount, res.size());\n            toggle = !toggle;\n        }\n    }\n    flushThread.done();\n    flushThread.join();\n    flushThread.checkNoError();\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 7,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testFlushCacheWhileScanning",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 44,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_4": {
            "Body": "{\n    for (int i = startRow; i < (startRow + numRows); i++) {\n        byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"\" + i);\n        org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n        for (byte[] family : families) {\n            get.addColumn(family, qf);\n        }\n        org.apache.hadoop.hbase.client.Result result = newReg.get(get, null);\n        org.apache.hadoop.hbase.KeyValue[] raw = result.sorted();\n        junit.framework.Assert.assertEquals(families.length, result.size());\n        for (int j = 0; j < families.length; j++) {\n            junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(row, raw[j].getRow()));\n            junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(families[j], raw[j].getFamily()));\n            junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(qf, raw[j].getQualifier()));\n        }\n    }\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "verifyData",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 17,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(familiy, qualifier);\n    org.apache.hadoop.hbase.client.Result result = region.get(get, null);\n    junit.framework.Assert.assertEquals(1, result.size());\n    org.apache.hadoop.hbase.KeyValue kv = result.raw()[0];\n    long r = org.apache.hadoop.hbase.util.Bytes.toLong(kv.getValue());\n    junit.framework.Assert.assertEquals(amount, r);\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "assertICV",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHRegion",
        "Commit": "2d6804b03ee4b4ed6ddef4ebd9e2474d1c1e530a",
        "CyclomaticComplexity": 10,
        "Date": "Fri, 21 May 2010 17:31:37 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testWritesWhileGetting",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 63,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 4,
        "ProjectName": "hbase",
        "Timestamp": "1274463097"
    },
    {
        "Body": "{\n    long expectedRows = 1;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\"testRow.+-2\"));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = 1;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows - 1;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows - 1;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows - 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\".+-2\"));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
        "CUT_1": {
            "Body": "{\n    long expectedRows = 1;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\"testRow.+-2\"));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = 1;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows - 1;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows - 1;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows - 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\".+-2\"));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testRowFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 54,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    long expectedRows = this.numRows / 2;\n    long expectedKeys = 2;\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 2;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 4;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 4;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo\"));\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 4;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo\"));\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 2;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo\"));\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2]));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\"test.+-2\"));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testQualifierFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 48,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.KeyValue[] expectedKVs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    long expectedRows = 6;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, expectedKVs);\n    expectedRows = 4;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, java.util.Arrays.copyOf(expectedKVs, 24));\n    expectedRows = 2;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, java.util.Arrays.copyOf(expectedKVs, 12));\n    expectedRows = 1;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, java.util.Arrays.copyOf(expectedKVs, 6));\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testPageFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 31,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter());\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testFirstKeyOnlyFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.SkipFilter(new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\"))));\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSkipFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestScannersWithFilters",
        "Commit": "2d6804b03ee4b4ed6ddef4ebd9e2474d1c1e530a",
        "CyclomaticComplexity": 0,
        "Date": "Fri, 21 May 2010 17:31:37 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testRowFilter",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 54,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274463097"
    },
    {
        "Body": "{\n    long expectedRows = (this.numRows / 2) - 1;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-0\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-3\"));\n    verifyScan(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-0\"));\n    s.setFilter(new org.apache.hadoop.hbase.filter.InclusiveStopFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-3\")));\n    verifyScan(s, expectedRows, expectedKeys);\n    expectedRows = (this.numRows / 2) - 1;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo-0\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo-3\"));\n    verifyScan(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo-0\"));\n    s.setFilter(new org.apache.hadoop.hbase.filter.InclusiveStopFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo-3\")));\n    verifyScan(s, expectedRows, expectedKeys);\n}",
        "CUT_1": {
            "Body": "{\n    long expectedRows = (this.numRows / 2) - 1;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-0\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-3\"));\n    verifyScan(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-0\"));\n    s.setFilter(new org.apache.hadoop.hbase.filter.InclusiveStopFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-3\")));\n    verifyScan(s, expectedRows, expectedKeys);\n    expectedRows = (this.numRows / 2) - 1;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo-0\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo-3\"));\n    verifyScan(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo-0\"));\n    s.setFilter(new org.apache.hadoop.hbase.filter.InclusiveStopFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo-3\")));\n    verifyScan(s, expectedRows, expectedKeys);\n}",
            "ClassName": "TestFilter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testInclusiveStopFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 18,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    long expectedRows = (this.numRows / 2) - 1;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-0\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-3\"));\n    verifyScan(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-0\"));\n    s.setFilter(new org.apache.hadoop.hbase.filter.InclusiveStopFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-3\")));\n    verifyScan(s, expectedRows, expectedKeys);\n    expectedRows = (this.numRows / 2) - 1;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo-0\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo-3\"));\n    verifyScan(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo-0\"));\n    s.setFilter(new org.apache.hadoop.hbase.filter.InclusiveStopFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo-3\")));\n    verifyScan(s, expectedRows, expectedKeys);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testInclusiveStopFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 18,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    long expectedRows = this.numRows / 2;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PrefixFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne\")));\n    verifyScan(s, expectedRows, expectedKeys);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testPrefixFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    long expectedRows = this.numRows / 2;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PrefixFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne\")));\n    verifyScan(s, expectedRows, expectedKeys);\n}",
            "ClassName": "TestFilter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testPrefixFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    long expectedRows = this.numRows;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    verifyScan(s, expectedRows, expectedKeys);\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.addFamily(org.apache.hadoop.hbase.filter.TestFilter.FAMILIES[0]);\n    verifyScan(s, expectedRows, expectedKeys / 2);\n}",
            "ClassName": "TestFilter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testNoFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestScannersWithFilters",
        "Commit": "2d6804b03ee4b4ed6ddef4ebd9e2474d1c1e530a",
        "CyclomaticComplexity": 0,
        "Date": "Fri, 21 May 2010 17:31:37 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testInclusiveStopFilter",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 18,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274463097"
    },
    {
        "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam1\");\n    byte[] fam2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam2\");\n    byte[] fam3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam3\");\n    byte[] fam4 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam4\");\n    byte[][] families = new byte[][]{ fam1, fam2, fam3, fam4 };\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, families);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(fam1, null, null);\n    put.add(fam2, null, null);\n    put.add(fam3, null, null);\n    put.add(fam4, null, null);\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = null;\n    org.apache.hadoop.hbase.regionserver.HRegion.RegionScanner is = null;\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(fam2);\n    scan.addFamily(fam4);\n    is = ((org.apache.hadoop.hbase.regionserver.HRegion.RegionScanner) (region.getScanner(scan)));\n    is.initHeap();\n    junit.framework.Assert.assertEquals(1, ((org.apache.hadoop.hbase.regionserver.HRegion.RegionScanner) (is)).storeHeap.getHeap().size());\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    is = ((org.apache.hadoop.hbase.regionserver.HRegion.RegionScanner) (region.getScanner(scan)));\n    is.initHeap();\n    junit.framework.Assert.assertEquals(families.length - 1, ((org.apache.hadoop.hbase.regionserver.HRegion.RegionScanner) (is)).storeHeap.getHeap().size());\n}",
        "CUT_1": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam1\");\n    byte[] fam2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam2\");\n    byte[] fam3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam3\");\n    byte[] fam4 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam4\");\n    byte[][] families = new byte[][]{ fam1, fam2, fam3, fam4 };\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, families);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(fam1, null, null);\n    put.add(fam2, null, null);\n    put.add(fam3, null, null);\n    put.add(fam4, null, null);\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = null;\n    org.apache.hadoop.hbase.regionserver.HRegion.RegionScanner is = null;\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(fam2);\n    scan.addFamily(fam4);\n    is = ((org.apache.hadoop.hbase.regionserver.HRegion.RegionScanner) (region.getScanner(scan)));\n    is.initHeap();\n    junit.framework.Assert.assertEquals(1, ((org.apache.hadoop.hbase.regionserver.HRegion.RegionScanner) (is)).storeHeap.getHeap().size());\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    is = ((org.apache.hadoop.hbase.regionserver.HRegion.RegionScanner) (region.getScanner(scan)));\n    is.initHeap();\n    junit.framework.Assert.assertEquals(families.length - 1, ((org.apache.hadoop.hbase.regionserver.HRegion.RegionScanner) (is)).storeHeap.getHeap().size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetScanner_WithNoFamilies",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 29,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    byte[] row2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row2\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam1\");\n    byte[] fam2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam2\");\n    byte[] fam3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam3\");\n    byte[] fam4 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam4\");\n    byte[][] families = new byte[][]{ fam1, fam2, fam3, fam4 };\n    long ts = java.lang.System.currentTimeMillis();\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, families);\n    org.apache.hadoop.hbase.client.Put put = null;\n    put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(fam1, null, ts, null);\n    put.add(fam2, null, ts, null);\n    put.add(fam3, null, ts, null);\n    put.add(fam4, null, ts, null);\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row2);\n    put.add(fam1, null, ts, null);\n    put.add(fam2, null, ts, null);\n    put.add(fam3, null, ts, null);\n    put.add(fam4, null, ts, null);\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(fam2);\n    scan.addFamily(fam4);\n    org.apache.hadoop.hbase.regionserver.InternalScanner is = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> res = null;\n    java.util.List<org.apache.hadoop.hbase.KeyValue> expected1 = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    expected1.add(new org.apache.hadoop.hbase.KeyValue(row1, fam2, null, ts, org.apache.hadoop.hbase.KeyValue.Type.Put, null));\n    expected1.add(new org.apache.hadoop.hbase.KeyValue(row1, fam4, null, ts, org.apache.hadoop.hbase.KeyValue.Type.Put, null));\n    res = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    is.next(res);\n    for (int i = 0; i < res.size(); i++) {\n        junit.framework.Assert.assertEquals(expected1.get(i), res.get(i));\n    }\n    java.util.List<org.apache.hadoop.hbase.KeyValue> expected2 = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    expected2.add(new org.apache.hadoop.hbase.KeyValue(row2, fam2, null, ts, org.apache.hadoop.hbase.KeyValue.Type.Put, null));\n    expected2.add(new org.apache.hadoop.hbase.KeyValue(row2, fam4, null, ts, org.apache.hadoop.hbase.KeyValue.Type.Put, null));\n    res = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    is.next(res);\n    for (int i = 0; i < res.size(); i++) {\n        junit.framework.Assert.assertEquals(expected2.get(i), res.get(i));\n    }\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testRegionScanner_Next",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 47,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam1\");\n    byte[] fam2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam2\");\n    byte[][] families = new byte[][]{ fam1, fam2 };\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, families);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(fam1);\n    scan.addFamily(fam2);\n    try {\n        region.getScanner(scan);\n    } catch (java.lang.Exception e) {\n        junit.framework.Assert.assertTrue(\"Families could not be found in Region\", false);\n    }\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetScanner_WithOkFamilies",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"test_table\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"columnA\");\n    byte[] fam2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"columnB\");\n    initHRegion(tableName, getName(), fam1, fam2);\n    byte[] rowA = org.apache.hadoop.hbase.util.Bytes.toBytes(\"rowA\");\n    byte[] rowB = org.apache.hadoop.hbase.util.Bytes.toBytes(\"rowB\");\n    byte[] value = org.apache.hadoop.hbase.util.Bytes.toBytes(\"value\");\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(rowA);\n    delete.deleteFamily(fam1);\n    region.delete(delete, null, true);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(rowA);\n    put.add(fam2, null, value);\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(rowB);\n    put.add(fam1, null, value);\n    put.add(fam2, null, value);\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(fam1).addFamily(fam2);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    s.next(results);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(rowA, results.get(0).getRow()));\n    results.clear();\n    s.next(results);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(rowB, results.get(0).getRow()));\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_DeleteOneFamilyNotAnother",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 28,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam1\");\n    byte[] fam2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam2\");\n    byte[][] families = new byte[][]{ fam1 };\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, families);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(fam2);\n    boolean ok = false;\n    try {\n        region.getScanner(scan);\n    } catch (java.lang.Exception e) {\n        ok = true;\n    }\n    junit.framework.Assert.assertTrue(\"Families could not be found in Region\", ok);\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetScanner_WithNotOkFamilies",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 17,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHRegion",
        "Commit": "2d6804b03ee4b4ed6ddef4ebd9e2474d1c1e530a",
        "CyclomaticComplexity": 0,
        "Date": "Fri, 21 May 2010 17:31:37 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testGetScanner_WithNoFamilies",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 29,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274463097"
    },
    {
        "Body": "{\n    long expectedRows = this.numRows / 2;\n    long expectedKeys = 2;\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 2;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 4;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 4;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo\"));\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 4;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo\"));\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 2;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo\"));\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2]));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\"test.+-2\"));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
        "CUT_1": {
            "Body": "{\n    long expectedRows = this.numRows / 2;\n    long expectedKeys = 2;\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 2;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 4;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 4;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo\"));\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 4;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo\"));\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 2;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo\"));\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2]));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\"test.+-2\"));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testQualifierFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 48,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    long expectedRows = 1;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\"testRow.+-2\"));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = 1;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows - 1;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows - 1;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows - 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\".+-2\"));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testRowFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 54,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.KeyValue[] expectedKVs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    long expectedRows = 6;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, expectedKVs);\n    expectedRows = 4;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, java.util.Arrays.copyOf(expectedKVs, 24));\n    expectedRows = 2;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, java.util.Arrays.copyOf(expectedKVs, 12));\n    expectedRows = 1;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, java.util.Arrays.copyOf(expectedKVs, 6));\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testPageFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 31,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter());\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testFirstKeyOnlyFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.SkipFilter(new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\"))));\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSkipFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestScannersWithFilters",
        "Commit": "2d6804b03ee4b4ed6ddef4ebd9e2474d1c1e530a",
        "CyclomaticComplexity": 0,
        "Date": "Fri, 21 May 2010 17:31:37 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testQualifierFilter",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 48,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274463097"
    },
    {
        "Body": "{\n    long expectedRows = this.numRows / 2;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PrefixFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne\")));\n    verifyScan(s, expectedRows, expectedKeys);\n}",
        "CUT_1": {
            "Body": "{\n    long expectedRows = this.numRows / 2;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PrefixFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne\")));\n    verifyScan(s, expectedRows, expectedKeys);\n}",
            "ClassName": "TestFilter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testPrefixFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    long expectedRows = this.numRows / 2;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PrefixFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne\")));\n    verifyScan(s, expectedRows, expectedKeys);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testPrefixFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    long expectedRows = (this.numRows / 2) - 1;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-0\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-3\"));\n    verifyScan(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-0\"));\n    s.setFilter(new org.apache.hadoop.hbase.filter.InclusiveStopFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-3\")));\n    verifyScan(s, expectedRows, expectedKeys);\n    expectedRows = (this.numRows / 2) - 1;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo-0\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo-3\"));\n    verifyScan(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo-0\"));\n    s.setFilter(new org.apache.hadoop.hbase.filter.InclusiveStopFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo-3\")));\n    verifyScan(s, expectedRows, expectedKeys);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testInclusiveStopFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 18,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    long expectedRows = (this.numRows / 2) - 1;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-0\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-3\"));\n    verifyScan(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-0\"));\n    s.setFilter(new org.apache.hadoop.hbase.filter.InclusiveStopFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-3\")));\n    verifyScan(s, expectedRows, expectedKeys);\n    expectedRows = (this.numRows / 2) - 1;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo-0\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo-3\"));\n    verifyScan(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo-0\"));\n    s.setFilter(new org.apache.hadoop.hbase.filter.InclusiveStopFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo-3\")));\n    verifyScan(s, expectedRows, expectedKeys);\n}",
            "ClassName": "TestFilter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testInclusiveStopFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 18,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    long expectedRows = this.numRows;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    verifyScan(s, expectedRows, expectedKeys);\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.addFamily(org.apache.hadoop.hbase.filter.TestFilter.FAMILIES[0]);\n    verifyScan(s, expectedRows, expectedKeys / 2);\n}",
            "ClassName": "TestFilter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testNoFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestScannersWithFilters",
        "Commit": "2d6804b03ee4b4ed6ddef4ebd9e2474d1c1e530a",
        "CyclomaticComplexity": 0,
        "Date": "Fri, 21 May 2010 17:31:37 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testPrefixFilter",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 7,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274463097"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.io.hfile.HFile.Writer writer = org.apache.hadoop.hbase.regionserver.StoreFile.createWriter(this.fs, new org.apache.hadoop.fs.Path(new org.apache.hadoop.fs.Path(this.testDir, \"regionname\"), \"familyname\"), 2 * 1024);\n    writeStoreFile(writer);\n    checkHalfHFile(new org.apache.hadoop.hbase.regionserver.StoreFile(this.fs, writer.getPath(), true, conf, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE, false));\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.io.hfile.HFile.Writer writer = org.apache.hadoop.hbase.regionserver.StoreFile.createWriter(this.fs, new org.apache.hadoop.fs.Path(new org.apache.hadoop.fs.Path(this.testDir, \"regionname\"), \"familyname\"), 2 * 1024);\n    writeStoreFile(writer);\n    checkHalfHFile(new org.apache.hadoop.hbase.regionserver.StoreFile(this.fs, writer.getPath(), true, conf, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE, false));\n}",
            "ClassName": "TestStoreFile",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testBasicHalfMapFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.fs.Path storedir = new org.apache.hadoop.fs.Path(new org.apache.hadoop.fs.Path(this.testDir, \"regionname\"), \"familyname\");\n    org.apache.hadoop.fs.Path dir = new org.apache.hadoop.fs.Path(storedir, \"1234567890\");\n    org.apache.hadoop.hbase.io.hfile.HFile.Writer writer = org.apache.hadoop.hbase.regionserver.StoreFile.createWriter(this.fs, dir, 8 * 1024);\n    writeStoreFile(writer);\n    org.apache.hadoop.hbase.regionserver.StoreFile hsf = new org.apache.hadoop.hbase.regionserver.StoreFile(this.fs, writer.getPath(), true, conf, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE, false);\n    org.apache.hadoop.hbase.io.hfile.HFile.Reader reader = hsf.createReader();\n    org.apache.hadoop.hbase.KeyValue kv = org.apache.hadoop.hbase.KeyValue.createKeyValueFromKey(reader.midkey());\n    byte[] midRow = kv.getRow();\n    kv = org.apache.hadoop.hbase.KeyValue.createKeyValueFromKey(reader.getLastKey());\n    byte[] finalRow = kv.getRow();\n    org.apache.hadoop.fs.Path refPath = org.apache.hadoop.hbase.regionserver.StoreFile.split(fs, dir, hsf, midRow, org.apache.hadoop.hbase.io.Reference.Range.top);\n    org.apache.hadoop.hbase.regionserver.StoreFile refHsf = new org.apache.hadoop.hbase.regionserver.StoreFile(this.fs, refPath, true, conf, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE, false);\n    org.apache.hadoop.hbase.io.hfile.HFileScanner s = refHsf.createReader().getScanner(false, false);\n    for (boolean first = true; ((!s.isSeeked()) && s.seekTo()) || s.next();) {\n        java.nio.ByteBuffer bb = s.getKey();\n        kv = org.apache.hadoop.hbase.KeyValue.createKeyValueFromKey(bb);\n        if (first) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(kv.getRow(), midRow));\n            first = false;\n        }\n    }\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(kv.getRow(), finalRow));\n}",
            "ClassName": "TestStoreFile",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testReference",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 24,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    if (!fs.exists(dir)) {\n        fs.mkdirs(dir);\n    }\n    org.apache.hadoop.fs.Path path = org.apache.hadoop.hbase.regionserver.StoreFile.getUniqueFile(fs, dir);\n    if ((conf == null) || (!conf.getBoolean(\"io.hfile.bloom.enabled\", true))) {\n        bloomType = org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE;\n    }\n    return new org.apache.hadoop.hbase.regionserver.StoreFile.Writer(fs, path, blocksize, algorithm == null ? org.apache.hadoop.hbase.io.hfile.HFile.DEFAULT_COMPRESSION_ALGORITHM : algorithm, conf, c == null ? org.apache.hadoop.hbase.KeyValue.COMPARATOR : c, bloomType, maxKeySize);\n}",
            "ClassName": "StoreFile",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createWriter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 10,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.fs.Path ncTFile = new org.apache.hadoop.fs.Path(this.testDir, \"basic.hfile\");\n    org.apache.hadoop.fs.FSDataOutputStream fout = this.fs.create(ncTFile);\n    org.apache.hadoop.hbase.io.hfile.HFile.Writer writer = new org.apache.hadoop.hbase.io.hfile.HFile.Writer(fout, 40, \"none\", null);\n    writer.append(org.apache.hadoop.hbase.util.Bytes.toBytes(\"c\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"value\"));\n    writer.append(org.apache.hadoop.hbase.util.Bytes.toBytes(\"e\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"value\"));\n    writer.append(org.apache.hadoop.hbase.util.Bytes.toBytes(\"g\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"value\"));\n    writer.append(org.apache.hadoop.hbase.util.Bytes.toBytes(\"i\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"value\"));\n    writer.append(org.apache.hadoop.hbase.util.Bytes.toBytes(\"k\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"value\"));\n    writer.close();\n    fout.close();\n    return ncTFile;\n}",
            "ClassName": "TestSeekTo",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "makeNewFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 13,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.fs.FileSystem fs = org.apache.hadoop.fs.FileSystem.getLocal(conf);\n    conf.setFloat(\"io.hfile.bloom.error.rate\", ((float) (0.01)));\n    conf.setBoolean(\"io.hfile.bloom.enabled\", true);\n    org.apache.hadoop.fs.Path f = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.TestStoreFile.ROOT_DIR, getName());\n    org.apache.hadoop.hbase.regionserver.StoreFile.Writer writer = new org.apache.hadoop.hbase.regionserver.StoreFile.Writer(fs, f, org.apache.hadoop.hbase.regionserver.StoreFile.DEFAULT_BLOCKSIZE_SMALL, org.apache.hadoop.hbase.io.hfile.HFile.DEFAULT_COMPRESSION_ALGORITHM, conf, org.apache.hadoop.hbase.KeyValue.COMPARATOR, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.ROW, 2000);\n    long now = java.lang.System.currentTimeMillis();\n    for (int i = 0; i < 2000; i += 2) {\n        java.lang.String row = java.lang.String.format(org.apache.hadoop.hbase.regionserver.TestStoreFile.localFormatter, java.lang.Integer.valueOf(i));\n        org.apache.hadoop.hbase.KeyValue kv = new org.apache.hadoop.hbase.KeyValue(row.getBytes(), \"family\".getBytes(), \"col\".getBytes(), now, \"value\".getBytes());\n        writer.append(kv);\n    }\n    writer.close();\n    org.apache.hadoop.hbase.regionserver.StoreFile.Reader reader = new org.apache.hadoop.hbase.regionserver.StoreFile.Reader(fs, f, null, false);\n    reader.loadFileInfo();\n    reader.loadBloomfilter();\n    org.apache.hadoop.hbase.io.hfile.HFileScanner scanner = reader.getScanner(false, false);\n    int falsePos = 0;\n    int falseNeg = 0;\n    for (int i = 0; i < 2000; i++) {\n        java.lang.String row = java.lang.String.format(org.apache.hadoop.hbase.regionserver.TestStoreFile.localFormatter, java.lang.Integer.valueOf(i));\n        java.util.TreeSet<byte[]> columns = new java.util.TreeSet<byte[]>();\n        columns.add(\"family:col\".getBytes());\n        boolean exists = scanner.shouldSeek(row.getBytes(), columns);\n        if ((i % 2) == 0) {\n            if (!exists)\n                falseNeg++;\n\n        } else {\n            if (exists)\n                falsePos++;\n\n        }\n    }\n    reader.close();\n    fs.delete(f, true);\n    java.lang.System.out.println(\"False negatives: \" + falseNeg);\n    junit.framework.Assert.assertEquals(0, falseNeg);\n    java.lang.System.out.println(\"False positives: \" + falsePos);\n    junit.framework.Assert.assertTrue(falsePos < 2);\n}",
            "ClassName": "TestStoreFile",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testBloomFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 41,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestStoreFile",
        "Commit": "2d6804b03ee4b4ed6ddef4ebd9e2474d1c1e530a",
        "CyclomaticComplexity": 0,
        "Date": "Fri, 21 May 2010 17:31:37 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testBasicHalfMapFile",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 5,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274463097"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.SkipFilter(new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\"))));\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.SkipFilter(new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\"))));\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSkipFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    long expectedRows = this.numRows / 2;\n    long expectedKeys = 2;\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 2;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 4;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 4;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo\"));\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 4;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo\"));\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 2;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo\"));\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2]));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\"test.+-2\"));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testQualifierFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 48,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter());\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testFirstKeyOnlyFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    long expectedRows = 1;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\"testRow.+-2\"));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = 1;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows - 1;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows - 1;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows - 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\".+-2\"));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testRowFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 54,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.KeyValue[] expectedKVs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    long expectedRows = 6;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, expectedKVs);\n    expectedRows = 4;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, java.util.Arrays.copyOf(expectedKVs, 24));\n    expectedRows = 2;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, java.util.Arrays.copyOf(expectedKVs, 12));\n    expectedRows = 1;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, java.util.Arrays.copyOf(expectedKVs, 6));\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testPageFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 31,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestScannersWithFilters",
        "Commit": "2d6804b03ee4b4ed6ddef4ebd9e2474d1c1e530a",
        "CyclomaticComplexity": 0,
        "Date": "Fri, 21 May 2010 17:31:37 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testSkipFilter",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 7,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274463097"
    },
    {
        "Body": "{\n    long expectedRows = this.numRows / 2;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testValueOne\")));\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testValueTwo\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\"testValue((One)|(Two))\"));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testValueTwo\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testValueTwo\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testValueOne\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testValueOne\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testValueOne\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testValueOne\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    f = new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testValueOne\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
        "CUT_1": {
            "Body": "{\n    long expectedRows = this.numRows / 2;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testValueOne\")));\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testValueTwo\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\"testValue((One)|(Two))\"));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testValueTwo\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testValueTwo\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testValueOne\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testValueOne\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testValueOne\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testValueOne\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    f = new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testValueOne\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testValueFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 61,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    long expectedRows = 1;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\"testRow.+-2\"));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = 1;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows - 1;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows - 1;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows - 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\".+-2\"));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testRowFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 54,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    long expectedRows = this.numRows / 2;\n    long expectedKeys = 2;\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 2;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 4;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 4;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo\"));\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 4;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo\"));\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 2;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo\"));\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2]));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\"test.+-2\"));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testQualifierFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 48,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.SkipFilter(new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\"))));\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSkipFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.KeyValue[] expectedKVs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    long expectedRows = 6;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, expectedKVs);\n    expectedRows = 4;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, java.util.Arrays.copyOf(expectedKVs, 24));\n    expectedRows = 2;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, java.util.Arrays.copyOf(expectedKVs, 12));\n    expectedRows = 1;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, java.util.Arrays.copyOf(expectedKVs, 6));\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testPageFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 31,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestScannersWithFilters",
        "Commit": "2d6804b03ee4b4ed6ddef4ebd9e2474d1c1e530a",
        "CyclomaticComplexity": 0,
        "Date": "Fri, 21 May 2010 17:31:37 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testValueFilter",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 61,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274463097"
    },
    {
        "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testWritesWhileScanning\");\n    int testCount = 100;\n    int numRows = 1;\n    int numFamilies = 10;\n    int numQualifiers = 100;\n    int flushInterval = 7;\n    int compactInterval = 5 * flushInterval;\n    byte[][] families = new byte[numFamilies][];\n    for (int i = 0; i < numFamilies; i++) {\n        families[i] = org.apache.hadoop.hbase.util.Bytes.toBytes(\"family\" + i);\n    }\n    byte[][] qualifiers = new byte[numQualifiers][];\n    for (int i = 0; i < numQualifiers; i++) {\n        qualifiers[i] = org.apache.hadoop.hbase.util.Bytes.toBytes(\"qual\" + i);\n    }\n    java.lang.String method = \"testWritesWhileScanning\";\n    initHRegion(tableName, method, families);\n    org.apache.hadoop.hbase.regionserver.TestHRegion.PutThread putThread = new org.apache.hadoop.hbase.regionserver.TestHRegion.PutThread(numRows, families, qualifiers);\n    putThread.start();\n    org.apache.hadoop.hbase.regionserver.TestHRegion.FlushThread flushThread = new org.apache.hadoop.hbase.regionserver.TestHRegion.FlushThread();\n    flushThread.start();\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row0\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\"));\n    int expectedCount = numFamilies * numQualifiers;\n    java.util.List<org.apache.hadoop.hbase.KeyValue> res = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    long prevTimestamp = 0L;\n    for (int i = 0; i < testCount; i++) {\n        if ((i != 0) && ((i % compactInterval) == 0)) {\n            region.compactStores(true);\n        }\n        if ((i != 0) && ((i % flushInterval) == 0)) {\n            flushThread.flush();\n        }\n        boolean previousEmpty = res.isEmpty();\n        res.clear();\n        org.apache.hadoop.hbase.regionserver.InternalScanner scanner = region.getScanner(scan);\n        while (scanner.next(res));\n        if (((!res.isEmpty()) || (!previousEmpty)) || (i > compactInterval)) {\n            junit.framework.Assert.assertEquals(\"i=\" + i, expectedCount, res.size());\n            long timestamp = res.get(0).getTimestamp();\n            junit.framework.Assert.assertTrue(((\"Timestamps were broke: \" + timestamp) + \" prev: \") + prevTimestamp, timestamp >= prevTimestamp);\n            prevTimestamp = timestamp;\n        }\n    }\n    putThread.done();\n    region.flushcache();\n    putThread.join();\n    putThread.checkNoError();\n    flushThread.done();\n    flushThread.join();\n    flushThread.checkNoError();\n}",
        "CUT_1": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testWritesWhileScanning\");\n    int testCount = 100;\n    int numRows = 1;\n    int numFamilies = 10;\n    int numQualifiers = 100;\n    int flushInterval = 7;\n    int compactInterval = 5 * flushInterval;\n    byte[][] families = new byte[numFamilies][];\n    for (int i = 0; i < numFamilies; i++) {\n        families[i] = org.apache.hadoop.hbase.util.Bytes.toBytes(\"family\" + i);\n    }\n    byte[][] qualifiers = new byte[numQualifiers][];\n    for (int i = 0; i < numQualifiers; i++) {\n        qualifiers[i] = org.apache.hadoop.hbase.util.Bytes.toBytes(\"qual\" + i);\n    }\n    java.lang.String method = \"testWritesWhileScanning\";\n    initHRegion(tableName, method, families);\n    org.apache.hadoop.hbase.regionserver.TestHRegion.PutThread putThread = new org.apache.hadoop.hbase.regionserver.TestHRegion.PutThread(numRows, families, qualifiers);\n    putThread.start();\n    org.apache.hadoop.hbase.regionserver.TestHRegion.FlushThread flushThread = new org.apache.hadoop.hbase.regionserver.TestHRegion.FlushThread();\n    flushThread.start();\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row0\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\"));\n    int expectedCount = numFamilies * numQualifiers;\n    java.util.List<org.apache.hadoop.hbase.KeyValue> res = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    long prevTimestamp = 0L;\n    for (int i = 0; i < testCount; i++) {\n        if ((i != 0) && ((i % compactInterval) == 0)) {\n            region.compactStores(true);\n        }\n        if ((i != 0) && ((i % flushInterval) == 0)) {\n            flushThread.flush();\n        }\n        boolean previousEmpty = res.isEmpty();\n        res.clear();\n        org.apache.hadoop.hbase.regionserver.InternalScanner scanner = region.getScanner(scan);\n        while (scanner.next(res));\n        if (((!res.isEmpty()) || (!previousEmpty)) || (i > compactInterval)) {\n            junit.framework.Assert.assertEquals(\"i=\" + i, expectedCount, res.size());\n            long timestamp = res.get(0).getTimestamp();\n            junit.framework.Assert.assertTrue(((\"Timestamps were broke: \" + timestamp) + \" prev: \") + prevTimestamp, timestamp >= prevTimestamp);\n            prevTimestamp = timestamp;\n        }\n    }\n    putThread.done();\n    region.flushcache();\n    putThread.join();\n    putThread.checkNoError();\n    flushThread.done();\n    flushThread.join();\n    flushThread.checkNoError();\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 7,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testWritesWhileScanning",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 52,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 4
        },
        "CUT_2": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testWritesWhileScanning\");\n    int testCount = 100;\n    int numRows = 1;\n    int numFamilies = 10;\n    int numQualifiers = 100;\n    int flushInterval = 10;\n    int compactInterval = 10 * flushInterval;\n    byte[][] families = new byte[numFamilies][];\n    for (int i = 0; i < numFamilies; i++) {\n        families[i] = org.apache.hadoop.hbase.util.Bytes.toBytes(\"family\" + i);\n    }\n    byte[][] qualifiers = new byte[numQualifiers][];\n    for (int i = 0; i < numQualifiers; i++) {\n        qualifiers[i] = org.apache.hadoop.hbase.util.Bytes.toBytes(\"qual\" + i);\n    }\n    java.lang.String method = \"testWritesWhileScanning\";\n    initHRegion(tableName, method, families);\n    org.apache.hadoop.hbase.regionserver.TestHRegion.PutThread putThread = new org.apache.hadoop.hbase.regionserver.TestHRegion.PutThread(numRows, families, qualifiers);\n    putThread.start();\n    org.apache.hadoop.hbase.regionserver.TestHRegion.FlushThread flushThread = new org.apache.hadoop.hbase.regionserver.TestHRegion.FlushThread();\n    flushThread.start();\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row0\"));\n    org.apache.hadoop.hbase.client.Result result = null;\n    int expectedCount = numFamilies * numQualifiers;\n    long prevTimestamp = 0L;\n    for (int i = 0; i < testCount; i++) {\n        if ((i != 0) && ((i % compactInterval) == 0)) {\n            region.compactStores(true);\n        }\n        if ((i != 0) && ((i % flushInterval) == 0)) {\n            flushThread.flush();\n        }\n        boolean previousEmpty = (result == null) || result.isEmpty();\n        result = region.get(get, null);\n        if (((!result.isEmpty()) || (!previousEmpty)) || (i > compactInterval)) {\n            junit.framework.Assert.assertEquals(\"i=\" + i, expectedCount, result.size());\n            long timestamp = 0;\n            for (org.apache.hadoop.hbase.KeyValue kv : result.sorted()) {\n                if (org.apache.hadoop.hbase.util.Bytes.equals(kv.getFamily(), families[0]) && org.apache.hadoop.hbase.util.Bytes.equals(kv.getQualifier(), qualifiers[0])) {\n                    timestamp = kv.getTimestamp();\n                }\n            }\n            junit.framework.Assert.assertTrue(timestamp >= prevTimestamp);\n            prevTimestamp = timestamp;\n            byte[] gotValue = null;\n            for (org.apache.hadoop.hbase.KeyValue kv : result.raw()) {\n                byte[] thisValue = kv.getValue();\n                if (gotValue != null) {\n                    org.apache.hadoop.hbase.HBaseTestCase.assertEquals(gotValue, thisValue);\n                }\n                gotValue = thisValue;\n            }\n        }\n    }\n    putThread.done();\n    region.flushcache();\n    putThread.join();\n    putThread.checkNoError();\n    flushThread.done();\n    flushThread.join();\n    flushThread.checkNoError();\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 10,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testWritesWhileGetting",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 63,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 4
        },
        "CUT_3": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testFlushCacheWhileScanning\");\n    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"family\");\n    int numRows = 1000;\n    int flushAndScanInterval = 10;\n    int compactInterval = 10 * flushAndScanInterval;\n    java.lang.String method = \"testFlushCacheWhileScanning\";\n    initHRegion(tableName, method, family);\n    org.apache.hadoop.hbase.regionserver.TestHRegion.FlushThread flushThread = new org.apache.hadoop.hbase.regionserver.TestHRegion.FlushThread();\n    flushThread.start();\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(family);\n    scan.setFilter(new org.apache.hadoop.hbase.filter.SingleColumnValueFilter(family, qual1, org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(5L))));\n    int expectedCount = 0;\n    java.util.List<org.apache.hadoop.hbase.KeyValue> res = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    boolean toggle = true;\n    for (long i = 0; i < numRows; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(i));\n        put.add(family, qual1, org.apache.hadoop.hbase.util.Bytes.toBytes(i % 10));\n        region.put(put);\n        if ((i != 0) && ((i % compactInterval) == 0)) {\n            region.compactStores(true);\n        }\n        if ((i % 10) == 5L) {\n            expectedCount++;\n        }\n        if ((i != 0) && ((i % flushAndScanInterval) == 0)) {\n            res.clear();\n            org.apache.hadoop.hbase.regionserver.InternalScanner scanner = region.getScanner(scan);\n            if (toggle) {\n                flushThread.flush();\n            }\n            while (scanner.next(res));\n            if (!toggle) {\n                flushThread.flush();\n            }\n            junit.framework.Assert.assertEquals(\"i=\" + i, expectedCount, res.size());\n            toggle = !toggle;\n        }\n    }\n    flushThread.done();\n    flushThread.join();\n    flushThread.checkNoError();\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 7,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testFlushCacheWhileScanning",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 44,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_4": {
            "Body": "{\n    java.lang.String method = this.getName();\n    initHRegion(org.apache.hadoop.hbase.HConstants.ROOT_TABLE_NAME, method, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, null);\n    region.put(put);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW);\n    get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.KeyValue kv1 = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.KeyValue[] expected = new org.apache.hadoop.hbase.KeyValue[]{ kv1 };\n    org.apache.hadoop.hbase.client.Result res = region.get(get, null);\n    junit.framework.Assert.assertEquals(expected.length, res.size());\n    for (int i = 0; i < res.size(); i++) {\n        junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(expected[i].getRow(), res.raw()[i].getRow()));\n        junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(expected[i].getFamily(), res.raw()[i].getFamily()));\n        junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(expected[i].getQualifier(), res.raw()[i].getQualifier()));\n    }\n    region.flushcache();\n    res = region.get(get, null);\n    junit.framework.Assert.assertEquals(expected.length, res.size());\n    for (int i = 0; i < res.size(); i++) {\n        junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(expected[i].getRow(), res.raw()[i].getRow()));\n        junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(expected[i].getFamily(), res.raw()[i].getFamily()));\n        junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(expected[i].getQualifier(), res.raw()[i].getQualifier()));\n    }\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> result = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    s.next(result);\n    junit.framework.Assert.assertEquals(expected.length, result.size());\n    for (int i = 0; i < res.size(); i++) {\n        junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(expected[i].getRow(), result.get(i).getRow()));\n        junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(expected[i].getFamily(), result.get(i).getFamily()));\n        junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(expected[i].getQualifier(), result.get(i).getQualifier()));\n    }\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "stestGet_Root",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 37,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam1\");\n    byte[] col1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"col1\");\n    byte[] col2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"col2\");\n    byte[] col3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"col3\");\n    byte[] col4 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"col4\");\n    byte[] col5 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"col5\");\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, fam1);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(fam1, col1, null);\n    put.add(fam1, col2, null);\n    put.add(fam1, col3, null);\n    put.add(fam1, col4, null);\n    put.add(fam1, col5, null);\n    region.put(put);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row1);\n    get.addColumn(fam1, col2);\n    get.addColumn(fam1, col4);\n    org.apache.hadoop.hbase.KeyValue kv1 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, col2);\n    org.apache.hadoop.hbase.KeyValue kv2 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, col4);\n    org.apache.hadoop.hbase.KeyValue[] expected = new org.apache.hadoop.hbase.KeyValue[]{ kv1, kv2 };\n    org.apache.hadoop.hbase.client.Result res = region.get(get, null);\n    junit.framework.Assert.assertEquals(expected.length, res.size());\n    for (int i = 0; i < res.size(); i++) {\n        junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(expected[i].getRow(), res.raw()[i].getRow()));\n        junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(expected[i].getFamily(), res.raw()[i].getFamily()));\n        junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(expected[i].getQualifier(), res.raw()[i].getQualifier()));\n    }\n    org.apache.hadoop.hbase.client.Get g = new org.apache.hadoop.hbase.client.Get(row1);\n    final int count = 2;\n    g.setFilter(new org.apache.hadoop.hbase.filter.ColumnCountGetFilter(count));\n    res = region.get(g, null);\n    junit.framework.Assert.assertEquals(count, res.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGet_Basic",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 37,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHRegion",
        "Commit": "2d6804b03ee4b4ed6ddef4ebd9e2474d1c1e530a",
        "CyclomaticComplexity": 7,
        "Date": "Fri, 21 May 2010 17:31:37 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testWritesWhileScanning",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 52,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 4,
        "ProjectName": "hbase",
        "Timestamp": "1274463097"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n    remoteTable.put(put);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    org.apache.hadoop.hbase.client.Result result = remoteTable.get(get);\n    byte[] value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    byte[] value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNotNull(value2);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value2));\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    delete.deleteColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    remoteTable.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    remoteTable.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNull(value1);\n    junit.framework.Assert.assertNull(value2);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n    remoteTable.put(put);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    org.apache.hadoop.hbase.client.Result result = remoteTable.get(get);\n    byte[] value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    byte[] value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNotNull(value2);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value2));\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    delete.deleteColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    remoteTable.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    remoteTable.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNull(value1);\n    junit.framework.Assert.assertNull(value2);\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDelete",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 38,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n    org.apache.hadoop.hbase.client.Result result = remoteTable.get(get);\n    byte[] value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    byte[] value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_3);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNull(value1);\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n    get.addColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    get.addColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value1));\n    junit.framework.Assert.assertNotNull(value2);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value2));\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    get.addColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value1));\n    junit.framework.Assert.assertNotNull(value2);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value2));\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    get.setTimeStamp(org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_1);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    get.setTimeRange(0, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_1 + 1);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.setMaxVersions(2);\n    result = remoteTable.get(get);\n    int count = 0;\n    for (org.apache.hadoop.hbase.KeyValue kv : result.list()) {\n        if (org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, kv.getFamily()) && (org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_1 == kv.getTimestamp())) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, kv.getValue()));\n            count++;\n        }\n        if (org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, kv.getFamily()) && (org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_2 == kv.getTimestamp())) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, kv.getValue()));\n            count++;\n        }\n    }\n    junit.framework.Assert.assertEquals(2, count);\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGet",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 87,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    remoteTable.put(put);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    org.apache.hadoop.hbase.client.Result result = remoteTable.get(get);\n    byte[] value = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    junit.framework.Assert.assertNotNull(value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value));\n    java.util.List<org.apache.hadoop.hbase.client.Put> puts = new java.util.ArrayList<org.apache.hadoop.hbase.client.Put>();\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n    puts.add(put);\n    remoteTable.put(puts);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    result = remoteTable.get(get);\n    value = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value));\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4);\n    result = remoteTable.get(get);\n    value = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    junit.framework.Assert.assertNotNull(value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value));\n    value = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value));\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testPut",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 36,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    super.setUp();\n    admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    if (!admin.tableExists(org.apache.hadoop.hbase.rest.client.TestRemoteTable.TABLE)) {\n        org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.rest.client.TestRemoteTable.TABLE);\n        htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1));\n        htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2));\n        htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_3));\n        admin.createTable(htd);\n        org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.rest.client.TestRemoteTable.TABLE);\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n        put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n        table.put(put);\n        put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n        put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n        put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n        put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n        table.put(put);\n        table.flushCommits();\n    }\n    remoteTable = new org.apache.hadoop.hbase.rest.client.RemoteHTable(new org.apache.hadoop.hbase.rest.client.Client(new org.apache.hadoop.hbase.rest.client.Cluster().add(\"localhost\", testServletPort)), conf, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TABLE, null);\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    java.util.List<org.apache.hadoop.hbase.client.Put> puts = new java.util.ArrayList<org.apache.hadoop.hbase.client.Put>();\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    remoteTable.put(puts);\n    org.apache.hadoop.hbase.client.ResultScanner scanner = remoteTable.getScanner(new org.apache.hadoop.hbase.client.Scan());\n    org.apache.hadoop.hbase.client.Result[] results = scanner.next(1);\n    junit.framework.Assert.assertNotNull(results);\n    junit.framework.Assert.assertEquals(1, results.length);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1, results[0].getRow()));\n    results = scanner.next(3);\n    junit.framework.Assert.assertNotNull(results);\n    junit.framework.Assert.assertEquals(3, results.length);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2, results[0].getRow()));\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3, results[1].getRow()));\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4, results[2].getRow()));\n    results = scanner.next(1);\n    junit.framework.Assert.assertNull(results);\n    scanner.close();\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 30,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestRemoteTable",
        "Commit": "851b6a9cfc98e4e0283f5babea156b8b5298fde2",
        "CyclomaticComplexity": 0,
        "Date": "Fri, 21 May 2010 21:35:30 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testDelete",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 38,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274477730"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.TableListModel model = ((org.apache.hadoop.hbase.rest.model.TableListModel) (context.createUnmarshaller().unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    checkTableList(model);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.TableListModel model = ((org.apache.hadoop.hbase.rest.model.TableListModel) (context.createUnmarshaller().unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    checkTableList(model);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableListXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.TableListModel model = new org.apache.hadoop.hbase.rest.model.TableListModel();\n    model.getObjectFromMessage(response.getBody());\n    checkTableList(model);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableListPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/status/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel model = ((org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel) (context.createUnmarshaller().unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    validate(model);\n}",
            "ClassName": "TestStatusResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetClusterStatusXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/status/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel model = new org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel();\n    model.getObjectFromMessage(response.getBody());\n    validate(model);\n}",
            "ClassName": "TestStatusResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetClusterStatusPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n    org.apache.hadoop.hbase.rest.model.VersionModel model = ((org.apache.hadoop.hbase.rest.model.VersionModel) (context.createUnmarshaller().unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    org.apache.hadoop.hbase.rest.TestVersionResource.validate(model);\n    org.apache.hadoop.hbase.rest.TestVersionResource.LOG.info(\"success retrieving Stargate version as XML\");\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStargateVersionXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableResource",
        "Commit": "851b6a9cfc98e4e0283f5babea156b8b5298fde2",
        "CyclomaticComplexity": 0,
        "Date": "Fri, 21 May 2010 21:35:30 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testTableListXML",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 6,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274477730"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.TableListModel model = new org.apache.hadoop.hbase.rest.model.TableListModel();\n    model.getObjectFromMessage(response.getBody());\n    checkTableList(model);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.TableListModel model = new org.apache.hadoop.hbase.rest.model.TableListModel();\n    model.getObjectFromMessage(response.getBody());\n    checkTableList(model);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableListPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.TableListModel model = ((org.apache.hadoop.hbase.rest.model.TableListModel) (context.createUnmarshaller().unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    checkTableList(model);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableListXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/status/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel model = new org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel();\n    model.getObjectFromMessage(response.getBody());\n    validate(model);\n}",
            "ClassName": "TestStatusResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetClusterStatusPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/status/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel model = ((org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel) (context.createUnmarshaller().unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    validate(model);\n}",
            "ClassName": "TestStatusResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetClusterStatusXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n    org.apache.hadoop.hbase.rest.model.VersionModel model = new org.apache.hadoop.hbase.rest.model.VersionModel();\n    model.getObjectFromMessage(response.getBody());\n    org.apache.hadoop.hbase.rest.TestVersionResource.validate(model);\n    org.apache.hadoop.hbase.rest.TestVersionResource.LOG.info(\"success retrieving Stargate version as protobuf\");\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStargateVersionPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableResource",
        "Commit": "851b6a9cfc98e4e0283f5babea156b8b5298fde2",
        "CyclomaticComplexity": 0,
        "Date": "Fri, 21 May 2010 21:35:30 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testTableListPB",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 7,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274477730"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_TEXT);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_TEXT);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableListText",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_TEXT);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStorageClusterVersionText",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_JSON);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableListJSON",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_JSON);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStargateVersionJSON",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_JSON);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStorageClusterVersionJSON",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableResource",
        "Commit": "851b6a9cfc98e4e0283f5babea156b8b5298fde2",
        "CyclomaticComplexity": 0,
        "Date": "Fri, 21 May 2010 21:35:30 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testTableListText",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 4,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274477730"
    },
    {
        "Body": "{\n    java.lang.String path = (\"/\" + org.apache.hadoop.hbase.rest.TestRowResource.TABLE) + \"/fakerow\";\n    org.apache.hadoop.hbase.rest.model.CellSetModel cellSetModel = new org.apache.hadoop.hbase.rest.model.CellSetModel();\n    org.apache.hadoop.hbase.rest.model.RowModel rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2)));\n    cellSetModel.addRow(rowModel);\n    rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4)));\n    cellSetModel.addRow(rowModel);\n    java.io.StringWriter writer = new java.io.StringWriter();\n    marshaller.marshal(cellSetModel, writer);\n    org.apache.hadoop.hbase.rest.client.Response response = client.put(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML, org.apache.hadoop.hbase.util.Bytes.toBytes(writer.toString()));\n    java.lang.Thread.yield();\n    response = client.get(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
        "CUT_1": {
            "Body": "{\n    java.lang.String path = (\"/\" + org.apache.hadoop.hbase.rest.TestRowResource.TABLE) + \"/fakerow\";\n    org.apache.hadoop.hbase.rest.model.CellSetModel cellSetModel = new org.apache.hadoop.hbase.rest.model.CellSetModel();\n    org.apache.hadoop.hbase.rest.model.RowModel rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2)));\n    cellSetModel.addRow(rowModel);\n    rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4)));\n    cellSetModel.addRow(rowModel);\n    java.io.StringWriter writer = new java.io.StringWriter();\n    marshaller.marshal(cellSetModel, writer);\n    org.apache.hadoop.hbase.rest.client.Response response = client.put(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML, org.apache.hadoop.hbase.util.Bytes.toBytes(writer.toString()));\n    java.lang.Thread.yield();\n    response = client.get(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testMultiCellGetPutXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 26,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    java.lang.String path = (\"/\" + org.apache.hadoop.hbase.rest.TestRowResource.TABLE) + \"/fakerow\";\n    org.apache.hadoop.hbase.rest.model.CellSetModel cellSetModel = new org.apache.hadoop.hbase.rest.model.CellSetModel();\n    org.apache.hadoop.hbase.rest.model.RowModel rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2)));\n    cellSetModel.addRow(rowModel);\n    rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4)));\n    cellSetModel.addRow(rowModel);\n    org.apache.hadoop.hbase.rest.client.Response response = client.put(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF, cellSetModel.createProtobufOutput());\n    java.lang.Thread.yield();\n    response = client.get(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testMultiCellGetPutPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 24,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSingleCellGetPutXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response;\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteValue(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDelete",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 20,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = getValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = putValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSingleCellGetPutPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 15,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestRowResource",
        "Commit": "851b6a9cfc98e4e0283f5babea156b8b5298fde2",
        "CyclomaticComplexity": 0,
        "Date": "Fri, 21 May 2010 21:35:30 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testMultiCellGetPutXML",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 26,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 2,
        "ProjectName": "hbase",
        "Timestamp": "1274477730"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.KeyValue[] expectedKVs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    long expectedRows = 6;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, expectedKVs);\n    expectedRows = 4;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, java.util.Arrays.copyOf(expectedKVs, 24));\n    expectedRows = 2;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, java.util.Arrays.copyOf(expectedKVs, 12));\n    expectedRows = 1;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, java.util.Arrays.copyOf(expectedKVs, 6));\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.KeyValue[] expectedKVs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    long expectedRows = 6;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, expectedKVs);\n    expectedRows = 4;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, java.util.Arrays.copyOf(expectedKVs, 24));\n    expectedRows = 2;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, java.util.Arrays.copyOf(expectedKVs, 12));\n    expectedRows = 1;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, java.util.Arrays.copyOf(expectedKVs, 6));\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testPageFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 31,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    long expectedRows = this.numRows / 2;\n    long expectedKeys = 2;\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 2;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 4;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 4;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo\"));\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 4;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo\"));\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 2;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo\"));\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2]));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\"test.+-2\"));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testQualifierFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 48,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter());\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testFirstKeyOnlyFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    long expectedRows = 1;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\"testRow.+-2\"));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = 1;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows - 1;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows - 1;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows - 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\".+-2\"));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testRowFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 54,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.SkipFilter(new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\"))));\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSkipFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestScannersWithFilters",
        "Commit": "851b6a9cfc98e4e0283f5babea156b8b5298fde2",
        "CyclomaticComplexity": 0,
        "Date": "Fri, 21 May 2010 21:35:30 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testPageFilter",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 31,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274477730"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n    org.apache.hadoop.hbase.client.Result result = remoteTable.get(get);\n    byte[] value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    byte[] value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_3);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNull(value1);\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n    get.addColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    get.addColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value1));\n    junit.framework.Assert.assertNotNull(value2);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value2));\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    get.addColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value1));\n    junit.framework.Assert.assertNotNull(value2);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value2));\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    get.setTimeStamp(org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_1);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    get.setTimeRange(0, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_1 + 1);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.setMaxVersions(2);\n    result = remoteTable.get(get);\n    int count = 0;\n    for (org.apache.hadoop.hbase.KeyValue kv : result.list()) {\n        if (org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, kv.getFamily()) && (org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_1 == kv.getTimestamp())) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, kv.getValue()));\n            count++;\n        }\n        if (org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, kv.getFamily()) && (org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_2 == kv.getTimestamp())) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, kv.getValue()));\n            count++;\n        }\n    }\n    junit.framework.Assert.assertEquals(2, count);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n    org.apache.hadoop.hbase.client.Result result = remoteTable.get(get);\n    byte[] value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    byte[] value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_3);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNull(value1);\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n    get.addColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    get.addColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value1));\n    junit.framework.Assert.assertNotNull(value2);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value2));\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    get.addColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value1));\n    junit.framework.Assert.assertNotNull(value2);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value2));\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    get.setTimeStamp(org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_1);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    get.setTimeRange(0, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_1 + 1);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.setMaxVersions(2);\n    result = remoteTable.get(get);\n    int count = 0;\n    for (org.apache.hadoop.hbase.KeyValue kv : result.list()) {\n        if (org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, kv.getFamily()) && (org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_1 == kv.getTimestamp())) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, kv.getValue()));\n            count++;\n        }\n        if (org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, kv.getFamily()) && (org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_2 == kv.getTimestamp())) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, kv.getValue()));\n            count++;\n        }\n    }\n    junit.framework.Assert.assertEquals(2, count);\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGet",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 87,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n    remoteTable.put(put);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    org.apache.hadoop.hbase.client.Result result = remoteTable.get(get);\n    byte[] value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    byte[] value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNotNull(value2);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value2));\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    delete.deleteColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    remoteTable.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    remoteTable.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNull(value1);\n    junit.framework.Assert.assertNull(value2);\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDelete",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 38,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    remoteTable.put(put);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    org.apache.hadoop.hbase.client.Result result = remoteTable.get(get);\n    byte[] value = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    junit.framework.Assert.assertNotNull(value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value));\n    java.util.List<org.apache.hadoop.hbase.client.Put> puts = new java.util.ArrayList<org.apache.hadoop.hbase.client.Put>();\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n    puts.add(put);\n    remoteTable.put(puts);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    result = remoteTable.get(get);\n    value = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value));\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4);\n    result = remoteTable.get(get);\n    value = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    junit.framework.Assert.assertNotNull(value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value));\n    value = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value));\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testPut",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 36,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    super.setUp();\n    admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    if (!admin.tableExists(org.apache.hadoop.hbase.rest.client.TestRemoteTable.TABLE)) {\n        org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.rest.client.TestRemoteTable.TABLE);\n        htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1));\n        htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2));\n        htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_3));\n        admin.createTable(htd);\n        org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.rest.client.TestRemoteTable.TABLE);\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n        put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n        table.put(put);\n        put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n        put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n        put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n        put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n        table.put(put);\n        table.flushCommits();\n    }\n    remoteTable = new org.apache.hadoop.hbase.rest.client.RemoteHTable(new org.apache.hadoop.hbase.rest.client.Client(new org.apache.hadoop.hbase.rest.client.Cluster().add(\"localhost\", testServletPort)), conf, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TABLE, null);\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    java.util.List<org.apache.hadoop.hbase.client.Put> puts = new java.util.ArrayList<org.apache.hadoop.hbase.client.Put>();\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    remoteTable.put(puts);\n    org.apache.hadoop.hbase.client.ResultScanner scanner = remoteTable.getScanner(new org.apache.hadoop.hbase.client.Scan());\n    org.apache.hadoop.hbase.client.Result[] results = scanner.next(1);\n    junit.framework.Assert.assertNotNull(results);\n    junit.framework.Assert.assertEquals(1, results.length);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1, results[0].getRow()));\n    results = scanner.next(3);\n    junit.framework.Assert.assertNotNull(results);\n    junit.framework.Assert.assertEquals(3, results.length);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2, results[0].getRow()));\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3, results[1].getRow()));\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4, results[2].getRow()));\n    results = scanner.next(1);\n    junit.framework.Assert.assertNull(results);\n    scanner.close();\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 30,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestRemoteTable",
        "Commit": "851b6a9cfc98e4e0283f5babea156b8b5298fde2",
        "CyclomaticComplexity": 3,
        "Date": "Fri, 21 May 2010 21:35:30 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testGet",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 87,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274477730"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_JSON);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_JSON);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableListJSON",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_JSON);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStargateVersionJSON",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_JSON);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStorageClusterVersionJSON",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_TEXT);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableListText",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_TEXT);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStorageClusterVersionText",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableResource",
        "Commit": "851b6a9cfc98e4e0283f5babea156b8b5298fde2",
        "CyclomaticComplexity": 0,
        "Date": "Fri, 21 May 2010 21:35:30 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testTableListJSON",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 4,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274477730"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor local = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TABLE).getTableDescriptor();\n    junit.framework.Assert.assertEquals(remoteTable.getTableDescriptor(), local);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor local = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TABLE).getTableDescriptor();\n    junit.framework.Assert.assertEquals(remoteTable.getTableDescriptor(), local);\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetTableDescriptor",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    super.setUp();\n    admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    if (!admin.tableExists(org.apache.hadoop.hbase.rest.client.TestRemoteTable.TABLE)) {\n        org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.rest.client.TestRemoteTable.TABLE);\n        htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1));\n        htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2));\n        htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_3));\n        admin.createTable(htd);\n        org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.rest.client.TestRemoteTable.TABLE);\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n        put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n        table.put(put);\n        put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n        put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n        put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n        put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n        table.put(put);\n        table.flushCommits();\n    }\n    remoteTable = new org.apache.hadoop.hbase.rest.client.RemoteHTable(new org.apache.hadoop.hbase.rest.client.Client(new org.apache.hadoop.hbase.rest.client.Cluster().add(\"localhost\", testServletPort)), conf, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TABLE, null);\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    remoteTable.put(put);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    org.apache.hadoop.hbase.client.Result result = remoteTable.get(get);\n    byte[] value = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    junit.framework.Assert.assertNotNull(value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value));\n    java.util.List<org.apache.hadoop.hbase.client.Put> puts = new java.util.ArrayList<org.apache.hadoop.hbase.client.Put>();\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n    puts.add(put);\n    remoteTable.put(puts);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    result = remoteTable.get(get);\n    value = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value));\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4);\n    result = remoteTable.get(get);\n    value = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    junit.framework.Assert.assertNotNull(value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value));\n    value = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value));\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testPut",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 36,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n    remoteTable.put(put);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    org.apache.hadoop.hbase.client.Result result = remoteTable.get(get);\n    byte[] value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    byte[] value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNotNull(value2);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value2));\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    delete.deleteColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    remoteTable.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    remoteTable.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNull(value1);\n    junit.framework.Assert.assertNull(value2);\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDelete",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 38,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n    org.apache.hadoop.hbase.client.Result result = remoteTable.get(get);\n    byte[] value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    byte[] value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_3);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNull(value1);\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n    get.addColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    get.addColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value1));\n    junit.framework.Assert.assertNotNull(value2);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value2));\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    get.addColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value1));\n    junit.framework.Assert.assertNotNull(value2);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value2));\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    get.setTimeStamp(org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_1);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    get.setTimeRange(0, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_1 + 1);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.setMaxVersions(2);\n    result = remoteTable.get(get);\n    int count = 0;\n    for (org.apache.hadoop.hbase.KeyValue kv : result.list()) {\n        if (org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, kv.getFamily()) && (org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_1 == kv.getTimestamp())) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, kv.getValue()));\n            count++;\n        }\n        if (org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, kv.getFamily()) && (org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_2 == kv.getTimestamp())) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, kv.getValue()));\n            count++;\n        }\n    }\n    junit.framework.Assert.assertEquals(2, count);\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGet",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 87,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestRemoteTable",
        "Commit": "851b6a9cfc98e4e0283f5babea156b8b5298fde2",
        "CyclomaticComplexity": 0,
        "Date": "Fri, 21 May 2010 21:35:30 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testGetTableDescriptor",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 4,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274477730"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    remoteTable.put(put);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    org.apache.hadoop.hbase.client.Result result = remoteTable.get(get);\n    byte[] value = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    junit.framework.Assert.assertNotNull(value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value));\n    java.util.List<org.apache.hadoop.hbase.client.Put> puts = new java.util.ArrayList<org.apache.hadoop.hbase.client.Put>();\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n    puts.add(put);\n    remoteTable.put(puts);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    result = remoteTable.get(get);\n    value = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value));\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4);\n    result = remoteTable.get(get);\n    value = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    junit.framework.Assert.assertNotNull(value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value));\n    value = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value));\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    remoteTable.put(put);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    org.apache.hadoop.hbase.client.Result result = remoteTable.get(get);\n    byte[] value = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    junit.framework.Assert.assertNotNull(value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value));\n    java.util.List<org.apache.hadoop.hbase.client.Put> puts = new java.util.ArrayList<org.apache.hadoop.hbase.client.Put>();\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n    puts.add(put);\n    remoteTable.put(puts);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    result = remoteTable.get(get);\n    value = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value));\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4);\n    result = remoteTable.get(get);\n    value = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    junit.framework.Assert.assertNotNull(value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value));\n    value = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value));\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testPut",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 36,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n    remoteTable.put(put);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    org.apache.hadoop.hbase.client.Result result = remoteTable.get(get);\n    byte[] value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    byte[] value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNotNull(value2);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value2));\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    delete.deleteColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    remoteTable.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    remoteTable.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNull(value1);\n    junit.framework.Assert.assertNull(value2);\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDelete",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 38,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    super.setUp();\n    admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    if (!admin.tableExists(org.apache.hadoop.hbase.rest.client.TestRemoteTable.TABLE)) {\n        org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.rest.client.TestRemoteTable.TABLE);\n        htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1));\n        htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2));\n        htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_3));\n        admin.createTable(htd);\n        org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.rest.client.TestRemoteTable.TABLE);\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n        put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n        table.put(put);\n        put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n        put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n        put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n        put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n        table.put(put);\n        table.flushCommits();\n    }\n    remoteTable = new org.apache.hadoop.hbase.rest.client.RemoteHTable(new org.apache.hadoop.hbase.rest.client.Client(new org.apache.hadoop.hbase.rest.client.Cluster().add(\"localhost\", testServletPort)), conf, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TABLE, null);\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n    org.apache.hadoop.hbase.client.Result result = remoteTable.get(get);\n    byte[] value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    byte[] value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_3);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNull(value1);\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n    get.addColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    get.addColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value1));\n    junit.framework.Assert.assertNotNull(value2);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value2));\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    get.addColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value1));\n    junit.framework.Assert.assertNotNull(value2);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value2));\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    get.setTimeStamp(org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_1);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    get.setTimeRange(0, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_1 + 1);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.setMaxVersions(2);\n    result = remoteTable.get(get);\n    int count = 0;\n    for (org.apache.hadoop.hbase.KeyValue kv : result.list()) {\n        if (org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, kv.getFamily()) && (org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_1 == kv.getTimestamp())) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, kv.getValue()));\n            count++;\n        }\n        if (org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, kv.getFamily()) && (org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_2 == kv.getTimestamp())) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, kv.getValue()));\n            count++;\n        }\n    }\n    junit.framework.Assert.assertEquals(2, count);\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGet",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 87,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    java.util.List<org.apache.hadoop.hbase.client.Put> puts = new java.util.ArrayList<org.apache.hadoop.hbase.client.Put>();\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    remoteTable.put(puts);\n    org.apache.hadoop.hbase.client.ResultScanner scanner = remoteTable.getScanner(new org.apache.hadoop.hbase.client.Scan());\n    org.apache.hadoop.hbase.client.Result[] results = scanner.next(1);\n    junit.framework.Assert.assertNotNull(results);\n    junit.framework.Assert.assertEquals(1, results.length);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1, results[0].getRow()));\n    results = scanner.next(3);\n    junit.framework.Assert.assertNotNull(results);\n    junit.framework.Assert.assertEquals(3, results.length);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2, results[0].getRow()));\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3, results[1].getRow()));\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4, results[2].getRow()));\n    results = scanner.next(1);\n    junit.framework.Assert.assertNull(results);\n    scanner.close();\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 30,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestRemoteTable",
        "Commit": "851b6a9cfc98e4e0283f5babea156b8b5298fde2",
        "CyclomaticComplexity": 0,
        "Date": "Fri, 21 May 2010 21:35:30 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testPut",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 36,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274477730"
    },
    {
        "Body": "{\n    java.lang.String path = (\"/\" + org.apache.hadoop.hbase.rest.TestRowResource.TABLE) + \"/fakerow\";\n    org.apache.hadoop.hbase.rest.model.CellSetModel cellSetModel = new org.apache.hadoop.hbase.rest.model.CellSetModel();\n    org.apache.hadoop.hbase.rest.model.RowModel rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2)));\n    cellSetModel.addRow(rowModel);\n    rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4)));\n    cellSetModel.addRow(rowModel);\n    org.apache.hadoop.hbase.rest.client.Response response = client.put(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF, cellSetModel.createProtobufOutput());\n    java.lang.Thread.yield();\n    response = client.get(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
        "CUT_1": {
            "Body": "{\n    java.lang.String path = (\"/\" + org.apache.hadoop.hbase.rest.TestRowResource.TABLE) + \"/fakerow\";\n    org.apache.hadoop.hbase.rest.model.CellSetModel cellSetModel = new org.apache.hadoop.hbase.rest.model.CellSetModel();\n    org.apache.hadoop.hbase.rest.model.RowModel rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2)));\n    cellSetModel.addRow(rowModel);\n    rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4)));\n    cellSetModel.addRow(rowModel);\n    org.apache.hadoop.hbase.rest.client.Response response = client.put(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF, cellSetModel.createProtobufOutput());\n    java.lang.Thread.yield();\n    response = client.get(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testMultiCellGetPutPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 24,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    java.lang.String path = (\"/\" + org.apache.hadoop.hbase.rest.TestRowResource.TABLE) + \"/fakerow\";\n    org.apache.hadoop.hbase.rest.model.CellSetModel cellSetModel = new org.apache.hadoop.hbase.rest.model.CellSetModel();\n    org.apache.hadoop.hbase.rest.model.RowModel rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2)));\n    cellSetModel.addRow(rowModel);\n    rowModel = new org.apache.hadoop.hbase.rest.model.RowModel(org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3)));\n    rowModel.addCell(new org.apache.hadoop.hbase.rest.model.CellModel(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2), org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4)));\n    cellSetModel.addRow(rowModel);\n    java.io.StringWriter writer = new java.io.StringWriter();\n    marshaller.marshal(cellSetModel, writer);\n    org.apache.hadoop.hbase.rest.client.Response response = client.put(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML, org.apache.hadoop.hbase.util.Bytes.toBytes(writer.toString()));\n    java.lang.Thread.yield();\n    response = client.get(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_4);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testMultiCellGetPutXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 26,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = getValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = putValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValuePB(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSingleCellGetPutPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 15,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSingleCellGetPutXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response;\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = putValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_1);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteValue(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    checkValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2, org.apache.hadoop.hbase.rest.TestRowResource.VALUE_2);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n    response = getValueXML(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_1, org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_2);\n    junit.framework.Assert.assertEquals(response.getCode(), 404);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDelete",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 20,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestRowResource",
        "Commit": "851b6a9cfc98e4e0283f5babea156b8b5298fde2",
        "CyclomaticComplexity": 0,
        "Date": "Fri, 21 May 2010 21:35:30 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testMultiCellGetPutPB",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 24,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 2,
        "ProjectName": "hbase",
        "Timestamp": "1274477730"
    },
    {
        "Body": "{\n    java.util.List<org.apache.hadoop.hbase.client.Put> puts = new java.util.ArrayList<org.apache.hadoop.hbase.client.Put>();\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    remoteTable.put(puts);\n    org.apache.hadoop.hbase.client.ResultScanner scanner = remoteTable.getScanner(new org.apache.hadoop.hbase.client.Scan());\n    org.apache.hadoop.hbase.client.Result[] results = scanner.next(1);\n    junit.framework.Assert.assertNotNull(results);\n    junit.framework.Assert.assertEquals(1, results.length);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1, results[0].getRow()));\n    results = scanner.next(3);\n    junit.framework.Assert.assertNotNull(results);\n    junit.framework.Assert.assertEquals(3, results.length);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2, results[0].getRow()));\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3, results[1].getRow()));\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4, results[2].getRow()));\n    results = scanner.next(1);\n    junit.framework.Assert.assertNull(results);\n    scanner.close();\n}",
        "CUT_1": {
            "Body": "{\n    java.util.List<org.apache.hadoop.hbase.client.Put> puts = new java.util.ArrayList<org.apache.hadoop.hbase.client.Put>();\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    remoteTable.put(puts);\n    org.apache.hadoop.hbase.client.ResultScanner scanner = remoteTable.getScanner(new org.apache.hadoop.hbase.client.Scan());\n    org.apache.hadoop.hbase.client.Result[] results = scanner.next(1);\n    junit.framework.Assert.assertNotNull(results);\n    junit.framework.Assert.assertEquals(1, results.length);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1, results[0].getRow()));\n    results = scanner.next(3);\n    junit.framework.Assert.assertNotNull(results);\n    junit.framework.Assert.assertEquals(3, results.length);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2, results[0].getRow()));\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3, results[1].getRow()));\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4, results[2].getRow()));\n    results = scanner.next(1);\n    junit.framework.Assert.assertNull(results);\n    scanner.close();\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 30,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    remoteTable.put(put);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    org.apache.hadoop.hbase.client.Result result = remoteTable.get(get);\n    byte[] value = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    junit.framework.Assert.assertNotNull(value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value));\n    java.util.List<org.apache.hadoop.hbase.client.Put> puts = new java.util.ArrayList<org.apache.hadoop.hbase.client.Put>();\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    puts.add(put);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n    puts.add(put);\n    remoteTable.put(puts);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    result = remoteTable.get(get);\n    value = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value));\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_4);\n    result = remoteTable.get(get);\n    value = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    junit.framework.Assert.assertNotNull(value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value));\n    value = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value));\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testPut",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 36,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    super.setUp();\n    admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    if (!admin.tableExists(org.apache.hadoop.hbase.rest.client.TestRemoteTable.TABLE)) {\n        org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.rest.client.TestRemoteTable.TABLE);\n        htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1));\n        htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2));\n        htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_3));\n        admin.createTable(htd);\n        org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.rest.client.TestRemoteTable.TABLE);\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n        put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n        table.put(put);\n        put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n        put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n        put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n        put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n        table.put(put);\n        table.flushCommits();\n    }\n    remoteTable = new org.apache.hadoop.hbase.rest.client.RemoteHTable(new org.apache.hadoop.hbase.rest.client.Client(new org.apache.hadoop.hbase.rest.client.Cluster().add(\"localhost\", testServletPort)), conf, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TABLE, null);\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1);\n    put.add(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2);\n    remoteTable.put(put);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    org.apache.hadoop.hbase.client.Result result = remoteTable.get(get);\n    byte[] value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    byte[] value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNotNull(value2);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value2));\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    delete.deleteColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    remoteTable.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    remoteTable.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_3);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNull(value1);\n    junit.framework.Assert.assertNull(value2);\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDelete",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 38,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n    org.apache.hadoop.hbase.client.Result result = remoteTable.get(get);\n    byte[] value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    byte[] value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_3);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNull(value1);\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_1);\n    get.addColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    get.addColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value1));\n    junit.framework.Assert.assertNotNull(value2);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value2));\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    get.addColumn(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value1));\n    junit.framework.Assert.assertNotNull(value2);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, value2));\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    get.setTimeStamp(org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_1);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2);\n    get.setTimeRange(0, org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_1 + 1);\n    result = remoteTable.get(get);\n    value1 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_1);\n    value2 = result.getValue(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_2, org.apache.hadoop.hbase.rest.client.TestRemoteTable.QUALIFIER_2);\n    junit.framework.Assert.assertNotNull(value1);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, value1));\n    junit.framework.Assert.assertNull(value2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.rest.client.TestRemoteTable.ROW_2);\n    get.addFamily(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1);\n    get.setMaxVersions(2);\n    result = remoteTable.get(get);\n    int count = 0;\n    for (org.apache.hadoop.hbase.KeyValue kv : result.list()) {\n        if (org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, kv.getFamily()) && (org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_1 == kv.getTimestamp())) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_1, kv.getValue()));\n            count++;\n        }\n        if (org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.COLUMN_1, kv.getFamily()) && (org.apache.hadoop.hbase.rest.client.TestRemoteTable.TS_2 == kv.getTimestamp())) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.rest.client.TestRemoteTable.VALUE_2, kv.getValue()));\n            count++;\n        }\n    }\n    junit.framework.Assert.assertEquals(2, count);\n}",
            "ClassName": "TestRemoteTable",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGet",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 87,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestRemoteTable",
        "Commit": "851b6a9cfc98e4e0283f5babea156b8b5298fde2",
        "CyclomaticComplexity": 0,
        "Date": "Fri, 21 May 2010 21:35:30 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testScanner",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 30,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274477730"
    },
    {
        "Body": "{\n    final int BATCH_SIZE = 5;\n    org.apache.hadoop.hbase.rest.model.ScannerModel model = new org.apache.hadoop.hbase.rest.model.ScannerModel();\n    model.setBatch(BATCH_SIZE);\n    model.addColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestScannerResource.COLUMN_1));\n    java.io.StringWriter writer = new java.io.StringWriter();\n    marshaller.marshal(model, writer);\n    byte[] body = org.apache.hadoop.hbase.util.Bytes.toBytes(writer.toString());\n    org.apache.hadoop.hbase.rest.client.Response response = client.put((\"/\" + org.apache.hadoop.hbase.rest.TestScannerResource.TABLE) + \"/scanner\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML, body);\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    java.lang.String scannerURI = response.getLocation();\n    junit.framework.Assert.assertNotNull(scannerURI);\n    response = client.get(scannerURI, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.CellSetModel cellSet = ((org.apache.hadoop.hbase.rest.model.CellSetModel) (unmarshaller.unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    junit.framework.Assert.assertEquals(countCellSet(cellSet), BATCH_SIZE);\n    response = client.delete(scannerURI);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
        "CUT_1": {
            "Body": "{\n    final int BATCH_SIZE = 5;\n    org.apache.hadoop.hbase.rest.model.ScannerModel model = new org.apache.hadoop.hbase.rest.model.ScannerModel();\n    model.setBatch(BATCH_SIZE);\n    model.addColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestScannerResource.COLUMN_1));\n    java.io.StringWriter writer = new java.io.StringWriter();\n    marshaller.marshal(model, writer);\n    byte[] body = org.apache.hadoop.hbase.util.Bytes.toBytes(writer.toString());\n    org.apache.hadoop.hbase.rest.client.Response response = client.put((\"/\" + org.apache.hadoop.hbase.rest.TestScannerResource.TABLE) + \"/scanner\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML, body);\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    java.lang.String scannerURI = response.getLocation();\n    junit.framework.Assert.assertNotNull(scannerURI);\n    response = client.get(scannerURI, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.CellSetModel cellSet = ((org.apache.hadoop.hbase.rest.model.CellSetModel) (unmarshaller.unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    junit.framework.Assert.assertEquals(countCellSet(cellSet), BATCH_SIZE);\n    response = client.delete(scannerURI);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestScannerResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSimpleScannerXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 19,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    final int BATCH_SIZE = 10;\n    org.apache.hadoop.hbase.rest.model.ScannerModel model = new org.apache.hadoop.hbase.rest.model.ScannerModel();\n    model.setBatch(BATCH_SIZE);\n    model.addColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestScannerResource.COLUMN_1));\n    org.apache.hadoop.hbase.rest.client.Response response = client.put((\"/\" + org.apache.hadoop.hbase.rest.TestScannerResource.TABLE) + \"/scanner\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF, model.createProtobufOutput());\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    java.lang.String scannerURI = response.getLocation();\n    junit.framework.Assert.assertNotNull(scannerURI);\n    response = client.get(scannerURI, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.CellSetModel cellSet = new org.apache.hadoop.hbase.rest.model.CellSetModel();\n    cellSet.getObjectFromMessage(response.getBody());\n    junit.framework.Assert.assertEquals(countCellSet(cellSet), BATCH_SIZE);\n    response = client.delete(scannerURI);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestScannerResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSimpleScannerPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 17,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.model.ScannerModel model = org.apache.hadoop.hbase.rest.model.ScannerModel.fromScan(s);\n    model.setBatch(java.lang.Integer.MAX_VALUE);\n    java.io.StringWriter writer = new java.io.StringWriter();\n    marshaller.marshal(model, writer);\n    org.apache.hadoop.hbase.rest.TestScannersWithFilters.LOG.debug(writer.toString());\n    byte[] body = org.apache.hadoop.hbase.util.Bytes.toBytes(writer.toString());\n    org.apache.hadoop.hbase.rest.client.Response response = client.put((\"/\" + getName()) + \"/scanner\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML, body);\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    java.lang.String scannerURI = response.getLocation();\n    junit.framework.Assert.assertNotNull(scannerURI);\n    response = client.get(scannerURI, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.CellSetModel cells = ((org.apache.hadoop.hbase.rest.model.CellSetModel) (unmarshaller.unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    int rows = cells.getRows().size();\n    junit.framework.Assert.assertTrue(((\"Scanned too many rows! Only expected \" + expectedRows) + \" total but scanned \") + rows, expectedRows == rows);\n    for (org.apache.hadoop.hbase.rest.model.RowModel row : cells.getRows()) {\n        int count = row.getCells().size();\n        junit.framework.Assert.assertEquals((((\"Expected \" + expectedKeys) + \" keys per row but \") + \"returned \") + count, expectedKeys, count);\n    }\n    response = client.delete(scannerURI);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "verifyScan",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.model.ScannerModel model = org.apache.hadoop.hbase.rest.model.ScannerModel.fromScan(s);\n    model.setBatch(java.lang.Integer.MAX_VALUE);\n    java.io.StringWriter writer = new java.io.StringWriter();\n    marshaller.marshal(model, writer);\n    org.apache.hadoop.hbase.rest.TestScannersWithFilters.LOG.debug(writer.toString());\n    byte[] body = org.apache.hadoop.hbase.util.Bytes.toBytes(writer.toString());\n    org.apache.hadoop.hbase.rest.client.Response response = client.put((\"/\" + getName()) + \"/scanner\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML, body);\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    java.lang.String scannerURI = response.getLocation();\n    junit.framework.Assert.assertNotNull(scannerURI);\n    response = client.get(scannerURI, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.CellSetModel cellSet = ((org.apache.hadoop.hbase.rest.model.CellSetModel) (unmarshaller.unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    response = client.delete(scannerURI);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    java.util.Iterator<org.apache.hadoop.hbase.rest.model.RowModel> i = cellSet.getRows().iterator();\n    int j = 0;\n    for (boolean done = true; done; j++) {\n        done = i.hasNext();\n        if (!done)\n            break;\n\n        org.apache.hadoop.hbase.rest.model.RowModel rowModel = i.next();\n        java.util.List<org.apache.hadoop.hbase.rest.model.CellModel> cells = rowModel.getCells();\n        if (cells.isEmpty())\n            break;\n\n        junit.framework.Assert.assertTrue(((\"Scanned too many rows! Only expected \" + expectedRows) + \" total but already scanned \") + (j + 1), expectedRows > j);\n        junit.framework.Assert.assertEquals((((\"Expected \" + expectedKeys) + \" keys per row but \") + \"returned \") + cells.size(), expectedKeys, cells.size());\n    }\n    junit.framework.Assert.assertEquals((((\"Expected \" + expectedRows) + \" rows but scanned \") + j) + \" rows\", expectedRows, j);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "verifyScanNoEarlyOut",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 33,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    model.setBatch(100);\n    org.apache.hadoop.hbase.rest.client.Response response = client.put((\"/\" + org.apache.hadoop.hbase.rest.TestScannerResource.TABLE) + \"/scanner\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF, model.createProtobufOutput());\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    java.lang.String scannerURI = response.getLocation();\n    junit.framework.Assert.assertNotNull(scannerURI);\n    int count = 0;\n    while (true) {\n        response = client.get(scannerURI, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n        junit.framework.Assert.assertTrue((response.getCode() == 200) || (response.getCode() == 204));\n        if (response.getCode() == 200) {\n            org.apache.hadoop.hbase.rest.model.CellSetModel cellSet = new org.apache.hadoop.hbase.rest.model.CellSetModel();\n            cellSet.getObjectFromMessage(response.getBody());\n            java.util.Iterator<org.apache.hadoop.hbase.rest.model.RowModel> rows = cellSet.getRows().iterator();\n            while (rows.hasNext()) {\n                org.apache.hadoop.hbase.rest.model.RowModel row = rows.next();\n                java.util.Iterator<org.apache.hadoop.hbase.rest.model.CellModel> cells = row.getCells().iterator();\n                while (cells.hasNext()) {\n                    cells.next();\n                    count++;\n                } \n            } \n        } else {\n            break;\n        }\n    } \n    response = client.delete(scannerURI);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    return count;\n}",
            "ClassName": "TestScannerResource",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "fullTableScan",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 30,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestScannerResource",
        "Commit": "0b590b5703f22d8a4f831dc20108f136bb8448f0",
        "CyclomaticComplexity": 0,
        "Date": "Sat, 22 May 2010 04:39:06 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testSimpleScannerXML",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 19,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274503146"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n    org.apache.hadoop.hbase.rest.model.VersionModel model = new org.apache.hadoop.hbase.rest.model.VersionModel();\n    model.getObjectFromMessage(response.getBody());\n    org.apache.hadoop.hbase.rest.TestVersionResource.validate(model);\n    org.apache.hadoop.hbase.rest.TestVersionResource.LOG.info(\"success retrieving Stargate version as protobuf\");\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n    org.apache.hadoop.hbase.rest.model.VersionModel model = new org.apache.hadoop.hbase.rest.model.VersionModel();\n    model.getObjectFromMessage(response.getBody());\n    org.apache.hadoop.hbase.rest.TestVersionResource.validate(model);\n    org.apache.hadoop.hbase.rest.TestVersionResource.LOG.info(\"success retrieving Stargate version as protobuf\");\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStargateVersionPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n    org.apache.hadoop.hbase.rest.model.VersionModel model = ((org.apache.hadoop.hbase.rest.model.VersionModel) (context.createUnmarshaller().unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    org.apache.hadoop.hbase.rest.TestVersionResource.validate(model);\n    org.apache.hadoop.hbase.rest.TestVersionResource.LOG.info(\"success retrieving Stargate version as XML\");\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStargateVersionXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/status/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel model = new org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel();\n    model.getObjectFromMessage(response.getBody());\n    validate(model);\n}",
            "ClassName": "TestStatusResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetClusterStatusPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.TableListModel model = new org.apache.hadoop.hbase.rest.model.TableListModel();\n    model.getObjectFromMessage(response.getBody());\n    checkTableList(model);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableListPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/status/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel model = ((org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel) (context.createUnmarshaller().unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    validate(model);\n}",
            "ClassName": "TestStatusResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetClusterStatusXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestVersionResource",
        "Commit": "0b590b5703f22d8a4f831dc20108f136bb8448f0",
        "CyclomaticComplexity": 0,
        "Date": "Sat, 22 May 2010 04:39:06 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testGetStargateVersionPB",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 8,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274503146"
    },
    {
        "Body": "{\n    junit.framework.Assert.assertTrue(remoteAdmin.isTableAvailable(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.TABLE_2));\n    remoteAdmin.deleteTable(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.TABLE_2);\n    junit.framework.Assert.assertFalse(remoteAdmin.isTableAvailable(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.TABLE_2));\n}",
        "CUT_1": {
            "Body": "{\n    junit.framework.Assert.assertTrue(remoteAdmin.isTableAvailable(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.TABLE_2));\n    remoteAdmin.deleteTable(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.TABLE_2);\n    junit.framework.Assert.assertFalse(remoteAdmin.isTableAvailable(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.TABLE_2));\n}",
            "ClassName": "TestRemoteAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDeleteTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    junit.framework.Assert.assertFalse(remoteAdmin.isTableAvailable(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.TABLE_1));\n    remoteAdmin.createTable(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.DESC_1);\n    junit.framework.Assert.assertTrue(remoteAdmin.isTableAvailable(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.TABLE_1));\n}",
            "ClassName": "TestRemoteAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testCreateTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    super.setUp();\n    localAdmin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    remoteAdmin = new org.apache.hadoop.hbase.rest.client.RemoteAdmin(new org.apache.hadoop.hbase.rest.client.Client(new org.apache.hadoop.hbase.rest.client.Cluster().add(\"localhost\", testServletPort)), conf);\n    if (localAdmin.tableExists(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.TABLE_1)) {\n        localAdmin.disableTable(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.TABLE_1);\n        localAdmin.deleteTable(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.TABLE_1);\n    }\n    if (!localAdmin.tableExists(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.TABLE_2)) {\n        localAdmin.createTable(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.DESC_2);\n    }\n}",
            "ClassName": "TestRemoteAdmin",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor tableDescriptor = getTableDescriptor();\n    org.apache.hadoop.hbase.rest.client.RemoteAdmin admin = new org.apache.hadoop.hbase.rest.client.RemoteAdmin(new org.apache.hadoop.hbase.rest.client.Client(org.apache.hadoop.hbase.rest.PerformanceEvaluation.cluster), conf, org.apache.hadoop.hbase.rest.PerformanceEvaluation.accessToken);\n    if (!admin.isTableAvailable(tableDescriptor.getName())) {\n        admin.createTable(tableDescriptor);\n        return true;\n    }\n    return false;\n}",
            "ClassName": "PerformanceEvaluation",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "checkTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return isTableAvailable(org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n}",
            "ClassName": "RemoteAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "isTableAvailable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestRemoteAdmin",
        "Commit": "0b590b5703f22d8a4f831dc20108f136bb8448f0",
        "CyclomaticComplexity": 0,
        "Date": "Sat, 22 May 2010 04:39:06 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testDeleteTable",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 5,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274503146"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_JSON);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_JSON);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStorageClusterVersionJSON",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_JSON);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStargateVersionJSON",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_JSON);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableListJSON",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_TEXT);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStorageClusterVersionText",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_TEXT);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableListText",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestVersionResource",
        "Commit": "0b590b5703f22d8a4f831dc20108f136bb8448f0",
        "CyclomaticComplexity": 0,
        "Date": "Sat, 22 May 2010 04:39:06 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testGetStorageClusterVersionJSON",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 4,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274503146"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter());\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter());\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testFirstKeyOnlyFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    long expectedRows = this.numRows / 2;\n    long expectedKeys = 2;\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 2;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 4;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 4;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo\"));\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 4;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo\"));\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows / 2;\n    expectedKeys = 2;\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowTwo\"));\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2]));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n    f = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\"test.+-2\"));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testQualifierFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 48,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.KeyValue[] expectedKVs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    long expectedRows = 6;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, expectedKVs);\n    expectedRows = 4;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, java.util.Arrays.copyOf(expectedKVs, 24));\n    expectedRows = 2;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, java.util.Arrays.copyOf(expectedKVs, 12));\n    expectedRows = 1;\n    expectedKeys = this.colsPerRow;\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScan(s, expectedRows, expectedKeys);\n    s.setFilter(new org.apache.hadoop.hbase.filter.PageFilter(expectedRows));\n    verifyScanFull(s, java.util.Arrays.copyOf(expectedKVs, 6));\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testPageFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 31,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    long expectedRows = 1;\n    long expectedKeys = this.colsPerRow;\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\"testRow.+-2\"));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = 1;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows - 1;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows - 1;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    expectedRows = this.numRows - 2;\n    expectedKeys = this.colsPerRow;\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, expectedRows, expectedKeys);\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n    f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\".+-2\"));\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testRowFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 54,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.SkipFilter(new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testQualifierOne-2\"))));\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]), new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[1], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_TWO[3], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[1]) };\n    verifyScanFull(s, kvs);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSkipFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestScannersWithFilters",
        "Commit": "0b590b5703f22d8a4f831dc20108f136bb8448f0",
        "CyclomaticComplexity": 0,
        "Date": "Sat, 22 May 2010 04:39:06 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testFirstKeyOnlyFilter",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 6,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274503146"
    },
    {
        "Body": "{\n    final int BATCH_SIZE = 10;\n    org.apache.hadoop.hbase.rest.model.ScannerModel model = new org.apache.hadoop.hbase.rest.model.ScannerModel();\n    model.setBatch(BATCH_SIZE);\n    model.addColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestScannerResource.COLUMN_1));\n    org.apache.hadoop.hbase.rest.client.Response response = client.put((\"/\" + org.apache.hadoop.hbase.rest.TestScannerResource.TABLE) + \"/scanner\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF, model.createProtobufOutput());\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    java.lang.String scannerURI = response.getLocation();\n    junit.framework.Assert.assertNotNull(scannerURI);\n    response = client.get(scannerURI, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.CellSetModel cellSet = new org.apache.hadoop.hbase.rest.model.CellSetModel();\n    cellSet.getObjectFromMessage(response.getBody());\n    junit.framework.Assert.assertEquals(countCellSet(cellSet), BATCH_SIZE);\n    response = client.delete(scannerURI);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
        "CUT_1": {
            "Body": "{\n    final int BATCH_SIZE = 10;\n    org.apache.hadoop.hbase.rest.model.ScannerModel model = new org.apache.hadoop.hbase.rest.model.ScannerModel();\n    model.setBatch(BATCH_SIZE);\n    model.addColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestScannerResource.COLUMN_1));\n    org.apache.hadoop.hbase.rest.client.Response response = client.put((\"/\" + org.apache.hadoop.hbase.rest.TestScannerResource.TABLE) + \"/scanner\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF, model.createProtobufOutput());\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    java.lang.String scannerURI = response.getLocation();\n    junit.framework.Assert.assertNotNull(scannerURI);\n    response = client.get(scannerURI, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.CellSetModel cellSet = new org.apache.hadoop.hbase.rest.model.CellSetModel();\n    cellSet.getObjectFromMessage(response.getBody());\n    junit.framework.Assert.assertEquals(countCellSet(cellSet), BATCH_SIZE);\n    response = client.delete(scannerURI);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestScannerResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSimpleScannerPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 17,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    final int BATCH_SIZE = 5;\n    org.apache.hadoop.hbase.rest.model.ScannerModel model = new org.apache.hadoop.hbase.rest.model.ScannerModel();\n    model.setBatch(BATCH_SIZE);\n    model.addColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestScannerResource.COLUMN_1));\n    java.io.StringWriter writer = new java.io.StringWriter();\n    marshaller.marshal(model, writer);\n    byte[] body = org.apache.hadoop.hbase.util.Bytes.toBytes(writer.toString());\n    org.apache.hadoop.hbase.rest.client.Response response = client.put((\"/\" + org.apache.hadoop.hbase.rest.TestScannerResource.TABLE) + \"/scanner\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML, body);\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    java.lang.String scannerURI = response.getLocation();\n    junit.framework.Assert.assertNotNull(scannerURI);\n    response = client.get(scannerURI, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.CellSetModel cellSet = ((org.apache.hadoop.hbase.rest.model.CellSetModel) (unmarshaller.unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    junit.framework.Assert.assertEquals(countCellSet(cellSet), BATCH_SIZE);\n    response = client.delete(scannerURI);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestScannerResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSimpleScannerXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 19,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    model.setBatch(100);\n    org.apache.hadoop.hbase.rest.client.Response response = client.put((\"/\" + org.apache.hadoop.hbase.rest.TestScannerResource.TABLE) + \"/scanner\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF, model.createProtobufOutput());\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    java.lang.String scannerURI = response.getLocation();\n    junit.framework.Assert.assertNotNull(scannerURI);\n    int count = 0;\n    while (true) {\n        response = client.get(scannerURI, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n        junit.framework.Assert.assertTrue((response.getCode() == 200) || (response.getCode() == 204));\n        if (response.getCode() == 200) {\n            org.apache.hadoop.hbase.rest.model.CellSetModel cellSet = new org.apache.hadoop.hbase.rest.model.CellSetModel();\n            cellSet.getObjectFromMessage(response.getBody());\n            java.util.Iterator<org.apache.hadoop.hbase.rest.model.RowModel> rows = cellSet.getRows().iterator();\n            while (rows.hasNext()) {\n                org.apache.hadoop.hbase.rest.model.RowModel row = rows.next();\n                java.util.Iterator<org.apache.hadoop.hbase.rest.model.CellModel> cells = row.getCells().iterator();\n                while (cells.hasNext()) {\n                    cells.next();\n                    count++;\n                } \n            } \n        } else {\n            break;\n        }\n    } \n    response = client.delete(scannerURI);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    return count;\n}",
            "ClassName": "TestScannerResource",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "fullTableScan",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 30,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.model.ScannerModel model = org.apache.hadoop.hbase.rest.model.ScannerModel.fromScan(s);\n    model.setBatch(java.lang.Integer.MAX_VALUE);\n    java.io.StringWriter writer = new java.io.StringWriter();\n    marshaller.marshal(model, writer);\n    org.apache.hadoop.hbase.rest.TestScannersWithFilters.LOG.debug(writer.toString());\n    byte[] body = org.apache.hadoop.hbase.util.Bytes.toBytes(writer.toString());\n    org.apache.hadoop.hbase.rest.client.Response response = client.put((\"/\" + getName()) + \"/scanner\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML, body);\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    java.lang.String scannerURI = response.getLocation();\n    junit.framework.Assert.assertNotNull(scannerURI);\n    response = client.get(scannerURI, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.CellSetModel cells = ((org.apache.hadoop.hbase.rest.model.CellSetModel) (unmarshaller.unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    int rows = cells.getRows().size();\n    junit.framework.Assert.assertTrue(((\"Scanned too many rows! Only expected \" + expectedRows) + \" total but scanned \") + rows, expectedRows == rows);\n    for (org.apache.hadoop.hbase.rest.model.RowModel row : cells.getRows()) {\n        int count = row.getCells().size();\n        junit.framework.Assert.assertEquals((((\"Expected \" + expectedKeys) + \" keys per row but \") + \"returned \") + count, expectedKeys, count);\n    }\n    response = client.delete(scannerURI);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "verifyScan",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/status/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel model = new org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel();\n    model.getObjectFromMessage(response.getBody());\n    validate(model);\n}",
            "ClassName": "TestStatusResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetClusterStatusPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestScannerResource",
        "Commit": "0b590b5703f22d8a4f831dc20108f136bb8448f0",
        "CyclomaticComplexity": 0,
        "Date": "Sat, 22 May 2010 04:39:06 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testSimpleScannerPB",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 17,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274503146"
    },
    {
        "Body": "{\n    for (int i = 0; i < regions.length; i++) {\n        for (int j = 0; j < rows[i].length; j++) {\n            org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(rows[i][j]);\n            get.addFamily(org.apache.hadoop.hbase.util.TestMergeTool.FAMILY);\n            org.apache.hadoop.hbase.client.Result result = regions[i].get(get, null);\n            byte[] bytes = result.sorted()[0].getValue();\n            junit.framework.Assert.assertNotNull(bytes);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(bytes, rows[i][j]));\n        }\n        regions[i].close();\n        regions[i].getLog().closeAndDelete();\n    }\n    org.apache.hadoop.fs.Path logPath = new org.apache.hadoop.fs.Path(\"/tmp\", (org.apache.hadoop.hbase.HConstants.HREGION_LOGDIR_NAME + \"_\") + java.lang.System.currentTimeMillis());\n    org.apache.hadoop.hbase.util.TestMergeTool.LOG.info(\"Creating log \" + logPath.toString());\n    org.apache.hadoop.fs.Path oldLogDir = new org.apache.hadoop.fs.Path(\"/tmp\", org.apache.hadoop.hbase.HConstants.HREGION_OLDLOGDIR_NAME);\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(this.fs, logPath, oldLogDir, this.conf, null);\n    try {\n        org.apache.hadoop.hbase.regionserver.HRegion merged = mergeAndVerify(\"merging regions 0 and 1\", this.sourceRegions[0].getRegionNameAsString(), this.sourceRegions[1].getRegionNameAsString(), log, 2);\n        merged = mergeAndVerify(\"merging regions 0+1 and 2\", merged.getRegionInfo().getRegionNameAsString(), this.sourceRegions[2].getRegionNameAsString(), log, 3);\n        merged = mergeAndVerify(\"merging regions 0+1+2 and 3\", merged.getRegionInfo().getRegionNameAsString(), this.sourceRegions[3].getRegionNameAsString(), log, 4);\n        merged = mergeAndVerify(\"merging regions 0+1+2+3 and 4\", merged.getRegionInfo().getRegionNameAsString(), this.sourceRegions[4].getRegionNameAsString(), log, rows.length);\n    } finally {\n        log.closeAndDelete();\n    }\n}",
        "CUT_1": {
            "Body": "{\n    for (int i = 0; i < regions.length; i++) {\n        for (int j = 0; j < rows[i].length; j++) {\n            org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(rows[i][j]);\n            get.addFamily(org.apache.hadoop.hbase.util.TestMergeTool.FAMILY);\n            org.apache.hadoop.hbase.client.Result result = regions[i].get(get, null);\n            byte[] bytes = result.sorted()[0].getValue();\n            junit.framework.Assert.assertNotNull(bytes);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(bytes, rows[i][j]));\n        }\n        regions[i].close();\n        regions[i].getLog().closeAndDelete();\n    }\n    org.apache.hadoop.fs.Path logPath = new org.apache.hadoop.fs.Path(\"/tmp\", (org.apache.hadoop.hbase.HConstants.HREGION_LOGDIR_NAME + \"_\") + java.lang.System.currentTimeMillis());\n    org.apache.hadoop.hbase.util.TestMergeTool.LOG.info(\"Creating log \" + logPath.toString());\n    org.apache.hadoop.fs.Path oldLogDir = new org.apache.hadoop.fs.Path(\"/tmp\", org.apache.hadoop.hbase.HConstants.HREGION_OLDLOGDIR_NAME);\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(this.fs, logPath, oldLogDir, this.conf, null);\n    try {\n        org.apache.hadoop.hbase.regionserver.HRegion merged = mergeAndVerify(\"merging regions 0 and 1\", this.sourceRegions[0].getRegionNameAsString(), this.sourceRegions[1].getRegionNameAsString(), log, 2);\n        merged = mergeAndVerify(\"merging regions 0+1 and 2\", merged.getRegionInfo().getRegionNameAsString(), this.sourceRegions[2].getRegionNameAsString(), log, 3);\n        merged = mergeAndVerify(\"merging regions 0+1+2 and 3\", merged.getRegionInfo().getRegionNameAsString(), this.sourceRegions[3].getRegionNameAsString(), log, 4);\n        merged = mergeAndVerify(\"merging regions 0+1+2+3 and 4\", merged.getRegionInfo().getRegionNameAsString(), this.sourceRegions[4].getRegionNameAsString(), log, rows.length);\n    } finally {\n        log.closeAndDelete();\n    }\n}",
            "ClassName": "TestMergeTool",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testMergeTool",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 26,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.util.TestMergeTool.FAMILY);\n    org.apache.hadoop.hbase.regionserver.InternalScanner scanner = merged.getScanner(scan);\n    try {\n        java.util.List<org.apache.hadoop.hbase.KeyValue> testRes = null;\n        while (true) {\n            testRes = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n            boolean hasNext = scanner.next(testRes);\n            if (!hasNext) {\n                break;\n            }\n        } \n    } finally {\n        scanner.close();\n    }\n    for (int i = 0; i < upperbound; i++) {\n        for (int j = 0; j < rows[i].length; j++) {\n            org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(rows[i][j]);\n            get.addFamily(org.apache.hadoop.hbase.util.TestMergeTool.FAMILY);\n            org.apache.hadoop.hbase.client.Result result = merged.get(get, null);\n            junit.framework.Assert.assertEquals(1, result.size());\n            byte[] bytes = result.sorted()[0].getValue();\n            junit.framework.Assert.assertNotNull(org.apache.hadoop.hbase.util.Bytes.toStringBinary(rows[i][j]), bytes);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(bytes, rows[i][j]));\n        }\n    }\n}",
            "ClassName": "TestMergeTool",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "verifyMerge",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 28,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    this.conf.set(\"hbase.hstore.compactionThreshold\", \"2\");\n    this.desc = new org.apache.hadoop.hbase.HTableDescriptor(\"TestMergeTool\");\n    this.desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.util.TestMergeTool.FAMILY));\n    sourceRegions[0] = new org.apache.hadoop.hbase.HRegionInfo(this.desc, org.apache.hadoop.hbase.util.Bytes.toBytes(\"row_0200\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"row_0300\"));\n    sourceRegions[1] = new org.apache.hadoop.hbase.HRegionInfo(this.desc, org.apache.hadoop.hbase.util.Bytes.toBytes(\"row_0250\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"row_0400\"));\n    sourceRegions[2] = new org.apache.hadoop.hbase.HRegionInfo(this.desc, org.apache.hadoop.hbase.util.Bytes.toBytes(\"row_0100\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"row_0200\"));\n    sourceRegions[3] = new org.apache.hadoop.hbase.HRegionInfo(this.desc, org.apache.hadoop.hbase.util.Bytes.toBytes(\"row_0500\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"row_0600\"));\n    sourceRegions[4] = new org.apache.hadoop.hbase.HRegionInfo(this.desc, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY);\n    this.rows = new byte[5][][];\n    this.rows[0] = org.apache.hadoop.hbase.util.Bytes.toByteArrays(new java.lang.String[]{ \"row_0210\", \"row_0280\" });\n    this.rows[1] = org.apache.hadoop.hbase.util.Bytes.toByteArrays(new java.lang.String[]{ \"row_0260\", \"row_0350\", \"row_035\" });\n    this.rows[2] = org.apache.hadoop.hbase.util.Bytes.toByteArrays(new java.lang.String[]{ \"row_0110\", \"row_0175\", \"row_0175\", \"row_0175\" });\n    this.rows[3] = org.apache.hadoop.hbase.util.Bytes.toByteArrays(new java.lang.String[]{ \"row_0525\", \"row_0560\", \"row_0560\", \"row_0560\", \"row_0560\" });\n    this.rows[4] = org.apache.hadoop.hbase.util.Bytes.toByteArrays(new java.lang.String[]{ \"row_0050\", \"row_1000\", \"row_1000\", \"row_1000\", \"row_1000\", \"row_1000\" });\n    this.dfsCluster = new org.apache.hadoop.hdfs.MiniDFSCluster(conf, 2, true, ((java.lang.String[]) (null)));\n    this.fs = this.dfsCluster.getFileSystem();\n    java.lang.System.out.println(\"fs=\" + this.fs);\n    this.conf.set(\"fs.defaultFS\", fs.getUri().toString());\n    org.apache.hadoop.fs.Path parentdir = fs.getHomeDirectory();\n    conf.set(org.apache.hadoop.hbase.HConstants.HBASE_DIR, parentdir.toString());\n    fs.mkdirs(parentdir);\n    org.apache.hadoop.hbase.util.FSUtils.setVersion(fs, parentdir);\n    super.setUp();\n    try {\n        createRootAndMetaRegions();\n        for (int i = 0; i < sourceRegions.length; i++) {\n            regions[i] = org.apache.hadoop.hbase.regionserver.HRegion.createHRegion(this.sourceRegions[i], this.testDir, this.conf);\n            for (int j = 0; j < rows[i].length; j++) {\n                byte[] row = rows[i][j];\n                org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n                put.add(org.apache.hadoop.hbase.util.TestMergeTool.FAMILY, org.apache.hadoop.hbase.util.TestMergeTool.QUALIFIER, row);\n                regions[i].put(put);\n            }\n            org.apache.hadoop.hbase.regionserver.HRegion.addRegionToMETA(meta, regions[i]);\n        }\n        closeRootAndMeta();\n    } catch (java.lang.Exception e) {\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(dfsCluster);\n        throw e;\n    }\n}",
            "ClassName": "TestMergeTool",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 42,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    if (this.log == null) {\n        org.apache.hadoop.fs.Path logdir = new org.apache.hadoop.fs.Path(this.fs.getHomeDirectory(), (org.apache.hadoop.hbase.HConstants.HREGION_LOGDIR_NAME + \"_\") + java.lang.System.currentTimeMillis());\n        org.apache.hadoop.fs.Path oldLogDir = new org.apache.hadoop.fs.Path(this.fs.getHomeDirectory(), org.apache.hadoop.hbase.HConstants.HREGION_OLDLOGDIR_NAME);\n        this.log = new org.apache.hadoop.hbase.regionserver.wal.HLog(this.fs, logdir, oldLogDir, this.conf, null);\n    }\n    return this.log;\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getLog",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    testDir = new org.apache.hadoop.fs.Path(conf.get(org.apache.hadoop.hbase.HConstants.HBASE_DIR));\n    this.desc = new org.apache.hadoop.hbase.HTableDescriptor(TABLE_NAME);\n    this.desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    INFOS[0] = new org.apache.hadoop.hbase.HRegionInfo(this.desc, org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.util.Bytes.toBytes(\"row_0500\"));\n    INFOS[1] = new org.apache.hadoop.hbase.HRegionInfo(this.desc, org.apache.hadoop.hbase.util.Bytes.toBytes(\"row_0500\"), org.apache.hadoop.hbase.HConstants.EMPTY_END_ROW);\n    createRootAndMetaRegions();\n    for (int i = 0; i < REGIONS.length; i++) {\n        REGIONS[i] = org.apache.hadoop.hbase.regionserver.HRegion.createHRegion(this.INFOS[i], this.testDir, this.conf);\n        for (int j = 0; j < TIMESTAMPS.length; j++) {\n            org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(ROWS[i], TIMESTAMPS[j], null);\n            put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, TIMESTAMPS[j], org.apache.hadoop.hbase.util.Bytes.toBytes(TIMESTAMPS[j]));\n            REGIONS[i].put(put);\n        }\n        org.apache.hadoop.hbase.regionserver.HRegion.addRegionToMETA(meta, REGIONS[i]);\n        REGIONS[i].close();\n        REGIONS[i].getLog().closeAndDelete();\n    }\n    closeRootAndMeta();\n}",
            "ClassName": "TestScanMultipleVersions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "preHBaseClusterSetup",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 20,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestMergeTool",
        "Commit": "0b590b5703f22d8a4f831dc20108f136bb8448f0",
        "CyclomaticComplexity": 2,
        "Date": "Sat, 22 May 2010 04:39:06 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testMergeTool",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 26,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274503146"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/status/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel model = new org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel();\n    model.getObjectFromMessage(response.getBody());\n    validate(model);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/status/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel model = new org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel();\n    model.getObjectFromMessage(response.getBody());\n    validate(model);\n}",
            "ClassName": "TestStatusResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetClusterStatusPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/status/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel model = ((org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel) (context.createUnmarshaller().unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    validate(model);\n}",
            "ClassName": "TestStatusResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetClusterStatusXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.TableListModel model = new org.apache.hadoop.hbase.rest.model.TableListModel();\n    model.getObjectFromMessage(response.getBody());\n    checkTableList(model);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableListPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n    org.apache.hadoop.hbase.rest.model.VersionModel model = new org.apache.hadoop.hbase.rest.model.VersionModel();\n    model.getObjectFromMessage(response.getBody());\n    org.apache.hadoop.hbase.rest.TestVersionResource.validate(model);\n    org.apache.hadoop.hbase.rest.TestVersionResource.LOG.info(\"success retrieving Stargate version as protobuf\");\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStargateVersionPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.TableListModel model = ((org.apache.hadoop.hbase.rest.model.TableListModel) (context.createUnmarshaller().unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    checkTableList(model);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableListXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestStatusResource",
        "Commit": "0b590b5703f22d8a4f831dc20108f136bb8448f0",
        "CyclomaticComplexity": 0,
        "Date": "Sat, 22 May 2010 04:39:06 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testGetClusterStatusPB",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 7,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274503146"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/status/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel model = ((org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel) (context.createUnmarshaller().unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    validate(model);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/status/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel model = ((org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel) (context.createUnmarshaller().unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    validate(model);\n}",
            "ClassName": "TestStatusResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetClusterStatusXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/status/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel model = new org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel();\n    model.getObjectFromMessage(response.getBody());\n    validate(model);\n}",
            "ClassName": "TestStatusResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetClusterStatusPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.TableListModel model = ((org.apache.hadoop.hbase.rest.model.TableListModel) (context.createUnmarshaller().unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    checkTableList(model);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableListXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n    org.apache.hadoop.hbase.rest.model.VersionModel model = ((org.apache.hadoop.hbase.rest.model.VersionModel) (context.createUnmarshaller().unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    org.apache.hadoop.hbase.rest.TestVersionResource.validate(model);\n    org.apache.hadoop.hbase.rest.TestVersionResource.LOG.info(\"success retrieving Stargate version as XML\");\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStargateVersionXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.TableListModel model = new org.apache.hadoop.hbase.rest.model.TableListModel();\n    model.getObjectFromMessage(response.getBody());\n    checkTableList(model);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableListPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestStatusResource",
        "Commit": "0b590b5703f22d8a4f831dc20108f136bb8448f0",
        "CyclomaticComplexity": 0,
        "Date": "Sat, 22 May 2010 04:39:06 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testGetClusterStatusXML",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 6,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274503146"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_TEXT);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_TEXT);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStorageClusterVersionText",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_TEXT);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableListText",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_JSON);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStorageClusterVersionJSON",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_JSON);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStargateVersionJSON",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_JSON);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableListJSON",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestVersionResource",
        "Commit": "0b590b5703f22d8a4f831dc20108f136bb8448f0",
        "CyclomaticComplexity": 0,
        "Date": "Sat, 22 May 2010 04:39:06 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testGetStorageClusterVersionText",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 4,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274503146"
    },
    {
        "Body": "{\n    junit.framework.Assert.assertNotNull(dfsCluster);\n    org.apache.hadoop.hbase.HMerge.merge(conf, dfsCluster.getFileSystem(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n}",
        "CUT_1": {
            "Body": "{\n    junit.framework.Assert.assertNotNull(dfsCluster);\n    org.apache.hadoop.hbase.HMerge.merge(conf, dfsCluster.getFileSystem(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n}",
            "ClassName": "TestMergeMeta",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testMergeMeta",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    junit.framework.Assert.assertNotNull(dfsCluster);\n    org.apache.hadoop.hbase.HMerge.merge(conf, dfsCluster.getFileSystem(), desc.getName());\n}",
            "ClassName": "TestMergeTable",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testMergeTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return dfsCluster;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getDFSCluster",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    if (this.dfsCluster != null) {\n        this.dfsCluster.shutdown();\n    }\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "shutdownMiniDFSCluster",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    super.tearDown();\n    org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(dfsCluster);\n}",
            "ClassName": "TestMergeTool",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDown",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestMergeMeta",
        "Commit": "0b590b5703f22d8a4f831dc20108f136bb8448f0",
        "CyclomaticComplexity": 0,
        "Date": "Sat, 22 May 2010 04:39:06 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testMergeMeta",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 4,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274503146"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n    org.apache.hadoop.hbase.rest.model.VersionModel model = ((org.apache.hadoop.hbase.rest.model.VersionModel) (context.createUnmarshaller().unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    org.apache.hadoop.hbase.rest.TestVersionResource.validate(model);\n    org.apache.hadoop.hbase.rest.TestVersionResource.LOG.info(\"success retrieving Stargate version as XML\");\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n    org.apache.hadoop.hbase.rest.model.VersionModel model = ((org.apache.hadoop.hbase.rest.model.VersionModel) (context.createUnmarshaller().unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    org.apache.hadoop.hbase.rest.TestVersionResource.validate(model);\n    org.apache.hadoop.hbase.rest.TestVersionResource.LOG.info(\"success retrieving Stargate version as XML\");\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStargateVersionXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n    org.apache.hadoop.hbase.rest.model.VersionModel model = new org.apache.hadoop.hbase.rest.model.VersionModel();\n    model.getObjectFromMessage(response.getBody());\n    org.apache.hadoop.hbase.rest.TestVersionResource.validate(model);\n    org.apache.hadoop.hbase.rest.TestVersionResource.LOG.info(\"success retrieving Stargate version as protobuf\");\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStargateVersionPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/status/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel model = ((org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel) (context.createUnmarshaller().unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    validate(model);\n}",
            "ClassName": "TestStatusResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetClusterStatusXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.TableListModel model = ((org.apache.hadoop.hbase.rest.model.TableListModel) (context.createUnmarshaller().unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    checkTableList(model);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableListXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n    org.apache.hadoop.hbase.rest.model.StorageClusterVersionModel clusterVersionModel = ((org.apache.hadoop.hbase.rest.model.StorageClusterVersionModel) (context.createUnmarshaller().unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    junit.framework.Assert.assertNotNull(clusterVersionModel);\n    junit.framework.Assert.assertNotNull(clusterVersionModel.getVersion());\n    org.apache.hadoop.hbase.rest.TestVersionResource.LOG.info(\"success retrieving storage cluster version as XML\");\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStorageClusterVersionXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestVersionResource",
        "Commit": "0b590b5703f22d8a4f831dc20108f136bb8448f0",
        "CyclomaticComplexity": 0,
        "Date": "Sat, 22 May 2010 04:39:06 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testGetStargateVersionXML",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 7,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274503146"
    },
    {
        "Body": "{\n    java.util.List<org.apache.hadoop.hbase.filter.Filter> filters = new java.util.ArrayList<org.apache.hadoop.hbase.filter.Filter>();\n    filters.add(new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\".+-2\")));\n    filters.add(new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\".+-2\")));\n    filters.add(new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.SubstringComparator(\"One\")));\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.FilterList(org.apache.hadoop.hbase.filter.FilterList.Operator.MUST_PASS_ALL, filters);\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.addFamily(org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0]);\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]) };\n    verifyScanFull(s, kvs);\n    filters.clear();\n    filters.add(new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\".+Two.+\")));\n    filters.add(new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\".+-2\")));\n    filters.add(new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.SubstringComparator(\"One\")));\n    f = new org.apache.hadoop.hbase.filter.FilterList(org.apache.hadoop.hbase.filter.FilterList.Operator.MUST_PASS_ONE, filters);\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, this.numRows, this.colsPerRow);\n}",
        "CUT_1": {
            "Body": "{\n    java.util.List<org.apache.hadoop.hbase.filter.Filter> filters = new java.util.ArrayList<org.apache.hadoop.hbase.filter.Filter>();\n    filters.add(new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\".+-2\")));\n    filters.add(new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\".+-2\")));\n    filters.add(new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.SubstringComparator(\"One\")));\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.FilterList(org.apache.hadoop.hbase.filter.FilterList.Operator.MUST_PASS_ALL, filters);\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.addFamily(org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0]);\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.rest.TestScannersWithFilters.ROWS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.FAMILIES[0], org.apache.hadoop.hbase.rest.TestScannersWithFilters.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.rest.TestScannersWithFilters.VALUES[0]) };\n    verifyScanFull(s, kvs);\n    filters.clear();\n    filters.add(new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\".+Two.+\")));\n    filters.add(new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\".+-2\")));\n    filters.add(new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.SubstringComparator(\"One\")));\n    f = new org.apache.hadoop.hbase.filter.FilterList(org.apache.hadoop.hbase.filter.FilterList.Operator.MUST_PASS_ONE, filters);\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, this.numRows, this.colsPerRow);\n}",
            "ClassName": "TestScannersWithFilters",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testFilterList",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 20,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    java.util.List<org.apache.hadoop.hbase.filter.Filter> filters = new java.util.ArrayList<org.apache.hadoop.hbase.filter.Filter>();\n    filters.add(new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\".+-2\")));\n    filters.add(new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\".+-2\")));\n    filters.add(new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.SubstringComparator(\"One\")));\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.FilterList(org.apache.hadoop.hbase.filter.FilterList.Operator.MUST_PASS_ALL, filters);\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    s.addFamily(org.apache.hadoop.hbase.filter.TestFilter.FAMILIES[0]);\n    s.setFilter(f);\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.filter.TestFilter.ROWS_ONE[2], org.apache.hadoop.hbase.filter.TestFilter.FAMILIES[0], org.apache.hadoop.hbase.filter.TestFilter.QUALIFIERS_ONE[2], org.apache.hadoop.hbase.filter.TestFilter.VALUES[0]) };\n    verifyScanFull(s, kvs);\n    filters.clear();\n    filters.add(new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\".+Two.+\")));\n    filters.add(new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\".+-2\")));\n    filters.add(new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.SubstringComparator(\"One\")));\n    f = new org.apache.hadoop.hbase.filter.FilterList(org.apache.hadoop.hbase.filter.FilterList.Operator.MUST_PASS_ONE, filters);\n    s = new org.apache.hadoop.hbase.client.Scan();\n    s.setFilter(f);\n    verifyScanNoEarlyOut(s, this.numRows, this.colsPerRow);\n}",
            "ClassName": "TestFilter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testFilterList",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 20,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.filter.Filter f = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowOne-2\")));\n    byte[] bytes = org.apache.hadoop.hbase.util.Writables.getBytes(f);\n    org.apache.hadoop.hbase.filter.Filter ff = ((org.apache.hadoop.hbase.filter.Filter) (org.apache.hadoop.hbase.util.Writables.getWritable(bytes, new org.apache.hadoop.hbase.filter.RowFilter())));\n    junit.framework.Assert.assertNotNull(ff);\n}",
            "ClassName": "TestSerialization",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testCompareFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    java.util.List<org.apache.hadoop.hbase.filter.Filter> filters = new java.util.ArrayList<org.apache.hadoop.hbase.filter.Filter>();\n    filters.add(new org.apache.hadoop.hbase.filter.PageFilter(org.apache.hadoop.hbase.filter.TestFilterList.MAX_PAGES));\n    filters.add(new org.apache.hadoop.hbase.filter.WhileMatchFilter(new org.apache.hadoop.hbase.filter.PrefixFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(\"yyy\"))));\n    org.apache.hadoop.hbase.filter.Filter filterMPALL = new org.apache.hadoop.hbase.filter.FilterList(org.apache.hadoop.hbase.filter.FilterList.Operator.MUST_PASS_ALL, filters);\n    java.io.ByteArrayOutputStream stream = new java.io.ByteArrayOutputStream();\n    java.io.DataOutputStream out = new java.io.DataOutputStream(stream);\n    filterMPALL.write(out);\n    out.close();\n    byte[] buffer = stream.toByteArray();\n    java.io.DataInputStream in = new java.io.DataInputStream(new java.io.ByteArrayInputStream(buffer));\n    org.apache.hadoop.hbase.filter.FilterList newFilter = new org.apache.hadoop.hbase.filter.FilterList();\n    newFilter.readFields(in);\n}",
            "ClassName": "TestFilterList",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSerialization",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 14,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan s = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.filter.WhileMatchFilter filter = new org.apache.hadoop.hbase.filter.WhileMatchFilter(new org.apache.hadoop.hbase.filter.SingleColumnValueFilter(org.apache.hadoop.hbase.filter.TestFilter.FAMILIES[0], org.apache.hadoop.hbase.filter.TestFilter.QUALIFIERS_ONE[0], org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, org.apache.hadoop.hbase.util.Bytes.toBytes(\"foo\")));\n    s.setFilter(filter);\n    org.apache.hadoop.hbase.regionserver.InternalScanner scanner = this.region.getScanner(s);\n    while (true) {\n        java.util.ArrayList<org.apache.hadoop.hbase.KeyValue> values = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        boolean isMoreResults = scanner.next(values);\n        junit.framework.Assert.assertTrue(\"The WhileMatchFilter should now filter all remaining\", filter.filterAllRemaining());\n        if (!isMoreResults) {\n            break;\n        }\n    } \n}",
            "ClassName": "TestFilter",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testWhileMatchFilterWithFilterKeyValue",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 14,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestScannersWithFilters",
        "Commit": "0b590b5703f22d8a4f831dc20108f136bb8448f0",
        "CyclomaticComplexity": 0,
        "Date": "Sat, 22 May 2010 04:39:06 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testFilterList",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 20,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274503146"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.rest.model.ScannerModel model = new org.apache.hadoop.hbase.rest.model.ScannerModel();\n    model.addColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestScannerResource.COLUMN_1));\n    junit.framework.Assert.assertEquals(fullTableScan(model), org.apache.hadoop.hbase.rest.TestScannerResource.expectedRows1);\n    model = new org.apache.hadoop.hbase.rest.model.ScannerModel();\n    model.addColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestScannerResource.COLUMN_2));\n    junit.framework.Assert.assertEquals(fullTableScan(model), org.apache.hadoop.hbase.rest.TestScannerResource.expectedRows2);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.model.ScannerModel model = new org.apache.hadoop.hbase.rest.model.ScannerModel();\n    model.addColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestScannerResource.COLUMN_1));\n    junit.framework.Assert.assertEquals(fullTableScan(model), org.apache.hadoop.hbase.rest.TestScannerResource.expectedRows1);\n    model = new org.apache.hadoop.hbase.rest.model.ScannerModel();\n    model.addColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestScannerResource.COLUMN_2));\n    junit.framework.Assert.assertEquals(fullTableScan(model), org.apache.hadoop.hbase.rest.TestScannerResource.expectedRows2);\n}",
            "ClassName": "TestScannerResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testFullTableScan",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return ((org.apache.hadoop.hbase.rest.model.ScannerModel) (new org.apache.hadoop.hbase.rest.model.ScannerModel().getObjectFromMessage(org.apache.hadoop.hbase.util.Base64.decode(org.apache.hadoop.hbase.rest.model.TestScannerModel.AS_PB))));\n}",
            "ClassName": "TestScannerModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "fromPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    super.setUp();\n    client = new org.apache.hadoop.hbase.rest.client.Client(new org.apache.hadoop.hbase.rest.client.Cluster().add(\"localhost\", testServletPort));\n    admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    if (admin.tableExists(org.apache.hadoop.hbase.rest.TestScannerResource.TABLE)) {\n        return;\n    }\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.rest.TestScannerResource.TABLE);\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.KeyValue.parseColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestScannerResource.COLUMN_1))[0]));\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.KeyValue.parseColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestScannerResource.COLUMN_2))[0]));\n    admin.createTable(htd);\n    org.apache.hadoop.hbase.rest.TestScannerResource.expectedRows1 = insertData(org.apache.hadoop.hbase.rest.TestScannerResource.TABLE, org.apache.hadoop.hbase.rest.TestScannerResource.COLUMN_1, 1.0);\n    org.apache.hadoop.hbase.rest.TestScannerResource.expectedRows2 = insertData(org.apache.hadoop.hbase.rest.TestScannerResource.TABLE, org.apache.hadoop.hbase.rest.TestScannerResource.COLUMN_2, 0.5);\n}",
            "ClassName": "TestScannerResource",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 14,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.model.TableListModel model = new org.apache.hadoop.hbase.rest.model.TableListModel();\n    model.add(new org.apache.hadoop.hbase.rest.model.TableModel(org.apache.hadoop.hbase.rest.model.TestTableListModel.TABLE1));\n    model.add(new org.apache.hadoop.hbase.rest.model.TableModel(org.apache.hadoop.hbase.rest.model.TestTableListModel.TABLE2));\n    model.add(new org.apache.hadoop.hbase.rest.model.TableModel(org.apache.hadoop.hbase.rest.model.TestTableListModel.TABLE3));\n    return model;\n}",
            "ClassName": "TestTableListModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "buildTestModel",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.model.ScannerModel model = new org.apache.hadoop.hbase.rest.model.ScannerModel();\n    model.setStartRow(org.apache.hadoop.hbase.rest.model.TestScannerModel.START_ROW);\n    model.setEndRow(org.apache.hadoop.hbase.rest.model.TestScannerModel.END_ROW);\n    model.addColumn(org.apache.hadoop.hbase.rest.model.TestScannerModel.COLUMN1);\n    model.addColumn(org.apache.hadoop.hbase.rest.model.TestScannerModel.COLUMN2);\n    model.setStartTime(org.apache.hadoop.hbase.rest.model.TestScannerModel.START_TIME);\n    model.setEndTime(org.apache.hadoop.hbase.rest.model.TestScannerModel.END_TIME);\n    model.setBatch(org.apache.hadoop.hbase.rest.model.TestScannerModel.BATCH);\n    return model;\n}",
            "ClassName": "TestScannerModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "buildTestModel",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 11,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestScannerResource",
        "Commit": "0b590b5703f22d8a4f831dc20108f136bb8448f0",
        "CyclomaticComplexity": 0,
        "Date": "Sat, 22 May 2010 04:39:06 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testFullTableScan",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 8,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274503146"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_JSON);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_JSON);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStargateVersionJSON",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_JSON);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStorageClusterVersionJSON",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_JSON);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableListJSON",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_TEXT);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStorageClusterVersionText",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_TEXT);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableListText",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestVersionResource",
        "Commit": "0b590b5703f22d8a4f831dc20108f136bb8448f0",
        "CyclomaticComplexity": 0,
        "Date": "Sat, 22 May 2010 04:39:06 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testGetStargateVersionJSON",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 4,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274503146"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.rest.model.ScannerModel model = new org.apache.hadoop.hbase.rest.model.ScannerModel();\n    model.setBatch(1);\n    model.addColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestScannerResource.COLUMN_1));\n    org.apache.hadoop.hbase.rest.client.Response response = client.put((\"/\" + org.apache.hadoop.hbase.rest.TestScannerResource.TABLE) + \"/scanner\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF, model.createProtobufOutput());\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    java.lang.String scannerURI = response.getLocation();\n    junit.framework.Assert.assertNotNull(scannerURI);\n    response = client.get(scannerURI, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_BINARY);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    junit.framework.Assert.assertTrue(response.getBody().length > 0);\n    boolean foundRowHeader = false;\n    boolean foundColumnHeader = false;\n    boolean foundTimestampHeader = false;\n    for (org.apache.commons.httpclient.Header header : response.getHeaders()) {\n        if (header.getName().equals(\"X-Row\")) {\n            foundRowHeader = true;\n        } else\n            if (header.getName().equals(\"X-Column\")) {\n                foundColumnHeader = true;\n            } else\n                if (header.getName().equals(\"X-Timestamp\")) {\n                    foundTimestampHeader = true;\n                }\n\n\n    }\n    junit.framework.Assert.assertTrue(foundRowHeader);\n    junit.framework.Assert.assertTrue(foundColumnHeader);\n    junit.framework.Assert.assertTrue(foundTimestampHeader);\n    response = client.delete(scannerURI);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.model.ScannerModel model = new org.apache.hadoop.hbase.rest.model.ScannerModel();\n    model.setBatch(1);\n    model.addColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestScannerResource.COLUMN_1));\n    org.apache.hadoop.hbase.rest.client.Response response = client.put((\"/\" + org.apache.hadoop.hbase.rest.TestScannerResource.TABLE) + \"/scanner\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF, model.createProtobufOutput());\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    java.lang.String scannerURI = response.getLocation();\n    junit.framework.Assert.assertNotNull(scannerURI);\n    response = client.get(scannerURI, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_BINARY);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    junit.framework.Assert.assertTrue(response.getBody().length > 0);\n    boolean foundRowHeader = false;\n    boolean foundColumnHeader = false;\n    boolean foundTimestampHeader = false;\n    for (org.apache.commons.httpclient.Header header : response.getHeaders()) {\n        if (header.getName().equals(\"X-Row\")) {\n            foundRowHeader = true;\n        } else\n            if (header.getName().equals(\"X-Column\")) {\n                foundColumnHeader = true;\n            } else\n                if (header.getName().equals(\"X-Timestamp\")) {\n                    foundTimestampHeader = true;\n                }\n\n\n    }\n    junit.framework.Assert.assertTrue(foundRowHeader);\n    junit.framework.Assert.assertTrue(foundColumnHeader);\n    junit.framework.Assert.assertTrue(foundTimestampHeader);\n    response = client.delete(scannerURI);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestScannerResource",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSimpleScannerBinary",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 33,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    final int BATCH_SIZE = 10;\n    org.apache.hadoop.hbase.rest.model.ScannerModel model = new org.apache.hadoop.hbase.rest.model.ScannerModel();\n    model.setBatch(BATCH_SIZE);\n    model.addColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestScannerResource.COLUMN_1));\n    org.apache.hadoop.hbase.rest.client.Response response = client.put((\"/\" + org.apache.hadoop.hbase.rest.TestScannerResource.TABLE) + \"/scanner\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF, model.createProtobufOutput());\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    java.lang.String scannerURI = response.getLocation();\n    junit.framework.Assert.assertNotNull(scannerURI);\n    response = client.get(scannerURI, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.CellSetModel cellSet = new org.apache.hadoop.hbase.rest.model.CellSetModel();\n    cellSet.getObjectFromMessage(response.getBody());\n    junit.framework.Assert.assertEquals(countCellSet(cellSet), BATCH_SIZE);\n    response = client.delete(scannerURI);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestScannerResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSimpleScannerPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 17,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    final int BATCH_SIZE = 5;\n    org.apache.hadoop.hbase.rest.model.ScannerModel model = new org.apache.hadoop.hbase.rest.model.ScannerModel();\n    model.setBatch(BATCH_SIZE);\n    model.addColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestScannerResource.COLUMN_1));\n    java.io.StringWriter writer = new java.io.StringWriter();\n    marshaller.marshal(model, writer);\n    byte[] body = org.apache.hadoop.hbase.util.Bytes.toBytes(writer.toString());\n    org.apache.hadoop.hbase.rest.client.Response response = client.put((\"/\" + org.apache.hadoop.hbase.rest.TestScannerResource.TABLE) + \"/scanner\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML, body);\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    java.lang.String scannerURI = response.getLocation();\n    junit.framework.Assert.assertNotNull(scannerURI);\n    response = client.get(scannerURI, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.CellSetModel cellSet = ((org.apache.hadoop.hbase.rest.model.CellSetModel) (unmarshaller.unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    junit.framework.Assert.assertEquals(countCellSet(cellSet), BATCH_SIZE);\n    response = client.delete(scannerURI);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestScannerResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSimpleScannerXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 19,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    final java.lang.String path = ((((\"/\" + org.apache.hadoop.hbase.rest.TestRowResource.TABLE) + \"/\") + org.apache.hadoop.hbase.rest.TestRowResource.ROW_3) + \"/\") + org.apache.hadoop.hbase.rest.TestRowResource.COLUMN_1;\n    final byte[] body = org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestRowResource.VALUE_3);\n    org.apache.hadoop.hbase.rest.client.Response response = client.put(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_BINARY, body);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    java.lang.Thread.yield();\n    response = client.get(path, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_BINARY);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(response.getBody(), body));\n    boolean foundTimestampHeader = false;\n    for (org.apache.commons.httpclient.Header header : response.getHeaders()) {\n        if (header.getName().equals(\"X-Timestamp\")) {\n            foundTimestampHeader = true;\n            break;\n        }\n    }\n    junit.framework.Assert.assertTrue(foundTimestampHeader);\n    response = deleteRow(org.apache.hadoop.hbase.rest.TestRowResource.TABLE, org.apache.hadoop.hbase.rest.TestRowResource.ROW_3);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSingleCellGetPutBinary",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 20,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_5": {
            "Body": "{\n    model.setBatch(100);\n    org.apache.hadoop.hbase.rest.client.Response response = client.put((\"/\" + org.apache.hadoop.hbase.rest.TestScannerResource.TABLE) + \"/scanner\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF, model.createProtobufOutput());\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    java.lang.String scannerURI = response.getLocation();\n    junit.framework.Assert.assertNotNull(scannerURI);\n    int count = 0;\n    while (true) {\n        response = client.get(scannerURI, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n        junit.framework.Assert.assertTrue((response.getCode() == 200) || (response.getCode() == 204));\n        if (response.getCode() == 200) {\n            org.apache.hadoop.hbase.rest.model.CellSetModel cellSet = new org.apache.hadoop.hbase.rest.model.CellSetModel();\n            cellSet.getObjectFromMessage(response.getBody());\n            java.util.Iterator<org.apache.hadoop.hbase.rest.model.RowModel> rows = cellSet.getRows().iterator();\n            while (rows.hasNext()) {\n                org.apache.hadoop.hbase.rest.model.RowModel row = rows.next();\n                java.util.Iterator<org.apache.hadoop.hbase.rest.model.CellModel> cells = row.getCells().iterator();\n                while (cells.hasNext()) {\n                    cells.next();\n                    count++;\n                } \n            } \n        } else {\n            break;\n        }\n    } \n    response = client.delete(scannerURI);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    return count;\n}",
            "ClassName": "TestScannerResource",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "fullTableScan",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 30,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestScannerResource",
        "Commit": "0b590b5703f22d8a4f831dc20108f136bb8448f0",
        "CyclomaticComplexity": 4,
        "Date": "Sat, 22 May 2010 04:39:06 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testSimpleScannerBinary",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 33,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274503146"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n    org.apache.hadoop.hbase.rest.model.StorageClusterVersionModel clusterVersionModel = ((org.apache.hadoop.hbase.rest.model.StorageClusterVersionModel) (context.createUnmarshaller().unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    junit.framework.Assert.assertNotNull(clusterVersionModel);\n    junit.framework.Assert.assertNotNull(clusterVersionModel.getVersion());\n    org.apache.hadoop.hbase.rest.TestVersionResource.LOG.info(\"success retrieving storage cluster version as XML\");\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n    org.apache.hadoop.hbase.rest.model.StorageClusterVersionModel clusterVersionModel = ((org.apache.hadoop.hbase.rest.model.StorageClusterVersionModel) (context.createUnmarshaller().unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    junit.framework.Assert.assertNotNull(clusterVersionModel);\n    junit.framework.Assert.assertNotNull(clusterVersionModel.getVersion());\n    org.apache.hadoop.hbase.rest.TestVersionResource.LOG.info(\"success retrieving storage cluster version as XML\");\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStorageClusterVersionXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n    org.apache.hadoop.hbase.rest.model.VersionModel model = ((org.apache.hadoop.hbase.rest.model.VersionModel) (context.createUnmarshaller().unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    org.apache.hadoop.hbase.rest.TestVersionResource.validate(model);\n    org.apache.hadoop.hbase.rest.TestVersionResource.LOG.info(\"success retrieving Stargate version as XML\");\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStargateVersionXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n    org.apache.hadoop.hbase.rest.model.VersionModel model = new org.apache.hadoop.hbase.rest.model.VersionModel();\n    model.getObjectFromMessage(response.getBody());\n    org.apache.hadoop.hbase.rest.TestVersionResource.validate(model);\n    org.apache.hadoop.hbase.rest.TestVersionResource.LOG.info(\"success retrieving Stargate version as protobuf\");\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStargateVersionPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/status/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel model = ((org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel) (context.createUnmarshaller().unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    validate(model);\n}",
            "ClassName": "TestStatusResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetClusterStatusXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_JSON);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStorageClusterVersionJSON",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestVersionResource",
        "Commit": "0b590b5703f22d8a4f831dc20108f136bb8448f0",
        "CyclomaticComplexity": 0,
        "Date": "Sat, 22 May 2010 04:39:06 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testGetStorageClusterVersionXML",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 8,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274503146"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_TEXT);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n    java.lang.String body = org.apache.hadoop.hbase.util.Bytes.toString(response.getBody());\n    junit.framework.Assert.assertTrue(body.length() > 0);\n    junit.framework.Assert.assertTrue(body.contains(org.apache.hadoop.hbase.rest.RESTServlet.VERSION_STRING));\n    junit.framework.Assert.assertTrue(body.contains(java.lang.System.getProperty(\"java.vm.vendor\")));\n    junit.framework.Assert.assertTrue(body.contains(java.lang.System.getProperty(\"java.version\")));\n    junit.framework.Assert.assertTrue(body.contains(java.lang.System.getProperty(\"java.vm.version\")));\n    junit.framework.Assert.assertTrue(body.contains(java.lang.System.getProperty(\"os.name\")));\n    junit.framework.Assert.assertTrue(body.contains(java.lang.System.getProperty(\"os.version\")));\n    junit.framework.Assert.assertTrue(body.contains(java.lang.System.getProperty(\"os.arch\")));\n    junit.framework.Assert.assertTrue(body.contains(com.sun.jersey.spi.container.servlet.ServletContainer.class.getPackage().getImplementationVersion()));\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/version\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_TEXT);\n    junit.framework.Assert.assertTrue(response.getCode() == 200);\n    java.lang.String body = org.apache.hadoop.hbase.util.Bytes.toString(response.getBody());\n    junit.framework.Assert.assertTrue(body.length() > 0);\n    junit.framework.Assert.assertTrue(body.contains(org.apache.hadoop.hbase.rest.RESTServlet.VERSION_STRING));\n    junit.framework.Assert.assertTrue(body.contains(java.lang.System.getProperty(\"java.vm.vendor\")));\n    junit.framework.Assert.assertTrue(body.contains(java.lang.System.getProperty(\"java.version\")));\n    junit.framework.Assert.assertTrue(body.contains(java.lang.System.getProperty(\"java.vm.version\")));\n    junit.framework.Assert.assertTrue(body.contains(java.lang.System.getProperty(\"os.name\")));\n    junit.framework.Assert.assertTrue(body.contains(java.lang.System.getProperty(\"os.version\")));\n    junit.framework.Assert.assertTrue(body.contains(java.lang.System.getProperty(\"os.arch\")));\n    junit.framework.Assert.assertTrue(body.contains(com.sun.jersey.spi.container.servlet.ServletContainer.class.getPackage().getImplementationVersion()));\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetStargateVersionText",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 14,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    junit.framework.Assert.assertNotNull(model);\n    junit.framework.Assert.assertNotNull(model.getRESTVersion());\n    junit.framework.Assert.assertEquals(model.getRESTVersion(), org.apache.hadoop.hbase.rest.RESTServlet.VERSION_STRING);\n    java.lang.String osVersion = model.getOSVersion();\n    junit.framework.Assert.assertNotNull(osVersion);\n    junit.framework.Assert.assertTrue(osVersion.contains(java.lang.System.getProperty(\"os.name\")));\n    junit.framework.Assert.assertTrue(osVersion.contains(java.lang.System.getProperty(\"os.version\")));\n    junit.framework.Assert.assertTrue(osVersion.contains(java.lang.System.getProperty(\"os.arch\")));\n    java.lang.String jvmVersion = model.getJVMVersion();\n    junit.framework.Assert.assertNotNull(jvmVersion);\n    junit.framework.Assert.assertTrue(jvmVersion.contains(java.lang.System.getProperty(\"java.vm.vendor\")));\n    junit.framework.Assert.assertTrue(jvmVersion.contains(java.lang.System.getProperty(\"java.version\")));\n    junit.framework.Assert.assertTrue(jvmVersion.contains(java.lang.System.getProperty(\"java.vm.version\")));\n    junit.framework.Assert.assertNotNull(model.getServerVersion());\n    java.lang.String jerseyVersion = model.getJerseyVersion();\n    junit.framework.Assert.assertNotNull(jerseyVersion);\n    junit.framework.Assert.assertEquals(jerseyVersion, com.sun.jersey.spi.container.servlet.ServletContainer.class.getPackage().getImplementationVersion());\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "validate",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 19,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    this.body = body;\n}",
            "ClassName": "Response",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setBody",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return body;\n}",
            "ClassName": "Response",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getBody",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return body != null;\n}",
            "ClassName": "Response",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "hasBody",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestVersionResource",
        "Commit": "0b590b5703f22d8a4f831dc20108f136bb8448f0",
        "CyclomaticComplexity": 0,
        "Date": "Sat, 22 May 2010 04:39:06 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testGetStargateVersionText",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 14,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274503146"
    },
    {
        "Body": "{\n    java.lang.String schemaPath = (\"/\" + org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2) + \"/schema\";\n    org.apache.hadoop.hbase.rest.model.TableSchemaModel model;\n    org.apache.hadoop.hbase.rest.client.Response response;\n    junit.framework.Assert.assertFalse(admin.tableExists(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2));\n    model = org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.buildTestModel(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2);\n    org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.checkModel(model, org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2);\n    response = client.put(schemaPath, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF, model.createProtobufOutput());\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    admin.enableTable(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2);\n    response = client.get(schemaPath, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    model = new org.apache.hadoop.hbase.rest.model.TableSchemaModel();\n    model.getObjectFromMessage(response.getBody());\n    org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.checkModel(model, org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2);\n    client.delete(schemaPath);\n    junit.framework.Assert.assertFalse(admin.tableExists(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2));\n}",
        "CUT_1": {
            "Body": "{\n    java.lang.String schemaPath = (\"/\" + org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2) + \"/schema\";\n    org.apache.hadoop.hbase.rest.model.TableSchemaModel model;\n    org.apache.hadoop.hbase.rest.client.Response response;\n    junit.framework.Assert.assertFalse(admin.tableExists(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2));\n    model = org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.buildTestModel(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2);\n    org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.checkModel(model, org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2);\n    response = client.put(schemaPath, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF, model.createProtobufOutput());\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    admin.enableTable(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2);\n    response = client.get(schemaPath, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    model = new org.apache.hadoop.hbase.rest.model.TableSchemaModel();\n    model.getObjectFromMessage(response.getBody());\n    org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.checkModel(model, org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2);\n    client.delete(schemaPath);\n    junit.framework.Assert.assertFalse(admin.tableExists(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2));\n}",
            "ClassName": "TestSchemaResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableCreateAndDeletePB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 18,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    java.lang.String schemaPath = (\"/\" + org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE1) + \"/schema\";\n    org.apache.hadoop.hbase.rest.model.TableSchemaModel model;\n    org.apache.hadoop.hbase.rest.client.Response response;\n    junit.framework.Assert.assertFalse(admin.tableExists(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE1));\n    model = org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.buildTestModel(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE1);\n    org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.checkModel(model, org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE1);\n    response = client.put(schemaPath, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML, toXML(model));\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    admin.enableTable(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE1);\n    response = client.get(schemaPath, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    model = fromXML(response.getBody());\n    org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.checkModel(model, org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE1);\n    client.delete(schemaPath);\n    junit.framework.Assert.assertFalse(admin.tableExists(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE1));\n}",
            "ClassName": "TestSchemaResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableCreateAndDeleteXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 17,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/status/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel model = new org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel();\n    model.getObjectFromMessage(response.getBody());\n    validate(model);\n}",
            "ClassName": "TestStatusResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetClusterStatusPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.TableListModel model = new org.apache.hadoop.hbase.rest.model.TableListModel();\n    model.getObjectFromMessage(response.getBody());\n    checkTableList(model);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableListPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    final int BATCH_SIZE = 10;\n    org.apache.hadoop.hbase.rest.model.ScannerModel model = new org.apache.hadoop.hbase.rest.model.ScannerModel();\n    model.setBatch(BATCH_SIZE);\n    model.addColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.rest.TestScannerResource.COLUMN_1));\n    org.apache.hadoop.hbase.rest.client.Response response = client.put((\"/\" + org.apache.hadoop.hbase.rest.TestScannerResource.TABLE) + \"/scanner\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF, model.createProtobufOutput());\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    java.lang.String scannerURI = response.getLocation();\n    junit.framework.Assert.assertNotNull(scannerURI);\n    response = client.get(scannerURI, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.CellSetModel cellSet = new org.apache.hadoop.hbase.rest.model.CellSetModel();\n    cellSet.getObjectFromMessage(response.getBody());\n    junit.framework.Assert.assertEquals(countCellSet(cellSet), BATCH_SIZE);\n    response = client.delete(scannerURI);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n}",
            "ClassName": "TestScannerResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSimpleScannerPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 17,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestSchemaResource",
        "Commit": "f8968911258dbee5284b0b87e4d9299316c4b2c7",
        "CyclomaticComplexity": 0,
        "Date": "Sat, 22 May 2010 23:38:16 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testTableCreateAndDeletePB",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 18,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274571496"
    },
    {
        "Body": "{\n    java.lang.String schemaPath = (\"/\" + org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE1) + \"/schema\";\n    org.apache.hadoop.hbase.rest.model.TableSchemaModel model;\n    org.apache.hadoop.hbase.rest.client.Response response;\n    junit.framework.Assert.assertFalse(admin.tableExists(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE1));\n    model = org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.buildTestModel(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE1);\n    org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.checkModel(model, org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE1);\n    response = client.put(schemaPath, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML, toXML(model));\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    admin.enableTable(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE1);\n    response = client.get(schemaPath, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    model = fromXML(response.getBody());\n    org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.checkModel(model, org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE1);\n    client.delete(schemaPath);\n    junit.framework.Assert.assertFalse(admin.tableExists(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE1));\n}",
        "CUT_1": {
            "Body": "{\n    java.lang.String schemaPath = (\"/\" + org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE1) + \"/schema\";\n    org.apache.hadoop.hbase.rest.model.TableSchemaModel model;\n    org.apache.hadoop.hbase.rest.client.Response response;\n    junit.framework.Assert.assertFalse(admin.tableExists(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE1));\n    model = org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.buildTestModel(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE1);\n    org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.checkModel(model, org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE1);\n    response = client.put(schemaPath, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML, toXML(model));\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    admin.enableTable(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE1);\n    response = client.get(schemaPath, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    model = fromXML(response.getBody());\n    org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.checkModel(model, org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE1);\n    client.delete(schemaPath);\n    junit.framework.Assert.assertFalse(admin.tableExists(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE1));\n}",
            "ClassName": "TestSchemaResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableCreateAndDeleteXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 17,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    java.lang.String schemaPath = (\"/\" + org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2) + \"/schema\";\n    org.apache.hadoop.hbase.rest.model.TableSchemaModel model;\n    org.apache.hadoop.hbase.rest.client.Response response;\n    junit.framework.Assert.assertFalse(admin.tableExists(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2));\n    model = org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.buildTestModel(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2);\n    org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.checkModel(model, org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2);\n    response = client.put(schemaPath, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF, model.createProtobufOutput());\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    admin.enableTable(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2);\n    response = client.get(schemaPath, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    model = new org.apache.hadoop.hbase.rest.model.TableSchemaModel();\n    model.getObjectFromMessage(response.getBody());\n    org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.checkModel(model, org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2);\n    client.delete(schemaPath);\n    junit.framework.Assert.assertFalse(admin.tableExists(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2));\n}",
            "ClassName": "TestSchemaResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableCreateAndDeletePB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 18,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/status/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel model = new org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel();\n    model.getObjectFromMessage(response.getBody());\n    validate(model);\n}",
            "ClassName": "TestStatusResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetClusterStatusPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.TableListModel model = new org.apache.hadoop.hbase.rest.model.TableListModel();\n    model.getObjectFromMessage(response.getBody());\n    checkTableList(model);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTableListPB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.rest.client.Response response = client.get(\"/status/cluster\", org.apache.hadoop.hbase.rest.Constants.MIMETYPE_XML);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel model = ((org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel) (context.createUnmarshaller().unmarshal(new java.io.ByteArrayInputStream(response.getBody()))));\n    validate(model);\n}",
            "ClassName": "TestStatusResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetClusterStatusXML",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestSchemaResource",
        "Commit": "f8968911258dbee5284b0b87e4d9299316c4b2c7",
        "CyclomaticComplexity": 0,
        "Date": "Sat, 22 May 2010 23:38:16 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testTableCreateAndDeleteXML",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 17,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274571496"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = createTable();\n    org.apache.hadoop.hbase.HBaseTestCase.Incommon incommon = new org.apache.hadoop.hbase.HBaseTestCase.HTableIncommon(t);\n    org.apache.hadoop.hbase.TimestampTestBase.doTestDelete(incommon, new org.apache.hadoop.hbase.HBaseTestCase.FlushCache() {\n        public void flushcache() throws java.io.IOException {\n            cluster.flushcache();\n        }\n    });\n    org.apache.hadoop.hbase.TimestampTestBase.doTestTimestampScanning(incommon, new org.apache.hadoop.hbase.HBaseTestCase.FlushCache() {\n        public void flushcache() throws java.io.IOException {\n            cluster.flushcache();\n        }\n    });\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = createTable();\n    org.apache.hadoop.hbase.HBaseTestCase.Incommon incommon = new org.apache.hadoop.hbase.HBaseTestCase.HTableIncommon(t);\n    org.apache.hadoop.hbase.TimestampTestBase.doTestDelete(incommon, new org.apache.hadoop.hbase.HBaseTestCase.FlushCache() {\n        public void flushcache() throws java.io.IOException {\n            cluster.flushcache();\n        }\n    });\n    org.apache.hadoop.hbase.TimestampTestBase.doTestTimestampScanning(incommon, new org.apache.hadoop.hbase.HBaseTestCase.FlushCache() {\n        public void flushcache() throws java.io.IOException {\n            cluster.flushcache();\n        }\n    });\n}",
            "ClassName": "TestTimestamp",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testTimestamps",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 14,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.TimestampTestBase.put(incommon, org.apache.hadoop.hbase.TimestampTestBase.T0);\n    org.apache.hadoop.hbase.TimestampTestBase.put(incommon, org.apache.hadoop.hbase.TimestampTestBase.T1);\n    org.apache.hadoop.hbase.TimestampTestBase.put(incommon, org.apache.hadoop.hbase.TimestampTestBase.T2);\n    org.apache.hadoop.hbase.TimestampTestBase.put(incommon);\n    org.apache.hadoop.hbase.TimestampTestBase.assertVersions(incommon, new long[]{ org.apache.hadoop.hbase.HConstants.LATEST_TIMESTAMP, org.apache.hadoop.hbase.TimestampTestBase.T2, org.apache.hadoop.hbase.TimestampTestBase.T1 });\n    org.apache.hadoop.hbase.TimestampTestBase.delete(incommon);\n    org.apache.hadoop.hbase.TimestampTestBase.assertVersions(incommon, new long[]{ org.apache.hadoop.hbase.TimestampTestBase.T2, org.apache.hadoop.hbase.TimestampTestBase.T1, org.apache.hadoop.hbase.TimestampTestBase.T0 });\n    flusher.flushcache();\n    org.apache.hadoop.hbase.TimestampTestBase.assertVersions(incommon, new long[]{ org.apache.hadoop.hbase.TimestampTestBase.T2, org.apache.hadoop.hbase.TimestampTestBase.T1, org.apache.hadoop.hbase.TimestampTestBase.T0 });\n    org.apache.hadoop.hbase.TimestampTestBase.put(incommon);\n    org.apache.hadoop.hbase.TimestampTestBase.assertVersions(incommon, new long[]{ org.apache.hadoop.hbase.HConstants.LATEST_TIMESTAMP, org.apache.hadoop.hbase.TimestampTestBase.T2, org.apache.hadoop.hbase.TimestampTestBase.T1 });\n    org.apache.hadoop.hbase.TimestampTestBase.delete(incommon, org.apache.hadoop.hbase.TimestampTestBase.T2);\n    org.apache.hadoop.hbase.TimestampTestBase.assertVersions(incommon, new long[]{ org.apache.hadoop.hbase.HConstants.LATEST_TIMESTAMP, org.apache.hadoop.hbase.TimestampTestBase.T1, org.apache.hadoop.hbase.TimestampTestBase.T0 });\n    flusher.flushcache();\n    org.apache.hadoop.hbase.TimestampTestBase.assertVersions(incommon, new long[]{ org.apache.hadoop.hbase.HConstants.LATEST_TIMESTAMP, org.apache.hadoop.hbase.TimestampTestBase.T1, org.apache.hadoop.hbase.TimestampTestBase.T0 });\n    org.apache.hadoop.hbase.TimestampTestBase.put(incommon, org.apache.hadoop.hbase.TimestampTestBase.T2);\n    org.apache.hadoop.hbase.TimestampTestBase.delete(incommon, org.apache.hadoop.hbase.TimestampTestBase.T1);\n    org.apache.hadoop.hbase.TimestampTestBase.put(incommon, org.apache.hadoop.hbase.TimestampTestBase.T1);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.TimestampTestBase.ROW);\n    delete.deleteColumns(org.apache.hadoop.hbase.TimestampTestBase.FAMILY_NAME, org.apache.hadoop.hbase.TimestampTestBase.QUALIFIER_NAME, org.apache.hadoop.hbase.TimestampTestBase.T2);\n    incommon.delete(delete, null, true);\n    org.apache.hadoop.hbase.TimestampTestBase.assertOnlyLatest(incommon, org.apache.hadoop.hbase.HConstants.LATEST_TIMESTAMP);\n    flusher.flushcache();\n    org.apache.hadoop.hbase.TimestampTestBase.assertOnlyLatest(incommon, org.apache.hadoop.hbase.HConstants.LATEST_TIMESTAMP);\n}",
            "ClassName": "TimestampTestBase",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "doTestDelete",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 26,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.TimestampTestBase.put(incommon, org.apache.hadoop.hbase.TimestampTestBase.T0);\n    org.apache.hadoop.hbase.TimestampTestBase.put(incommon, org.apache.hadoop.hbase.TimestampTestBase.T1);\n    org.apache.hadoop.hbase.TimestampTestBase.put(incommon, org.apache.hadoop.hbase.HConstants.LATEST_TIMESTAMP);\n    int count = org.apache.hadoop.hbase.TimestampTestBase.assertScanContentTimestamp(incommon, org.apache.hadoop.hbase.HConstants.LATEST_TIMESTAMP);\n    junit.framework.Assert.assertEquals(count, org.apache.hadoop.hbase.TimestampTestBase.assertScanContentTimestamp(incommon, org.apache.hadoop.hbase.TimestampTestBase.T0));\n    junit.framework.Assert.assertEquals(count, org.apache.hadoop.hbase.TimestampTestBase.assertScanContentTimestamp(incommon, org.apache.hadoop.hbase.TimestampTestBase.T1));\n    flusher.flushcache();\n    junit.framework.Assert.assertEquals(count, org.apache.hadoop.hbase.TimestampTestBase.assertScanContentTimestamp(incommon, org.apache.hadoop.hbase.TimestampTestBase.T0));\n    junit.framework.Assert.assertEquals(count, org.apache.hadoop.hbase.TimestampTestBase.assertScanContentTimestamp(incommon, org.apache.hadoop.hbase.TimestampTestBase.T1));\n}",
            "ClassName": "TimestampTestBase",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "doTestTimestampScanning",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 11,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    initHRegion(tableName, getName(), org.apache.hadoop.hbase.HBaseTestCase.fam1);\n    long value = 1L;\n    long amount = 3L;\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HBaseTestCase.fam1, qual1, org.apache.hadoop.hbase.util.Bytes.toBytes(value));\n    region.put(put);\n    java.lang.Thread t = new java.lang.Thread() {\n        public void run() {\n            try {\n                region.flushcache();\n            } catch (java.io.IOException e) {\n                org.apache.hadoop.hbase.regionserver.TestHRegion.LOG.info(\"test ICV, got IOE during flushcache()\");\n            }\n        }\n    };\n    t.start();\n    long r = region.incrementColumnValue(row, org.apache.hadoop.hbase.HBaseTestCase.fam1, qual1, amount, true);\n    junit.framework.Assert.assertEquals(value + amount, r);\n    assertICV(row, org.apache.hadoop.hbase.HBaseTestCase.fam1, qual1, value + amount);\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testIncrementColumnValue_ConcurrentFlush",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 6
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon loader = new org.apache.hadoop.hbase.HBaseTestCase.HRegionIncommon(region);\n    org.apache.hadoop.hbase.HBaseTestCase.addContent(loader, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.TestCompaction.COLUMN_FAMILY));\n    loader.flushcache();\n}",
            "ClassName": "TestCompaction",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createStoreFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTimestamp",
        "Commit": "5f679cb525377b95a3213ffd96236016b410df94",
        "CyclomaticComplexity": 0,
        "Date": "Sun, 23 May 2010 05:30:27 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testTimestamps",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 14,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274592627"
    },
    {
        "Body": "{\n    runTestOnTable(new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.mapred.TestTableMapReduce.MULTI_REGION_TABLE_NAME));\n}",
        "CUT_1": {
            "Body": "{\n    runTestOnTable(new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.MULTI_REGION_TABLE_NAME));\n}",
            "ClassName": "TestTableMapReduce",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testMultiRegionTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    this.server = cluster.getRegionServerThreads().get(0).getRegionServer();\n    this.log = server.getLog();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    for (int i = 1; i <= 256; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + java.lang.String.format(\"%1$04d\", i)));\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, value);\n        table.put(put);\n        if ((i % 32) == 0) {\n            try {\n                java.lang.Thread.sleep(2000);\n            } catch (java.lang.InterruptedException e) {\n            }\n        }\n    }\n}",
            "ClassName": "TestLogRolling",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startAndWriteData",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_3": {
            "Body": "{\n    return org.apache.hadoop.hbase.client.HTable.isTableEnabled(conf, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n}",
            "ClassName": "HTable",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "isTableEnabled",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        org.apache.hadoop.hbase.HRegionInfo hri = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(b);\n        byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(row);\n        p.add(org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily(), org.apache.hadoop.hbase.master.TestMasterTransitions.getTestQualifier(), row);\n        t.put(p);\n        rows++;\n    }\n    s.close();\n    org.junit.Assert.assertEquals(expected, rows);\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addToEachStartKey",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.mapred.MiniMRCluster mrCluster = new org.apache.hadoop.mapred.MiniMRCluster(2, fs.getUri().toString(), 1);\n    org.apache.hadoop.mapreduce.Job job = null;\n    try {\n        org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.LOG.info(\"Before map/reduce startup\");\n        job = new org.apache.hadoop.mapreduce.Job(conf, \"process column contents\");\n        job.setNumReduceTasks(1);\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.addFamily(org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.INPUT_FAMILY);\n        org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(org.apache.hadoop.hbase.util.Bytes.toString(table.getTableName()), scan, org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.ProcessContentsMapper.class, org.apache.hadoop.hbase.io.ImmutableBytesWritable.class, org.apache.hadoop.hbase.client.Put.class, job);\n        org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob(org.apache.hadoop.hbase.util.Bytes.toString(table.getTableName()), org.apache.hadoop.hbase.mapreduce.IdentityTableReducer.class, job);\n        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new org.apache.hadoop.fs.Path(\"test\"));\n        org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.LOG.info(\"Started \" + org.apache.hadoop.hbase.util.Bytes.toString(table.getTableName()));\n        job.waitForCompletion(true);\n        org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.LOG.info(\"After map/reduce completion\");\n        verify(org.apache.hadoop.hbase.util.Bytes.toString(table.getTableName()));\n    } finally {\n        mrCluster.shutdown();\n        if (job != null) {\n            org.apache.hadoop.fs.FileUtil.fullyDelete(new java.io.File(job.getConfiguration().get(\"hadoop.tmp.dir\")));\n        }\n    }\n}",
            "ClassName": "TestTableMapReduce",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "runTestOnTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 3,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableMapReduce",
        "Commit": "90a56da42ed62a6ab4dab87801c55fd3b453ae42",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 27 May 2010 22:24:03 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testMultiRegionTable",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274999043"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(conf, TABLE_NAME);\n    for (int i = 0; i < ROWS.length; i++) {\n        for (int j = 0; j < TIMESTAMPS.length; j++) {\n            org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(ROWS[i]);\n            get.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n            get.setTimeStamp(TIMESTAMPS[j]);\n            org.apache.hadoop.hbase.client.Result result = t.get(get);\n            int cellCount = 0;\n            for (@java.lang.SuppressWarnings(\"unused\")\n            org.apache.hadoop.hbase.KeyValue kv : result.sorted()) {\n                cellCount++;\n            }\n            junit.framework.Assert.assertTrue(cellCount == 1);\n        }\n    }\n    int count = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.apache.hadoop.hbase.client.ResultScanner s = t.getScanner(scan);\n    try {\n        for (org.apache.hadoop.hbase.client.Result rr = null; (rr = s.next()) != null;) {\n            java.lang.System.out.println(rr.toString());\n            count += 1;\n        }\n        junit.framework.Assert.assertEquals(\"Number of rows should be 2\", 2, count);\n    } finally {\n        s.close();\n    }\n    count = 0;\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.setTimeRange(1000L, java.lang.Long.MAX_VALUE);\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    s = t.getScanner(scan);\n    try {\n        while (s.next() != null) {\n            count += 1;\n        } \n        junit.framework.Assert.assertEquals(\"Number of rows should be 2\", 2, count);\n    } finally {\n        s.close();\n    }\n    count = 0;\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.setTimeStamp(1000L);\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    s = t.getScanner(scan);\n    try {\n        while (s.next() != null) {\n            count += 1;\n        } \n        junit.framework.Assert.assertEquals(\"Number of rows should be 2\", 2, count);\n    } finally {\n        s.close();\n    }\n    count = 0;\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.setTimeRange(100L, 1000L);\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    s = t.getScanner(scan);\n    try {\n        while (s.next() != null) {\n            count += 1;\n        } \n        junit.framework.Assert.assertEquals(\"Number of rows should be 2\", 2, count);\n    } finally {\n        s.close();\n    }\n    count = 0;\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.setTimeStamp(100L);\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    s = t.getScanner(scan);\n    try {\n        while (s.next() != null) {\n            count += 1;\n        } \n        junit.framework.Assert.assertEquals(\"Number of rows should be 2\", 2, count);\n    } finally {\n        s.close();\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(conf, TABLE_NAME);\n    for (int i = 0; i < ROWS.length; i++) {\n        for (int j = 0; j < TIMESTAMPS.length; j++) {\n            org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(ROWS[i]);\n            get.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n            get.setTimeStamp(TIMESTAMPS[j]);\n            org.apache.hadoop.hbase.client.Result result = t.get(get);\n            int cellCount = 0;\n            for (@java.lang.SuppressWarnings(\"unused\")\n            org.apache.hadoop.hbase.KeyValue kv : result.sorted()) {\n                cellCount++;\n            }\n            junit.framework.Assert.assertTrue(cellCount == 1);\n        }\n    }\n    int count = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.apache.hadoop.hbase.client.ResultScanner s = t.getScanner(scan);\n    try {\n        for (org.apache.hadoop.hbase.client.Result rr = null; (rr = s.next()) != null;) {\n            java.lang.System.out.println(rr.toString());\n            count += 1;\n        }\n        junit.framework.Assert.assertEquals(\"Number of rows should be 2\", 2, count);\n    } finally {\n        s.close();\n    }\n    count = 0;\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.setTimeRange(1000L, java.lang.Long.MAX_VALUE);\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    s = t.getScanner(scan);\n    try {\n        while (s.next() != null) {\n            count += 1;\n        } \n        junit.framework.Assert.assertEquals(\"Number of rows should be 2\", 2, count);\n    } finally {\n        s.close();\n    }\n    count = 0;\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.setTimeStamp(1000L);\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    s = t.getScanner(scan);\n    try {\n        while (s.next() != null) {\n            count += 1;\n        } \n        junit.framework.Assert.assertEquals(\"Number of rows should be 2\", 2, count);\n    } finally {\n        s.close();\n    }\n    count = 0;\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.setTimeRange(100L, 1000L);\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    s = t.getScanner(scan);\n    try {\n        while (s.next() != null) {\n            count += 1;\n        } \n        junit.framework.Assert.assertEquals(\"Number of rows should be 2\", 2, count);\n    } finally {\n        s.close();\n    }\n    count = 0;\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.setTimeStamp(100L);\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    s = t.getScanner(scan);\n    try {\n        while (s.next() != null) {\n            count += 1;\n        } \n        junit.framework.Assert.assertEquals(\"Number of rows should be 2\", 2, count);\n    } finally {\n        s.close();\n    }\n}",
            "ClassName": "TestScanMultipleVersions",
            "CyclomaticComplexity": 8,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanMultipleVersions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 82,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.ResultScanner results = table.getScanner(scan);\n    int count = 0;\n    for (@java.lang.SuppressWarnings(\"unused\")\n    org.apache.hadoop.hbase.client.Result res : results) {\n        count++;\n    }\n    results.close();\n    return count;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "countRows",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 11,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    final int COUNT = 5;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(getName());\n    for (int i = 0; i < COUNT; i++) {\n        byte[] regionName = org.apache.hadoop.hbase.HRegionInfo.createRegionName(tableName, org.apache.hadoop.hbase.util.Bytes.toBytes(i == 0 ? \"\" : java.lang.Integer.toString(i)), java.lang.Long.toString(java.lang.System.currentTimeMillis()));\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(regionName);\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER, org.apache.hadoop.hbase.util.Bytes.toBytes(\"localhost:1234\"));\n        t.put(put);\n    }\n    long sleepTime = conf.getLong(\"hbase.master.meta.thread.rescanfrequency\", 10000);\n    int tries = conf.getInt(\"hbase.client.retries.number\", 5);\n    int count = 0;\n    do {\n        tries -= 1;\n        try {\n            java.lang.Thread.sleep(sleepTime);\n        } catch (java.lang.InterruptedException e) {\n        }\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITA_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITB_QUALIFIER);\n        org.apache.hadoop.hbase.client.ResultScanner scanner = t.getScanner(scan);\n        try {\n            count = 0;\n            org.apache.hadoop.hbase.client.Result r;\n            while ((r = scanner.next()) != null) {\n                if (!r.isEmpty()) {\n                    count += 1;\n                }\n            } \n        } finally {\n            scanner.close();\n        }\n    } while ((count != 0) && (tries >= 0) );\n    junit.framework.Assert.assertTrue(tries >= 0);\n    junit.framework.Assert.assertEquals(0, count);\n}",
            "ClassName": "TestEmptyMetaInfo",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testEmptyMetaInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 41,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_4": {
            "Body": "{\n    byte[] startrow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"bbb\");\n    byte[] stoprow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"ccc\");\n    try {\n        this.r = createNewHRegion(org.apache.hadoop.hbase.regionserver.TestScanner.REGION_INFO.getTableDesc(), null, null);\n        org.apache.hadoop.hbase.HBaseTestCase.addContent(this.r, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"abc\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"abd\"));\n        scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        org.apache.hadoop.hbase.regionserver.InternalScanner s = r.getScanner(scan);\n        int count = 0;\n        while (s.next(results)) {\n            count++;\n        } \n        s.close();\n        junit.framework.Assert.assertEquals(0, count);\n        scan = new org.apache.hadoop.hbase.client.Scan(startrow, stoprow);\n        scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n        s = r.getScanner(scan);\n        count = 0;\n        org.apache.hadoop.hbase.KeyValue kv = null;\n        results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        for (boolean first = true; s.next(results);) {\n            kv = results.get(0);\n            if (first) {\n                junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.BYTES_COMPARATOR.compare(startrow, kv.getRow()) == 0);\n                first = false;\n            }\n            count++;\n        }\n        junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.BYTES_COMPARATOR.compare(stoprow, kv.getRow()) > 0);\n        junit.framework.Assert.assertTrue(count > 10);\n        s.close();\n    } finally {\n        this.r.close();\n        this.r.getLog().closeAndDelete();\n        org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(this.cluster);\n    }\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testStopRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 39,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        rows++;\n    }\n    s.close();\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "countOfMetaRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestScanMultipleVersions",
        "Commit": "90a56da42ed62a6ab4dab87801c55fd3b453ae42",
        "CyclomaticComplexity": 8,
        "Date": "Thu, 27 May 2010 22:24:03 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testScanMultipleVersions",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 82,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274999043"
    },
    {
        "Body": "{\n    junit.framework.Assert.assertNotNull(dfsCluster);\n    org.apache.hadoop.hbase.HMerge.merge(conf, dfsCluster.getFileSystem(), desc.getName());\n}",
        "CUT_1": {
            "Body": "{\n    junit.framework.Assert.assertNotNull(dfsCluster);\n    org.apache.hadoop.hbase.HMerge.merge(conf, dfsCluster.getFileSystem(), desc.getName());\n}",
            "ClassName": "TestMergeTable",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testMergeTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    junit.framework.Assert.assertNotNull(dfsCluster);\n    org.apache.hadoop.hbase.HMerge.merge(conf, dfsCluster.getFileSystem(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n}",
            "ClassName": "TestMergeMeta",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testMergeMeta",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return dfsCluster;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getDFSCluster",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    if (this.dfsCluster != null) {\n        this.dfsCluster.shutdown();\n    }\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "shutdownMiniDFSCluster",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    super.tearDown();\n    org.apache.hadoop.hbase.HBaseTestCase.shutdownDfs(dfsCluster);\n}",
            "ClassName": "TestMergeTool",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDown",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestMergeTable",
        "Commit": "90a56da42ed62a6ab4dab87801c55fd3b453ae42",
        "CyclomaticComplexity": 0,
        "Date": "Thu, 27 May 2010 22:24:03 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testMergeTable",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 4,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1274999043"
    },
    {
        "Body": "{\n    int keyInterval = 1000;\n    float err = ((float) (0.01));\n    java.util.BitSet valid = new java.util.BitSet(keyInterval * 4);\n    org.apache.hadoop.hbase.util.DynamicByteBloomFilter bf1 = new org.apache.hadoop.hbase.util.DynamicByteBloomFilter(keyInterval, err, org.apache.hadoop.hbase.util.Hash.MURMUR_HASH);\n    bf1.allocBloom();\n    long seed = java.lang.System.currentTimeMillis();\n    java.util.Random r = new java.util.Random(seed);\n    java.lang.System.out.println(\"seed = \" + seed);\n    for (int i = 0; i < (keyInterval * 4); ++i) {\n        if (r.nextBoolean()) {\n            bf1.add(org.apache.hadoop.hbase.util.Bytes.toBytes(i));\n            valid.set(i);\n            if (bf1.getKeyCount() == 2000) {\n                break;\n            }\n        }\n    }\n    junit.framework.Assert.assertTrue(2 <= bf1.bloomCount());\n    java.lang.System.out.println(\"keys added = \" + bf1.getKeyCount());\n    java.io.ByteArrayOutputStream metaOut = new java.io.ByteArrayOutputStream();\n    java.io.ByteArrayOutputStream dataOut = new java.io.ByteArrayOutputStream();\n    bf1.getMetaWriter().write(new java.io.DataOutputStream(metaOut));\n    bf1.getDataWriter().write(new java.io.DataOutputStream(dataOut));\n    java.nio.ByteBuffer bb = java.nio.ByteBuffer.wrap(dataOut.toByteArray());\n    org.apache.hadoop.hbase.util.DynamicByteBloomFilter newBf1 = new org.apache.hadoop.hbase.util.DynamicByteBloomFilter(java.nio.ByteBuffer.wrap(metaOut.toByteArray()));\n    int falsePositives = 0;\n    for (int i = 0; i < (keyInterval * 4); ++i) {\n        if (newBf1.contains(org.apache.hadoop.hbase.util.Bytes.toBytes(i), bb)) {\n            if (!valid.get(i))\n                ++falsePositives;\n\n        } else {\n            if (valid.get(i)) {\n                assert false;\n            }\n        }\n    }\n    java.lang.System.out.println(\"False positives: \" + falsePositives);\n    junit.framework.Assert.assertTrue(falsePositives <= ((keyInterval * 5) * err));\n}",
        "CUT_1": {
            "Body": "{\n    int keyInterval = 1000;\n    float err = ((float) (0.01));\n    java.util.BitSet valid = new java.util.BitSet(keyInterval * 4);\n    org.apache.hadoop.hbase.util.DynamicByteBloomFilter bf1 = new org.apache.hadoop.hbase.util.DynamicByteBloomFilter(keyInterval, err, org.apache.hadoop.hbase.util.Hash.MURMUR_HASH);\n    bf1.allocBloom();\n    long seed = java.lang.System.currentTimeMillis();\n    java.util.Random r = new java.util.Random(seed);\n    java.lang.System.out.println(\"seed = \" + seed);\n    for (int i = 0; i < (keyInterval * 4); ++i) {\n        if (r.nextBoolean()) {\n            bf1.add(org.apache.hadoop.hbase.util.Bytes.toBytes(i));\n            valid.set(i);\n            if (bf1.getKeyCount() == 2000) {\n                break;\n            }\n        }\n    }\n    junit.framework.Assert.assertTrue(2 <= bf1.bloomCount());\n    java.lang.System.out.println(\"keys added = \" + bf1.getKeyCount());\n    java.io.ByteArrayOutputStream metaOut = new java.io.ByteArrayOutputStream();\n    java.io.ByteArrayOutputStream dataOut = new java.io.ByteArrayOutputStream();\n    bf1.getMetaWriter().write(new java.io.DataOutputStream(metaOut));\n    bf1.getDataWriter().write(new java.io.DataOutputStream(dataOut));\n    java.nio.ByteBuffer bb = java.nio.ByteBuffer.wrap(dataOut.toByteArray());\n    org.apache.hadoop.hbase.util.DynamicByteBloomFilter newBf1 = new org.apache.hadoop.hbase.util.DynamicByteBloomFilter(java.nio.ByteBuffer.wrap(metaOut.toByteArray()));\n    int falsePositives = 0;\n    for (int i = 0; i < (keyInterval * 4); ++i) {\n        if (newBf1.contains(org.apache.hadoop.hbase.util.Bytes.toBytes(i), bb)) {\n            if (!valid.get(i))\n                ++falsePositives;\n\n        } else {\n            if (valid.get(i)) {\n                assert false;\n            }\n        }\n    }\n    java.lang.System.out.println(\"False positives: \" + falsePositives);\n    junit.framework.Assert.assertTrue(falsePositives <= ((keyInterval * 5) * err));\n}",
            "ClassName": "TestByteBloomFilter",
            "CyclomaticComplexity": 7,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDynamicBloom",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 41,
            "NumberOfRandoms": 5,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.util.ByteBloomFilter bf1 = new org.apache.hadoop.hbase.util.ByteBloomFilter(1000, ((float) (0.01)), org.apache.hadoop.hbase.util.Hash.MURMUR_HASH, 0);\n    org.apache.hadoop.hbase.util.ByteBloomFilter bf2 = new org.apache.hadoop.hbase.util.ByteBloomFilter(1000, ((float) (0.01)), org.apache.hadoop.hbase.util.Hash.MURMUR_HASH, 0);\n    bf1.allocBloom();\n    bf2.allocBloom();\n    byte[] key1 = new byte[]{ 1, 2, 3, 4, 5, 6, 7, 8, 9 };\n    byte[] key2 = new byte[]{ 1, 2, 3, 4, 5, 6, 7, 8, 7 };\n    bf1.add(key1);\n    bf2.add(key2);\n    junit.framework.Assert.assertTrue(bf1.contains(key1));\n    junit.framework.Assert.assertFalse(bf1.contains(key2));\n    junit.framework.Assert.assertFalse(bf2.contains(key1));\n    junit.framework.Assert.assertTrue(bf2.contains(key2));\n    byte[] bkey = new byte[]{ 1, 2, 3, 4 };\n    byte[] bval = \"this is a much larger byte array\".getBytes();\n    bf1.add(bkey);\n    bf1.add(bval, 1, bval.length - 1);\n    junit.framework.Assert.assertTrue(bf1.contains(bkey));\n    junit.framework.Assert.assertTrue(bf1.contains(bval, 1, bval.length - 1));\n    junit.framework.Assert.assertFalse(bf1.contains(bval));\n    junit.framework.Assert.assertFalse(bf1.contains(bval));\n    java.io.ByteArrayOutputStream bOut = new java.io.ByteArrayOutputStream();\n    bf1.writeBloom(new java.io.DataOutputStream(bOut));\n    java.nio.ByteBuffer bb = java.nio.ByteBuffer.wrap(bOut.toByteArray());\n    org.apache.hadoop.hbase.util.ByteBloomFilter newBf1 = new org.apache.hadoop.hbase.util.ByteBloomFilter(1000, ((float) (0.01)), org.apache.hadoop.hbase.util.Hash.MURMUR_HASH, 0);\n    junit.framework.Assert.assertTrue(newBf1.contains(key1, bb));\n    junit.framework.Assert.assertFalse(newBf1.contains(key2, bb));\n    junit.framework.Assert.assertTrue(newBf1.contains(bkey, bb));\n    junit.framework.Assert.assertTrue(newBf1.contains(bval, 1, bval.length - 1, bb));\n    junit.framework.Assert.assertFalse(newBf1.contains(bval, bb));\n    junit.framework.Assert.assertFalse(newBf1.contains(bval, bb));\n    java.lang.System.out.println((\"Serialized as \" + bOut.size()) + \" bytes\");\n    junit.framework.Assert.assertTrue((bOut.size() - bf1.byteSize) < 10);\n}",
            "ClassName": "TestByteBloomFilter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testBasicBloom",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 34,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    float err = ((float) (0.01));\n    org.apache.hadoop.hbase.util.ByteBloomFilter b = new org.apache.hadoop.hbase.util.ByteBloomFilter((10 * 1000) * 1000, ((float) (err)), org.apache.hadoop.hbase.util.Hash.MURMUR_HASH, 3);\n    b.allocBloom();\n    long startTime = java.lang.System.currentTimeMillis();\n    int origSize = b.getByteSize();\n    for (int i = 0; i < ((1 * 1000) * 1000); ++i) {\n        b.add(org.apache.hadoop.hbase.util.Bytes.toBytes(i));\n    }\n    long endTime = java.lang.System.currentTimeMillis();\n    java.lang.System.out.println((\"Total Add time = \" + (endTime - startTime)) + \"ms\");\n    startTime = java.lang.System.currentTimeMillis();\n    b.finalize();\n    endTime = java.lang.System.currentTimeMillis();\n    java.lang.System.out.println((\"Total Fold time = \" + (endTime - startTime)) + \"ms\");\n    junit.framework.Assert.assertTrue(origSize >= (b.getByteSize() << 3));\n    startTime = java.lang.System.currentTimeMillis();\n    int falsePositives = 0;\n    for (int i = 0; i < ((2 * 1000) * 1000); ++i) {\n        if (b.contains(org.apache.hadoop.hbase.util.Bytes.toBytes(i))) {\n            if (i >= ((1 * 1000) * 1000))\n                falsePositives++;\n\n        } else {\n            junit.framework.Assert.assertFalse(i < ((1 * 1000) * 1000));\n        }\n    }\n    endTime = java.lang.System.currentTimeMillis();\n    java.lang.System.out.println((\"Total Contains time = \" + (endTime - startTime)) + \"ms\");\n    java.lang.System.out.println(\"False Positive = \" + falsePositives);\n    junit.framework.Assert.assertTrue(falsePositives <= (((1 * 1000) * 1000) * err));\n}",
            "ClassName": "TestByteBloomFilter",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testBloomPerf",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 32,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    java.io.ByteArrayOutputStream out = new java.io.ByteArrayOutputStream();\n    java.io.DataOutputStream dos = new java.io.DataOutputStream(out);\n    scan.write(dos);\n    return org.apache.hadoop.hbase.util.Base64.encodeBytes(out.toByteArray());\n}",
            "ClassName": "TableMapReduceUtil",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "convertScanToString",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.util.ByteBloomFilter b = new org.apache.hadoop.hbase.util.ByteBloomFilter(1003, ((float) (0.01)), org.apache.hadoop.hbase.util.Hash.MURMUR_HASH, 2);\n    b.allocBloom();\n    int origSize = b.getByteSize();\n    junit.framework.Assert.assertEquals(1204, origSize);\n    for (int i = 0; i < 12; ++i) {\n        b.add(org.apache.hadoop.hbase.util.Bytes.toBytes(i));\n    }\n    b.finalize();\n    junit.framework.Assert.assertEquals(origSize >> 2, b.getByteSize());\n    int falsePositives = 0;\n    for (int i = 0; i < 25; ++i) {\n        if (b.contains(org.apache.hadoop.hbase.util.Bytes.toBytes(i))) {\n            if (i >= 12)\n                falsePositives++;\n\n        } else {\n            junit.framework.Assert.assertFalse(i < 12);\n        }\n    }\n    junit.framework.Assert.assertTrue(falsePositives <= 1);\n}",
            "ClassName": "TestByteBloomFilter",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testBloomFold",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestByteBloomFilter",
        "Commit": "e9da90e10927e97d932c1e5248e0e53d23cf03d4",
        "CyclomaticComplexity": 7,
        "Date": "Sun, 30 May 2010 14:36:12 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testDynamicBloom",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 41,
        "NumberOfRandoms": 5,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1275230172"
    },
    {
        "Body": "{\n    util.startMiniCluster(3);\n    try {\n        runTestAtomicity(20000);\n    } finally {\n        util.shutdownMiniCluster();\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "afterAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.TestFullLogReconstruction.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFullLogReconstruction",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAcidGuarantees",
        "Commit": "d380a628bfac05b158a059306edf68baa2b33abd",
        "CyclomaticComplexity": 0,
        "Date": "Sat, 5 Jun 2010 08:54:32 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testAtomicity",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 8,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1275728072"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.util.EnvironmentEdge edge = org.apache.hadoop.hbase.util.EnvironmentEdgeManager.getDelegate();\n    org.junit.Assert.assertNotNull(edge);\n    org.junit.Assert.assertTrue(edge instanceof org.apache.hadoop.hbase.util.DefaultEnvironmentEdge);\n    org.apache.hadoop.hbase.util.EnvironmentEdgeManager.reset();\n    org.apache.hadoop.hbase.util.EnvironmentEdge edge2 = org.apache.hadoop.hbase.util.EnvironmentEdgeManager.getDelegate();\n    org.junit.Assert.assertFalse(edge == edge2);\n    org.apache.hadoop.hbase.util.IncrementingEnvironmentEdge newEdge = new org.apache.hadoop.hbase.util.IncrementingEnvironmentEdge();\n    org.apache.hadoop.hbase.util.EnvironmentEdgeManager.injectEdge(newEdge);\n    org.junit.Assert.assertEquals(newEdge, org.apache.hadoop.hbase.util.EnvironmentEdgeManager.getDelegate());\n    org.apache.hadoop.hbase.util.EnvironmentEdgeManager.injectEdge(null);\n    org.apache.hadoop.hbase.util.EnvironmentEdge nullResult = org.apache.hadoop.hbase.util.EnvironmentEdgeManager.getDelegate();\n    org.junit.Assert.assertTrue(nullResult instanceof org.apache.hadoop.hbase.util.DefaultEnvironmentEdge);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.util.EnvironmentEdgeManager.injectEdge(edge);\n}",
            "ClassName": "EnvironmentEdgeManagerTestHelper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "injectEdge",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.util.EnvironmentEdgeManager.injectEdge(new org.apache.hadoop.hbase.util.DefaultEnvironmentEdge());\n}",
            "ClassName": "EnvironmentEdgeManager",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "reset",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return org.apache.hadoop.hbase.util.EnvironmentEdgeManager.getDelegate().currentTimeMillis();\n}",
            "ClassName": "EnvironmentEdgeManager",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "currentTimeMillis",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    if (edge == null) {\n        org.apache.hadoop.hbase.util.EnvironmentEdgeManager.reset();\n    } else {\n        org.apache.hadoop.hbase.util.EnvironmentEdgeManager.delegate = edge;\n    }\n}",
            "ClassName": "EnvironmentEdgeManager",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "injectEdge",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.util.EnvironmentEdgeManager.reset();\n}",
            "ClassName": "EnvironmentEdgeManagerTestHelper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "reset",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestEnvironmentEdgeManager",
        "Commit": "fef909299afcc72e677a11288d68454eada2ce04",
        "CyclomaticComplexity": 0,
        "Date": "Tue, 15 Jun 2010 22:37:56 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testManageSingleton",
        "NumberOfAsserts": 5,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 14,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1276641476"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.master.TestMasterTransitions.LOG.info(\"Running testKillRSWithOpeningRegion2482\");\n    org.apache.hadoop.hbase.MiniHBaseCluster cluster = org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getHBaseCluster();\n    if (cluster.getLiveRegionServerThreads().size() < 2) {\n        cluster.startRegionServer();\n    }\n    int countOfMetaRegions = org.apache.hadoop.hbase.master.TestMasterTransitions.countOfMetaRegions();\n    org.apache.hadoop.hbase.master.HMaster m = cluster.getMaster();\n    org.apache.hadoop.hbase.MiniHBaseCluster.MiniHBaseClusterRegionServer hrs = ((org.apache.hadoop.hbase.MiniHBaseCluster.MiniHBaseClusterRegionServer) (cluster.startRegionServer().getRegionServer()));\n    org.apache.hadoop.hbase.master.TestMasterTransitions.LOG.info(\"Started new regionserver: \" + hrs.toString());\n    int minimumRegions = countOfMetaRegions / (cluster.getRegionServerThreads().size() * 2);\n    while (hrs.getOnlineRegions().size() < minimumRegions)\n        org.apache.hadoop.hbase.util.Threads.sleep(100);\n\n    org.apache.hadoop.hbase.master.TestMasterTransitions.HBase2482Listener listener = new org.apache.hadoop.hbase.master.TestMasterTransitions.HBase2482Listener(hrs);\n    m.getRegionServerOperationQueue().registerRegionServerOperationListener(listener);\n    try {\n        closeAllNonCatalogRegions(cluster, hrs);\n        cluster.addMessageToSendRegionServer(hrs, new org.apache.hadoop.hbase.HMsg(org.apache.hadoop.hbase.HMsg.Type.TESTING_MSG_BLOCK_RS));\n        while (!listener.closed)\n            org.apache.hadoop.hbase.util.Threads.sleep(100);\n\n        org.apache.hadoop.hbase.master.TestMasterTransitions.LOG.info(\"Past close\");\n        while (!listener.abortSent)\n            org.apache.hadoop.hbase.util.Threads.sleep(100);\n\n        org.apache.hadoop.hbase.master.TestMasterTransitions.LOG.info(\"Past abort send; waiting on all regions to redeploy\");\n        assertRegionIsBackOnline(listener.regionToFind);\n    } finally {\n        m.getRegionServerOperationQueue().unregisterRegionServerOperationListener(listener);\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration().setInt(\"hbase.regions.percheckin\", 2);\n    org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.startMiniCluster(2);\n    org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.createTable(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME), org.apache.hadoop.hbase.master.TestMasterTransitions.FAMILIES);\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    int countOfRegions = org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.createMultiRegions(t, org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily());\n    org.apache.hadoop.hbase.master.TestMasterTransitions.waitUntilAllRegionsAssigned(countOfRegions);\n    org.apache.hadoop.hbase.master.TestMasterTransitions.addToEachStartKey(countOfRegions);\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "beforeAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 10,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return org.apache.hadoop.hbase.master.TestMasterTransitions.FAMILIES[0];\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTestFamily",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.Get g = new org.apache.hadoop.hbase.client.Get(row);\n    org.junit.Assert.assertTrue(t.get(g).size() > 0);\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "assertRegionIsBackOnline",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        org.apache.hadoop.hbase.HRegionInfo hri = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(b);\n        byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(row);\n        p.add(org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily(), org.apache.hadoop.hbase.master.TestMasterTransitions.getTestQualifier(), row);\n        t.put(p);\n        rows++;\n    }\n    s.close();\n    org.junit.Assert.assertEquals(expected, rows);\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addToEachStartKey",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily();\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTestQualifier",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestMasterTransitions",
        "Commit": "609ce409a06d2d60ba88a9f950c83ed2f239bf2a",
        "CyclomaticComplexity": 4,
        "Date": "Tue, 15 Jun 2010 22:44:07 +0000",
        "HasTimeoutInAnnotations": 1,
        "Label": 1,
        "MethodName": "testKillRSWithOpeningRegion2482",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 32,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1276641847"
    },
    {
        "Body": "{\n    final byte[] TABLENAME = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testCachePrewarm\");\n    org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration();\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLENAME, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.apache.hadoop.hbase.client.HTable.setRegionCachePrefetch(conf, TABLENAME, false);\n    org.junit.Assert.assertFalse(\"The table is disabled for region cache prefetch\", org.apache.hadoop.hbase.client.HTable.getRegionCachePrefetch(conf, TABLENAME));\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, TABLENAME);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createMultiRegions(table, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.apache.hadoop.hbase.client.Get g = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaa\"));\n    table.get(g);\n    org.junit.Assert.assertEquals(\"Number of cached region is incorrect \", 1, org.apache.hadoop.hbase.client.HConnectionManager.getCachedRegionCount(conf, TABLENAME));\n    org.apache.hadoop.hbase.client.HTable.setRegionCachePrefetch(conf, TABLENAME, true);\n    org.junit.Assert.assertTrue(\"The table is enabled for region cache prefetch\", org.apache.hadoop.hbase.client.HTable.getRegionCachePrefetch(conf, TABLENAME));\n    org.apache.hadoop.hbase.client.HTable.setRegionCachePrefetch(conf, TABLENAME, false);\n    org.junit.Assert.assertFalse(\"The table is disabled for region cache prefetch\", org.apache.hadoop.hbase.client.HTable.getRegionCachePrefetch(conf, TABLENAME));\n    org.apache.hadoop.hbase.client.HTable.setRegionCachePrefetch(conf, TABLENAME, true);\n    org.junit.Assert.assertTrue(\"The table is enabled for region cache prefetch\", org.apache.hadoop.hbase.client.HTable.getRegionCachePrefetch(conf, TABLENAME));\n    table.getConnection().clearRegionCache();\n    org.junit.Assert.assertEquals(\"Number of cached region is incorrect \", 0, org.apache.hadoop.hbase.client.HConnectionManager.getCachedRegionCount(conf, TABLENAME));\n    org.apache.hadoop.hbase.client.Get g2 = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.util.Bytes.toBytes(\"bbb\"));\n    table.get(g2);\n    int prefetchRegionNumber = conf.getInt(\"hbase.client.prefetch.limit\", 10);\n    org.junit.Assert.assertEquals(\"Number of cached region is incorrect \", prefetchRegionNumber, org.apache.hadoop.hbase.client.HConnectionManager.getCachedRegionCount(conf, TABLENAME));\n    table.getConnection().clearRegionCache();\n    org.apache.hadoop.hbase.client.Get g3 = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.util.Bytes.toBytes(\"abc\"));\n    table.get(g3);\n    org.junit.Assert.assertEquals(\"Number of cached region is incorrect \", prefetchRegionNumber, org.apache.hadoop.hbase.client.HConnectionManager.getCachedRegionCount(conf, TABLENAME));\n}",
        "CUT_1": {
            "Body": "{\n    return org.apache.hadoop.hbase.client.HTable.isTableEnabled(conf, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n}",
            "ClassName": "HTable",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "isTableEnabled",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    this.server = cluster.getRegionServerThreads().get(0).getRegionServer();\n    this.log = server.getLog();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    for (int i = 1; i <= 256; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + java.lang.String.format(\"%1$04d\", i)));\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, value);\n        table.put(put);\n        if ((i % 32) == 0) {\n            try {\n                java.lang.Thread.sleep(2000);\n            } catch (java.lang.InterruptedException e) {\n            }\n        }\n    }\n}",
            "ClassName": "TestLogRolling",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startAndWriteData",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.TestZooKeeper.conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(getConfiguration(), tableName);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.ResultScanner resScan = table.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result res : resScan) {\n        org.apache.hadoop.hbase.client.Delete del = new org.apache.hadoop.hbase.client.Delete(res.getRow());\n        table.delete(del);\n    }\n    return table;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "truncateTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 10,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d7181bd7c5c2e8b22604fc0b847fdbebaff919c7",
        "CyclomaticComplexity": 0,
        "Date": "Wed, 16 Jun 2010 18:05:00 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testRegionCachePreWarm",
        "NumberOfAsserts": 8,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 28,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1276711500"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.master.TestMasterTransitions.LOG.info(\"Running testRegionCloseWhenNoMetaHBase2428\");\n    org.apache.hadoop.hbase.MiniHBaseCluster cluster = org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getHBaseCluster();\n    final org.apache.hadoop.hbase.master.HMaster master = cluster.getMaster();\n    int metaIndex = cluster.getServerWithMeta();\n    int otherServerIndex = -1;\n    for (int i = 0; i < cluster.getRegionServerThreads().size(); i++) {\n        if (i == metaIndex)\n            continue;\n\n        otherServerIndex = i;\n        break;\n    }\n    final org.apache.hadoop.hbase.regionserver.HRegionServer otherServer = cluster.getRegionServer(otherServerIndex);\n    final org.apache.hadoop.hbase.regionserver.HRegionServer metaHRS = cluster.getRegionServer(metaIndex);\n    final org.apache.hadoop.hbase.HRegionInfo hri = otherServer.getOnlineRegions().iterator().next().getRegionInfo();\n    org.apache.hadoop.hbase.master.TestMasterTransitions.HBase2428Listener listener = new org.apache.hadoop.hbase.master.TestMasterTransitions.HBase2428Listener(cluster, metaHRS.getHServerInfo().getServerAddress(), hri, otherServerIndex);\n    master.getRegionServerOperationQueue().registerRegionServerOperationListener(listener);\n    try {\n        cluster.abortRegionServer(metaIndex);\n        while (!listener.metaShutdownReceived)\n            org.apache.hadoop.hbase.util.Threads.sleep(100);\n\n        while (!listener.isDone())\n            org.apache.hadoop.hbase.util.Threads.sleep(10);\n\n        org.junit.Assert.assertTrue(listener.getCloseCount() > 1);\n        org.junit.Assert.assertTrue(listener.getCloseCount() < ((org.apache.hadoop.hbase.master.TestMasterTransitions.HBase2428Listener.SERVER_DURATION / org.apache.hadoop.hbase.master.TestMasterTransitions.HBase2428Listener.CLOSE_DURATION) * 2));\n        assertRegionIsBackOnline(hri);\n    } finally {\n        master.getRegionServerOperationQueue().unregisterRegionServerOperationListener(listener);\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration().setInt(\"hbase.regions.percheckin\", 2);\n    org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.startMiniCluster(2);\n    org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.createTable(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME), org.apache.hadoop.hbase.master.TestMasterTransitions.FAMILIES);\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    int countOfRegions = org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.createMultiRegions(t, org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily());\n    org.apache.hadoop.hbase.master.TestMasterTransitions.waitUntilAllRegionsAssigned(countOfRegions);\n    org.apache.hadoop.hbase.master.TestMasterTransitions.addToEachStartKey(countOfRegions);\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "beforeAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 10,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.Get g = new org.apache.hadoop.hbase.client.Get(row);\n    org.junit.Assert.assertTrue(t.get(g).size() > 0);\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "assertRegionIsBackOnline",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return org.apache.hadoop.hbase.master.TestMasterTransitions.FAMILIES[0];\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTestFamily",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        org.apache.hadoop.hbase.HRegionInfo hri = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(b);\n        byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(row);\n        p.add(org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily(), org.apache.hadoop.hbase.master.TestMasterTransitions.getTestQualifier(), row);\n        t.put(p);\n        rows++;\n    }\n    s.close();\n    org.junit.Assert.assertEquals(expected, rows);\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addToEachStartKey",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily();\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTestQualifier",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestMasterTransitions",
        "Commit": "d7181bd7c5c2e8b22604fc0b847fdbebaff919c7",
        "CyclomaticComplexity": 4,
        "Date": "Wed, 16 Jun 2010 18:05:00 +0000",
        "HasTimeoutInAnnotations": 1,
        "Label": 1,
        "MethodName": "testRegionCloseWhenNoMetaHBase2428",
        "NumberOfAsserts": 2,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 33,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1276711500"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.MiniHBaseCluster cluster = org.apache.hadoop.hbase.master.TestMaster.TEST_UTIL.getHBaseCluster();\n    org.apache.hadoop.hbase.master.HMaster m = cluster.getMaster();\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = org.apache.hadoop.hbase.master.TestMaster.TEST_UTIL.getHBaseAdmin();\n    org.apache.hadoop.hbase.master.TestMaster.TEST_UTIL.createTable(org.apache.hadoop.hbase.master.TestMaster.TABLENAME, org.apache.hadoop.hbase.master.TestMaster.FAMILYNAME);\n    org.apache.hadoop.hbase.master.TestMaster.TEST_UTIL.loadTable(new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMaster.TABLENAME), org.apache.hadoop.hbase.master.TestMaster.FAMILYNAME);\n    java.util.concurrent.CountDownLatch aboutToOpen = new java.util.concurrent.CountDownLatch(1);\n    java.util.concurrent.CountDownLatch proceed = new java.util.concurrent.CountDownLatch(1);\n    org.apache.hadoop.hbase.master.TestMaster.RegionOpenListener list = new org.apache.hadoop.hbase.master.TestMaster.RegionOpenListener(aboutToOpen, proceed);\n    org.apache.hadoop.hbase.executor.HBaseEventHandler.registerListener(list);\n    admin.split(org.apache.hadoop.hbase.master.TestMaster.TABLENAME);\n    aboutToOpen.await(60, java.util.concurrent.TimeUnit.SECONDS);\n    try {\n        m.getTableRegions(org.apache.hadoop.hbase.master.TestMaster.TABLENAME);\n        org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> pair = m.getTableRegionClosest(org.apache.hadoop.hbase.master.TestMaster.TABLENAME, org.apache.hadoop.hbase.util.Bytes.toBytes(\"cde\"));\n        org.junit.Assert.assertNotNull(pair);\n        m.getTableRegionFromName(pair.getFirst().getRegionName());\n    } finally {\n        proceed.countDown();\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestMaster.TEST_UTIL.startMiniCluster(1);\n}",
            "ClassName": "TestMaster",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "beforeAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestMaster.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestMaster",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "afterAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    org.apache.hadoop.hbase.master.HMaster master = this.cluster.getMaster();\n    org.apache.hadoop.hbase.HServerAddress address = master.getMasterAddress();\n    org.apache.hadoop.hbase.HTableDescriptor tableDesc = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.util.Bytes.toBytes(\"_MY_TABLE_\"));\n    org.apache.hadoop.hbase.HTableDescriptor metaTableDesc = meta.getTableDescriptor();\n    byte[] startKey0 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"f\");\n    byte[] endKey0 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"h\");\n    org.apache.hadoop.hbase.HRegionInfo regionInfo0 = new org.apache.hadoop.hbase.HRegionInfo(tableDesc, startKey0, endKey0);\n    org.apache.hadoop.hbase.HRegionInfo metaRegionInfo0 = new org.apache.hadoop.hbase.HRegionInfo(metaTableDesc, org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), regionInfo0.getRegionName());\n    org.apache.hadoop.hbase.master.MetaRegion meta0 = new org.apache.hadoop.hbase.master.MetaRegion(address, metaRegionInfo0);\n    byte[] startKey1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"j\");\n    byte[] endKey1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"m\");\n    org.apache.hadoop.hbase.HRegionInfo regionInfo1 = new org.apache.hadoop.hbase.HRegionInfo(tableDesc, startKey1, endKey1);\n    org.apache.hadoop.hbase.HRegionInfo metaRegionInfo1 = new org.apache.hadoop.hbase.HRegionInfo(metaTableDesc, regionInfo0.getRegionName(), regionInfo1.getRegionName());\n    org.apache.hadoop.hbase.master.MetaRegion meta1 = new org.apache.hadoop.hbase.master.MetaRegion(address, metaRegionInfo1);\n    org.apache.hadoop.hbase.HRegionInfo metaRegionInfo2 = new org.apache.hadoop.hbase.HRegionInfo(metaTableDesc, regionInfo1.getRegionName(), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"));\n    org.apache.hadoop.hbase.master.MetaRegion meta2 = new org.apache.hadoop.hbase.master.MetaRegion(address, metaRegionInfo2);\n    byte[] startKeyX = org.apache.hadoop.hbase.util.Bytes.toBytes(\"h\");\n    byte[] endKeyX = org.apache.hadoop.hbase.util.Bytes.toBytes(\"j\");\n    org.apache.hadoop.hbase.HRegionInfo regionInfoX = new org.apache.hadoop.hbase.HRegionInfo(tableDesc, startKeyX, endKeyX);\n    master.getRegionManager().offlineMetaRegionWithStartKey(startKey0);\n    master.getRegionManager().putMetaRegionOnline(meta0);\n    master.getRegionManager().putMetaRegionOnline(meta1);\n    master.getRegionManager().putMetaRegionOnline(meta2);\n    org.apache.hadoop.hbase.HBaseTestCase.assertEquals(metaRegionInfo1.getStartKey(), master.getRegionManager().getFirstMetaRegionForRegion(regionInfoX).getStartKey());\n    org.apache.hadoop.hbase.HBaseTestCase.assertEquals(metaRegionInfo1.getRegionName(), master.getRegionManager().getFirstMetaRegionForRegion(regionInfoX).getRegionName());\n}",
            "ClassName": "TestRegionManager",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetFirstMetaRegionForRegionAfterMetaSplit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 28,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        org.apache.hadoop.hbase.HRegionInfo hri = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(b);\n        byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(row);\n        p.add(org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily(), org.apache.hadoop.hbase.master.TestMasterTransitions.getTestQualifier(), row);\n        t.put(p);\n        rows++;\n    }\n    s.close();\n    org.junit.Assert.assertEquals(expected, rows);\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addToEachStartKey",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress>> result = new java.util.ArrayList<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress>>();\n    java.util.Set<org.apache.hadoop.hbase.master.MetaRegion> regions = this.regionManager.getMetaRegionsForTable(tableName);\n    byte[] firstRowInTable = org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.util.Bytes.toString(tableName) + \",,\");\n    for (org.apache.hadoop.hbase.master.MetaRegion m : regions) {\n        byte[] metaRegionName = m.getRegionName();\n        org.apache.hadoop.hbase.ipc.HRegionInterface srvr = this.connection.getHRegionConnection(m.getServer());\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(firstRowInTable);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        long scannerid = srvr.openScanner(metaRegionName, scan);\n        try {\n            while (true) {\n                org.apache.hadoop.hbase.client.Result data = srvr.next(scannerid);\n                if ((data == null) || (data.size() <= 0))\n                    break;\n\n                org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(data.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER));\n                if (org.apache.hadoop.hbase.util.Bytes.equals(info.getTableDesc().getName(), tableName)) {\n                    final byte[] value = data.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n                    if ((value != null) && (value.length > 0)) {\n                        org.apache.hadoop.hbase.HServerAddress server = new org.apache.hadoop.hbase.HServerAddress(org.apache.hadoop.hbase.util.Bytes.toString(value));\n                        result.add(new org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress>(info, server));\n                    }\n                } else {\n                    break;\n                }\n            } \n        } finally {\n            srvr.close(scannerid);\n        }\n    }\n    return result;\n}",
            "ClassName": "HMaster",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 34,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestMaster",
        "Commit": "d7181bd7c5c2e8b22604fc0b847fdbebaff919c7",
        "CyclomaticComplexity": 0,
        "Date": "Wed, 16 Jun 2010 18:05:00 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testMasterOpsWhileSplitting",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 21,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 18,
        "ProjectName": "hbase",
        "Timestamp": "1276711500"
    },
    {
        "Body": "{\n    addRows(this.memstore);\n    java.lang.Thread.sleep(1);\n    addRows(this.memstore);\n    org.apache.hadoop.hbase.KeyValue closestToEmpty = this.memstore.getNextRow(org.apache.hadoop.hbase.KeyValue.LOWESTKEY);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compareRows(closestToEmpty, new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(0), java.lang.System.currentTimeMillis())) == 0);\n    for (int i = 0; i < org.apache.hadoop.hbase.regionserver.TestMemStore.ROW_COUNT; i++) {\n        org.apache.hadoop.hbase.KeyValue nr = this.memstore.getNextRow(new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(i), java.lang.System.currentTimeMillis()));\n        if ((i + 1) == org.apache.hadoop.hbase.regionserver.TestMemStore.ROW_COUNT) {\n            junit.framework.Assert.assertEquals(nr, null);\n        } else {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compareRows(nr, new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(i + 1), java.lang.System.currentTimeMillis())) == 0);\n        }\n    }\n    for (int startRowId = 0; startRowId < org.apache.hadoop.hbase.regionserver.TestMemStore.ROW_COUNT; startRowId++) {\n        org.apache.hadoop.hbase.regionserver.InternalScanner scanner = new org.apache.hadoop.hbase.regionserver.StoreScanner(new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(startRowId)), org.apache.hadoop.hbase.regionserver.TestMemStore.FAMILY, java.lang.Integer.MAX_VALUE, this.memstore.comparator, null, memstore.getScanners());\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        for (int i = 0; scanner.next(results); i++) {\n            int rowId = startRowId + i;\n            junit.framework.Assert.assertTrue(\"Row name\", org.apache.hadoop.hbase.KeyValue.COMPARATOR.compareRows(results.get(0), org.apache.hadoop.hbase.util.Bytes.toBytes(rowId)) == 0);\n            junit.framework.Assert.assertEquals(\"Count of columns\", org.apache.hadoop.hbase.regionserver.TestMemStore.QUALIFIER_COUNT, results.size());\n            java.util.List<org.apache.hadoop.hbase.KeyValue> row = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n            for (org.apache.hadoop.hbase.KeyValue kv : results) {\n                row.add(kv);\n            }\n            isExpectedRowWithoutTimestamps(rowId, row);\n            results.clear();\n        }\n    }\n}",
        "CUT_1": {
            "Body": "{\n    addRows(this.memstore);\n    java.lang.Thread.sleep(1);\n    addRows(this.memstore);\n    org.apache.hadoop.hbase.KeyValue closestToEmpty = this.memstore.getNextRow(org.apache.hadoop.hbase.KeyValue.LOWESTKEY);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compareRows(closestToEmpty, new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(0), java.lang.System.currentTimeMillis())) == 0);\n    for (int i = 0; i < org.apache.hadoop.hbase.regionserver.TestMemStore.ROW_COUNT; i++) {\n        org.apache.hadoop.hbase.KeyValue nr = this.memstore.getNextRow(new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(i), java.lang.System.currentTimeMillis()));\n        if ((i + 1) == org.apache.hadoop.hbase.regionserver.TestMemStore.ROW_COUNT) {\n            junit.framework.Assert.assertEquals(nr, null);\n        } else {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compareRows(nr, new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(i + 1), java.lang.System.currentTimeMillis())) == 0);\n        }\n    }\n    for (int startRowId = 0; startRowId < org.apache.hadoop.hbase.regionserver.TestMemStore.ROW_COUNT; startRowId++) {\n        org.apache.hadoop.hbase.regionserver.InternalScanner scanner = new org.apache.hadoop.hbase.regionserver.StoreScanner(new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(startRowId)), org.apache.hadoop.hbase.regionserver.TestMemStore.FAMILY, java.lang.Integer.MAX_VALUE, this.memstore.comparator, null, memstore.getScanners());\n        java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n        for (int i = 0; scanner.next(results); i++) {\n            int rowId = startRowId + i;\n            junit.framework.Assert.assertTrue(\"Row name\", org.apache.hadoop.hbase.KeyValue.COMPARATOR.compareRows(results.get(0), org.apache.hadoop.hbase.util.Bytes.toBytes(rowId)) == 0);\n            junit.framework.Assert.assertEquals(\"Count of columns\", org.apache.hadoop.hbase.regionserver.TestMemStore.QUALIFIER_COUNT, results.size());\n            java.util.List<org.apache.hadoop.hbase.KeyValue> row = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n            for (org.apache.hadoop.hbase.KeyValue kv : results) {\n                row.add(kv);\n            }\n            isExpectedRowWithoutTimestamps(rowId, row);\n            results.clear();\n        }\n    }\n}",
            "ClassName": "TestMemStore",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetNextRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 30,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ org.apache.hadoop.hbase.KeyValueTestUtil.create(\"R1\", \"cf\", \"a\", 1, org.apache.hadoop.hbase.KeyValue.Type.Put, \"dont-care\"), org.apache.hadoop.hbase.KeyValueTestUtil.create(\"R1\", \"cf\", \"a\", 1, org.apache.hadoop.hbase.KeyValue.Type.Put, \"dont-care\") };\n    java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner> scanners = java.util.Arrays.asList(new org.apache.hadoop.hbase.regionserver.KeyValueScanner[]{ new org.apache.hadoop.hbase.regionserver.KeyValueScanFixture(org.apache.hadoop.hbase.KeyValue.COMPARATOR, kvs) });\n    org.apache.hadoop.hbase.client.Scan scanSpec = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"R1\"));\n    org.apache.hadoop.hbase.regionserver.StoreScanner scan = new org.apache.hadoop.hbase.regionserver.StoreScanner(scanSpec, CF, java.lang.Long.MAX_VALUE, org.apache.hadoop.hbase.KeyValue.COMPARATOR, getCols(\"a\"), scanners);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    junit.framework.Assert.assertEquals(true, scan.next(results));\n    junit.framework.Assert.assertEquals(1, results.size());\n    junit.framework.Assert.assertEquals(kvs[0], results.get(0));\n}",
            "ClassName": "TestStoreScanner",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanSameTimestamp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 10,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    long now = java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.KeyValue a = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\".META.,,99999999999999\"), now);\n    org.apache.hadoop.hbase.KeyValue b = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\".META.,,1\"), now);\n    org.apache.hadoop.hbase.KeyValue.KVComparator c = new org.apache.hadoop.hbase.KeyValue.RootComparator();\n    junit.framework.Assert.assertTrue(c.compare(b, a) < 0);\n    org.apache.hadoop.hbase.KeyValue aa = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\".META.,,1\"), now);\n    org.apache.hadoop.hbase.KeyValue bb = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\".META.,,1\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"info\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"regioninfo\"), 1235943454602L, ((byte[]) (null)));\n    junit.framework.Assert.assertTrue(c.compare(aa, bb) < 0);\n    org.apache.hadoop.hbase.KeyValue aaa = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"TestScanMultipleVersions,row_0500,1236020145502\"), now);\n    org.apache.hadoop.hbase.KeyValue bbb = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"TestScanMultipleVersions,,99999999999999\"), now);\n    c = new org.apache.hadoop.hbase.KeyValue.MetaComparator();\n    junit.framework.Assert.assertTrue(c.compare(bbb, aaa) < 0);\n    org.apache.hadoop.hbase.KeyValue aaaa = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"TestScanMultipleVersions,,1236023996656\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"info\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"regioninfo\"), 1236024396271L, ((byte[]) (null)));\n    junit.framework.Assert.assertTrue(c.compare(aaaa, bbb) < 0);\n    org.apache.hadoop.hbase.KeyValue x = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"TestScanMultipleVersions,row_0500,1236034574162\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"info\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), 9223372036854775807L, ((byte[]) (null)));\n    org.apache.hadoop.hbase.KeyValue y = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"TestScanMultipleVersions,row_0500,1236034574162\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"info\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"regioninfo\"), 1236034574912L, ((byte[]) (null)));\n    junit.framework.Assert.assertTrue(c.compare(x, y) < 0);\n    comparisons(new org.apache.hadoop.hbase.KeyValue.MetaComparator());\n    comparisons(new org.apache.hadoop.hbase.KeyValue.KVComparator());\n    metacomparisons(new org.apache.hadoop.hbase.KeyValue.RootComparator());\n    metacomparisons(new org.apache.hadoop.hbase.KeyValue.MetaComparator());\n}",
            "ClassName": "TestKeyValue",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testMoreComparisons",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ org.apache.hadoop.hbase.KeyValueTestUtil.create(\"R1\", \"cf\", \"a\", 1, org.apache.hadoop.hbase.KeyValue.Type.Put, \"dont-care\"), org.apache.hadoop.hbase.KeyValueTestUtil.create(\"R1\", \"cf\", \"a\", 1, org.apache.hadoop.hbase.KeyValue.Type.Delete, \"dont-care\") };\n    java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner> scanners = org.apache.hadoop.hbase.regionserver.KeyValueScanFixture.scanFixture(kvs);\n    org.apache.hadoop.hbase.client.Scan scanSpec = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"R1\"));\n    org.apache.hadoop.hbase.regionserver.StoreScanner scan = new org.apache.hadoop.hbase.regionserver.StoreScanner(scanSpec, CF, java.lang.Long.MAX_VALUE, org.apache.hadoop.hbase.KeyValue.COMPARATOR, getCols(\"a\"), scanners);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    junit.framework.Assert.assertFalse(scan.next(results));\n    junit.framework.Assert.assertEquals(0, results.size());\n}",
            "ClassName": "TestStoreScanner",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDeleteVersionSameTimestamp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.KeyValue[] kvs = new org.apache.hadoop.hbase.KeyValue[]{ org.apache.hadoop.hbase.KeyValueTestUtil.create(\"R1\", \"cf\", \"a\", 2, org.apache.hadoop.hbase.KeyValue.Type.Put, \"dont-care\"), org.apache.hadoop.hbase.KeyValueTestUtil.create(\"R1\", \"cf\", \"b\", 1, org.apache.hadoop.hbase.KeyValue.Type.Put, \"dont-care\"), org.apache.hadoop.hbase.KeyValueTestUtil.create(\"R1\", \"cf\", \"a\", 1, org.apache.hadoop.hbase.KeyValue.Type.DeleteColumn, \"dont-care\") };\n    java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner> scanners = org.apache.hadoop.hbase.regionserver.KeyValueScanFixture.scanFixture(kvs);\n    org.apache.hadoop.hbase.regionserver.StoreScanner scan = new org.apache.hadoop.hbase.regionserver.StoreScanner(new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(\"R1\")), CF, java.lang.Long.MAX_VALUE, org.apache.hadoop.hbase.KeyValue.COMPARATOR, null, scanners);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    junit.framework.Assert.assertEquals(true, scan.next(results));\n    junit.framework.Assert.assertEquals(2, results.size());\n    junit.framework.Assert.assertEquals(kvs[0], results.get(0));\n    junit.framework.Assert.assertEquals(kvs[1], results.get(1));\n}",
            "ClassName": "TestStoreScanner",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testWildCardOneVersionScan",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 10,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestMemStore",
        "Commit": "d7181bd7c5c2e8b22604fc0b847fdbebaff919c7",
        "CyclomaticComplexity": 5,
        "Date": "Wed, 16 Jun 2010 18:05:00 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testGetNextRow",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 1,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 30,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 2,
        "ProjectName": "hbase",
        "Timestamp": "1276711500"
    },
    {
        "Body": "{\n    util.startMiniCluster(1);\n    try {\n        runTestAtomicity(20000, 5, 5, 0, 3);\n    } finally {\n        util.shutdownMiniCluster();\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "afterAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestZKBasedCloseRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "afterAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestZKBasedReopenRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "afterAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAcidGuarantees",
        "Commit": "d7181bd7c5c2e8b22604fc0b847fdbebaff919c7",
        "CyclomaticComplexity": 0,
        "Date": "Wed, 16 Jun 2010 18:05:00 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testGetAtomicity",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 8,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1276711500"
    },
    {
        "Body": "{\n    long maxSize = 100000;\n    long blockSize = calculateBlockSizeDefault(maxSize, 9);\n    org.apache.hadoop.hbase.io.hfile.LruBlockCache cache = new org.apache.hadoop.hbase.io.hfile.LruBlockCache(maxSize, blockSize);\n    org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.Block[] blocks = generateFixedBlocks(10, blockSize, \"block\");\n    for (org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.Block block : blocks) {\n        cache.cacheBlock(block.blockName, block.buf);\n    }\n    int n = 0;\n    while (cache.getEvictionCount() == 0) {\n        java.lang.System.out.println(\"sleep\");\n        java.lang.Thread.sleep(1000);\n        junit.framework.Assert.assertTrue((n++) < 2);\n    } \n    java.lang.System.out.println(\"Background Evictions run: \" + cache.getEvictionCount());\n    junit.framework.Assert.assertEquals(cache.getEvictionCount(), 1);\n}",
        "CUT_1": {
            "Body": "{\n    long maxSize = 100000;\n    long blockSize = calculateBlockSizeDefault(maxSize, 9);\n    org.apache.hadoop.hbase.io.hfile.LruBlockCache cache = new org.apache.hadoop.hbase.io.hfile.LruBlockCache(maxSize, blockSize);\n    org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.Block[] blocks = generateFixedBlocks(10, blockSize, \"block\");\n    for (org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.Block block : blocks) {\n        cache.cacheBlock(block.blockName, block.buf);\n    }\n    int n = 0;\n    while (cache.getEvictionCount() == 0) {\n        java.lang.System.out.println(\"sleep\");\n        java.lang.Thread.sleep(1000);\n        junit.framework.Assert.assertTrue((n++) < 2);\n    } \n    java.lang.System.out.println(\"Background Evictions run: \" + cache.getEvictionCount());\n    junit.framework.Assert.assertEquals(cache.getEvictionCount(), 1);\n}",
            "ClassName": "TestLruBlockCache",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testBackgroundEvictionThread",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 17,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    long maxSize = 100000;\n    long blockSize = calculateBlockSizeDefault(maxSize, 10);\n    org.apache.hadoop.hbase.io.hfile.LruBlockCache cache = new org.apache.hadoop.hbase.io.hfile.LruBlockCache(maxSize, blockSize, false);\n    org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.Block[] blocks = generateFixedBlocks(10, blockSize, \"block\");\n    long expectedCacheSize = cache.heapSize();\n    for (org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.Block block : blocks) {\n        cache.cacheBlock(block.blockName, block.buf);\n        expectedCacheSize += block.heapSize();\n    }\n    junit.framework.Assert.assertEquals(1, cache.getEvictionCount());\n    junit.framework.Assert.assertTrue(expectedCacheSize > (maxSize * org.apache.hadoop.hbase.io.hfile.LruBlockCache.DEFAULT_ACCEPTABLE_FACTOR));\n    junit.framework.Assert.assertTrue(cache.heapSize() < maxSize);\n    junit.framework.Assert.assertTrue(cache.heapSize() < (maxSize * org.apache.hadoop.hbase.io.hfile.LruBlockCache.DEFAULT_ACCEPTABLE_FACTOR));\n    junit.framework.Assert.assertTrue(cache.getBlock(blocks[0].blockName) == null);\n    junit.framework.Assert.assertTrue(cache.getBlock(blocks[1].blockName) == null);\n    for (int i = 2; i < blocks.length; i++) {\n        junit.framework.Assert.assertEquals(cache.getBlock(blocks[i].blockName), blocks[i].buf);\n    }\n}",
            "ClassName": "TestLruBlockCache",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testCacheEvictionSimple",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 20,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    long maxSize = 1000000;\n    long blockSize = calculateBlockSizeDefault(maxSize, 101);\n    org.apache.hadoop.hbase.io.hfile.LruBlockCache cache = new org.apache.hadoop.hbase.io.hfile.LruBlockCache(maxSize, blockSize);\n    org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.Block[] blocks = generateRandomBlocks(100, blockSize);\n    long expectedCacheSize = cache.heapSize();\n    for (org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.Block block : blocks) {\n        junit.framework.Assert.assertTrue(cache.getBlock(block.blockName) == null);\n    }\n    for (org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.Block block : blocks) {\n        cache.cacheBlock(block.blockName, block.buf);\n        expectedCacheSize += block.heapSize();\n    }\n    junit.framework.Assert.assertEquals(expectedCacheSize, cache.heapSize());\n    for (org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.Block block : blocks) {\n        java.nio.ByteBuffer buf = cache.getBlock(block.blockName);\n        junit.framework.Assert.assertTrue(buf != null);\n        junit.framework.Assert.assertEquals(buf.capacity(), block.buf.capacity());\n    }\n    for (org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.Block block : blocks) {\n        try {\n            cache.cacheBlock(block.blockName, block.buf);\n            junit.framework.Assert.assertTrue(\"Cache should not allow re-caching a block\", false);\n        } catch (java.lang.RuntimeException re) {\n        }\n    }\n    junit.framework.Assert.assertEquals(expectedCacheSize, cache.heapSize());\n    for (org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.Block block : blocks) {\n        java.nio.ByteBuffer buf = cache.getBlock(block.blockName);\n        junit.framework.Assert.assertTrue(buf != null);\n        junit.framework.Assert.assertEquals(buf.capacity(), block.buf.capacity());\n    }\n    junit.framework.Assert.assertEquals(0, cache.getEvictionCount());\n}",
            "ClassName": "TestLruBlockCache",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testCacheSimple",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 34,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    long maxSize = 100000;\n    long blockSize = calculateBlockSizeDefault(maxSize, 10);\n    org.apache.hadoop.hbase.io.hfile.LruBlockCache cache = new org.apache.hadoop.hbase.io.hfile.LruBlockCache(maxSize, blockSize, false);\n    org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.Block[] singleBlocks = generateFixedBlocks(5, 10000, \"single\");\n    org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.Block[] multiBlocks = generateFixedBlocks(5, 10000, \"multi\");\n    long expectedCacheSize = cache.heapSize();\n    for (org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.Block block : multiBlocks) {\n        cache.cacheBlock(block.blockName, block.buf);\n        expectedCacheSize += block.heapSize();\n        junit.framework.Assert.assertEquals(cache.getBlock(block.blockName), block.buf);\n    }\n    for (org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.Block block : singleBlocks) {\n        cache.cacheBlock(block.blockName, block.buf);\n        expectedCacheSize += block.heapSize();\n    }\n    junit.framework.Assert.assertEquals(cache.getEvictionCount(), 1);\n    junit.framework.Assert.assertEquals(cache.getEvictedCount(), 2);\n    junit.framework.Assert.assertTrue(expectedCacheSize > (maxSize * org.apache.hadoop.hbase.io.hfile.LruBlockCache.DEFAULT_ACCEPTABLE_FACTOR));\n    junit.framework.Assert.assertTrue(cache.heapSize() <= maxSize);\n    junit.framework.Assert.assertTrue(cache.heapSize() <= (maxSize * org.apache.hadoop.hbase.io.hfile.LruBlockCache.DEFAULT_ACCEPTABLE_FACTOR));\n    junit.framework.Assert.assertTrue(cache.getBlock(singleBlocks[0].blockName) == null);\n    junit.framework.Assert.assertTrue(cache.getBlock(multiBlocks[0].blockName) == null);\n    for (int i = 1; i < 4; i++) {\n        junit.framework.Assert.assertEquals(cache.getBlock(singleBlocks[i].blockName), singleBlocks[i].buf);\n        junit.framework.Assert.assertEquals(cache.getBlock(multiBlocks[i].blockName), multiBlocks[i].buf);\n    }\n}",
            "ClassName": "TestLruBlockCache",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testCacheEvictionTwoPriorities",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 28,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    long maxSize = 100000;\n    long blockSize = calculateBlockSize(maxSize, 10);\n    org.apache.hadoop.hbase.io.hfile.LruBlockCache cache = new org.apache.hadoop.hbase.io.hfile.LruBlockCache(maxSize, blockSize, false, ((int) (java.lang.Math.ceil((1.2 * maxSize) / blockSize))), org.apache.hadoop.hbase.io.hfile.LruBlockCache.DEFAULT_LOAD_FACTOR, org.apache.hadoop.hbase.io.hfile.LruBlockCache.DEFAULT_CONCURRENCY_LEVEL, 0.66F, 0.99F, 0.33F, 0.33F, 0.34F);\n    org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.Block[] singleBlocks = generateFixedBlocks(20, blockSize, \"single\");\n    org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.Block[] multiBlocks = generateFixedBlocks(5, blockSize, \"multi\");\n    for (org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.Block block : multiBlocks) {\n        cache.cacheBlock(block.blockName, block.buf);\n        cache.getBlock(block.blockName);\n    }\n    for (int i = 0; i < 5; i++) {\n        cache.cacheBlock(singleBlocks[i].blockName, singleBlocks[i].buf);\n    }\n    junit.framework.Assert.assertEquals(1, cache.getEvictionCount());\n    junit.framework.Assert.assertEquals(4, cache.getEvictedCount());\n    junit.framework.Assert.assertEquals(null, cache.getBlock(singleBlocks[0].blockName));\n    junit.framework.Assert.assertEquals(null, cache.getBlock(singleBlocks[1].blockName));\n    junit.framework.Assert.assertEquals(null, cache.getBlock(multiBlocks[0].blockName));\n    junit.framework.Assert.assertEquals(null, cache.getBlock(multiBlocks[1].blockName));\n    for (int i = 5; i < 18; i++) {\n        cache.cacheBlock(singleBlocks[i].blockName, singleBlocks[i].buf);\n    }\n    junit.framework.Assert.assertEquals(4, cache.getEvictionCount());\n    junit.framework.Assert.assertEquals(16, cache.getEvictedCount());\n    junit.framework.Assert.assertEquals(7, cache.size());\n}",
            "ClassName": "TestLruBlockCache",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanResistance",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 26,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestLruBlockCache",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 2,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 1,
        "MethodName": "testBackgroundEvictionThread",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 1,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 17,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 2,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    generateHLogs(-1);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    org.apache.hadoop.fs.FileStatus[] statuses = null;\n    try {\n        statuses = fs.listStatus(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir);\n        org.junit.Assert.assertNull(statuses);\n    } catch (java.io.FileNotFoundException e) {\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.removeAll(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions);\n    for (int i = 0; i < 100; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.add(\"region__\" + i);\n    }\n    generateHLogs(1, 100, -1);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    fs.rename(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir);\n    org.apache.hadoop.fs.Path firstSplitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME) + \".first\");\n    org.apache.hadoop.fs.Path splitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME));\n    fs.rename(splitPath, firstSplitPath);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    org.junit.Assert.assertEquals(0, compareHLogSplitDirs(firstSplitPath, splitPath));\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSplittingLargeNumberOfRegionsConsistency",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    for (int i = 0; i < writers; i++) {\n        writer[i] = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + i), conf);\n        for (int j = 0; j < entries; j++) {\n            int prefix = 0;\n            for (java.lang.String region : org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions) {\n                java.lang.String row_key = ((region + (prefix++)) + i) + j;\n                appendEntry(writer[i], org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, region.getBytes(), row_key.getBytes(), org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.FAMILY, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.QUALIFIER, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.VALUE, seq);\n            }\n        }\n        if (i != leaveOpen) {\n            writer[i].close();\n            flushToConsole(\"Closing writer \" + i);\n        }\n    }\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "generateHLogs",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Writer writer = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + suffix), conf);\n    if (closeFile)\n        writer.close();\n\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "injectEmptyFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.flushlogentries\", 1);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setStrings(\"hbase.rootdir\", org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir.toString());\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setClass(\"hbase.regionserver.hlog.writer.impl\", org.apache.hadoop.hbase.regionserver.wal.InstrumentedSequenceFileLogWriter.class, org.apache.hadoop.hbase.regionserver.wal.HLog.Writer.class);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.startMiniDFSCluster(2);\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    conf = org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration();\n    fs = org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getDFSCluster().getFileSystem();\n    org.apache.hadoop.fs.FileStatus[] entries = fs.listStatus(new org.apache.hadoop.fs.Path(\"/\"));\n    for (org.apache.hadoop.fs.FileStatus dir : entries) {\n        fs.delete(dir.getPath(), true);\n    }\n    seq = 0;\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions = new java.util.ArrayList<java.lang.String>();\n    java.util.Collections.addAll(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions, \"bbb\", \"ccc\");\n    org.apache.hadoop.hbase.regionserver.wal.InstrumentedSequenceFileLogWriter.activateFailure = false;\n    org.apache.hadoop.hdfs.MiniDFSCluster dfsCluster = org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getDFSCluster();\n    java.lang.reflect.Field field = dfsCluster.getClass().getDeclaredField(\"nameNode\");\n    field.setAccessible(true);\n    org.apache.hadoop.hdfs.server.namenode.NameNode nn = ((org.apache.hadoop.hdfs.server.namenode.NameNode) (field.get(dfsCluster)));\n    nn.namesystem.leaseManager.setLeasePeriod(100, 50000);\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 17,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHLogSplit",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testLogDirectoryShouldBeDeletedAfterSuccessfulSplit",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 1,
        "NumberOfLines": 11,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = new org.apache.hadoop.hbase.client.HTablePool(org.apache.hadoop.hbase.client.TestHTablePool.TEST_UTIL.getConfiguration(), 4);\n    java.lang.String tableName = \"testTable\";\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestHTablePool.TEST_UTIL.getConfiguration());\n    if (admin.tableExists(tableName)) {\n        admin.deleteTable(tableName);\n    }\n    org.apache.hadoop.hbase.HTableDescriptor tableDescriptor = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    tableDescriptor.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(\"randomFamily\"));\n    admin.createTable(tableDescriptor);\n    org.apache.hadoop.hbase.client.HTableInterface[] tables = new org.apache.hadoop.hbase.client.HTableInterface[4];\n    for (int i = 0; i < 4; ++i) {\n        tables[i] = pool.getTable(tableName);\n    }\n    pool.closeTablePool(tableName);\n    for (int i = 0; i < 4; ++i) {\n        pool.putTable(tables[i]);\n    }\n    junit.framework.Assert.assertEquals(4, pool.getCurrentPoolSize(tableName));\n    pool.closeTablePool(tableName);\n    junit.framework.Assert.assertEquals(0, pool.getCurrentPoolSize(tableName));\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    try {\n        return ((org.apache.hadoop.hbase.client.HTable) (table)).getRegionsInfo();\n    } finally {\n        pool.putTable(table);\n    }\n}",
            "ClassName": "RegionsResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    try {\n        return table.getTableDescriptor();\n    } finally {\n        pool.putTable(table);\n    }\n}",
            "ClassName": "SchemaResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableSchema",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return pool;\n}",
            "ClassName": "RESTServlet",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTablePool",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.TestZooKeeper.conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHTablePool",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 3,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testCloseTablePool",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 22,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    testScan(\"opp\", null, \"zzz\");\n}",
        "CUT_1": {
            "Body": "{\n    return ((x == null) && (y == null)) || ((x != null) && x.equals(y));\n}",
            "ClassName": "Pair",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "equals",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HalfStoreFileReader",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "midkey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return null;\n}",
            "ClassName": "WildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return null;\n}",
            "ClassName": "ScanWildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "internalPreFlushcacheCommit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableInputFormatScan",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testScanOPPToEmpty",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    conf.setBoolean(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HBASE_SKIP_ERRORS, true);\n    generateHLogs(java.lang.Integer.MAX_VALUE);\n    corruptHLog(new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + \"5\"), org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.Corruptions.INSERT_GARBAGE_ON_FIRST_LINE, true, fs);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    for (java.lang.String region : org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions) {\n        org.apache.hadoop.fs.Path logfile = getLogForRegion(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, region);\n        org.junit.Assert.assertEquals((org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.NUM_WRITERS - 1) * org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.ENTRIES, countHLog(logfile, fs, conf));\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.removeAll(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions);\n    for (int i = 0; i < 100; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.add(\"region__\" + i);\n    }\n    generateHLogs(1, 100, -1);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    fs.rename(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir);\n    org.apache.hadoop.fs.Path firstSplitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME) + \".first\");\n    org.apache.hadoop.fs.Path splitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME));\n    fs.rename(splitPath, firstSplitPath);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    org.junit.Assert.assertEquals(0, compareHLogSplitDirs(firstSplitPath, splitPath));\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSplittingLargeNumberOfRegionsConsistency",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    for (int i = 0; i < writers; i++) {\n        writer[i] = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + i), conf);\n        for (int j = 0; j < entries; j++) {\n            int prefix = 0;\n            for (java.lang.String region : org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions) {\n                java.lang.String row_key = ((region + (prefix++)) + i) + j;\n                appendEntry(writer[i], org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, region.getBytes(), row_key.getBytes(), org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.FAMILY, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.QUALIFIER, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.VALUE, seq);\n            }\n        }\n        if (i != leaveOpen) {\n            writer[i].close();\n            flushToConsole(\"Closing writer \" + i);\n        }\n    }\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "generateHLogs",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.flushlogentries\", 1);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setStrings(\"hbase.rootdir\", org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir.toString());\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setClass(\"hbase.regionserver.hlog.writer.impl\", org.apache.hadoop.hbase.regionserver.wal.InstrumentedSequenceFileLogWriter.class, org.apache.hadoop.hbase.regionserver.wal.HLog.Writer.class);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.startMiniDFSCluster(2);\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Writer writer = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + suffix), conf);\n    if (closeFile)\n        writer.close();\n\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "injectEmptyFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.shutdownMiniDFSCluster();\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHLogSplit",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 1,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testFirstLineCorruptionLogFileSkipErrorsPasses",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 11,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.HTable table = org.apache.hadoop.hbase.client.TestHCM.TEST_UTIL.createTable(org.apache.hadoop.hbase.client.TestHCM.TABLE_NAME, org.apache.hadoop.hbase.client.TestHCM.FAM_NAM);\n    org.apache.hadoop.hbase.client.TestHCM.TEST_UTIL.createMultiRegions(table, org.apache.hadoop.hbase.client.TestHCM.FAM_NAM);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestHCM.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestHCM.FAM_NAM, org.apache.hadoop.hbase.client.TestHCM.ROW, org.apache.hadoop.hbase.client.TestHCM.ROW);\n    table.put(put);\n    org.apache.hadoop.hbase.client.HConnectionManager.TableServers conn = ((org.apache.hadoop.hbase.client.HConnectionManager.TableServers) (table.getConnection()));\n    org.junit.Assert.assertNotNull(conn.getCachedLocation(org.apache.hadoop.hbase.client.TestHCM.TABLE_NAME, org.apache.hadoop.hbase.client.TestHCM.ROW));\n    conn.deleteCachedLocation(org.apache.hadoop.hbase.client.TestHCM.TABLE_NAME, org.apache.hadoop.hbase.client.TestHCM.ROW);\n    org.apache.hadoop.hbase.HRegionLocation rl = conn.getCachedLocation(org.apache.hadoop.hbase.client.TestHCM.TABLE_NAME, org.apache.hadoop.hbase.client.TestHCM.ROW);\n    org.junit.Assert.assertNull(\"What is this location?? \" + rl, rl);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestHCM.TEST_UTIL.startMiniCluster(1);\n}",
            "ClassName": "TestHCM",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    this.server = cluster.getRegionServerThreads().get(0).getRegionServer();\n    this.log = server.getLog();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    for (int i = 1; i <= 256; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + java.lang.String.format(\"%1$04d\", i)));\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, value);\n        table.put(put);\n        if ((i % 32) == 0) {\n            try {\n                java.lang.Thread.sleep(2000);\n            } catch (java.lang.InterruptedException e) {\n            }\n        }\n    }\n}",
            "ClassName": "TestLogRolling",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startAndWriteData",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        org.apache.hadoop.hbase.HRegionInfo hri = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(b);\n        byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(row);\n        p.add(org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily(), org.apache.hadoop.hbase.master.TestMasterTransitions.getTestQualifier(), row);\n        t.put(p);\n        rows++;\n    }\n    s.close();\n    org.junit.Assert.assertEquals(expected, rows);\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addToEachStartKey",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.TestZooKeeper.conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHCM",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testRegionCaching",
        "NumberOfAsserts": 2,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 12,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testMaxKeyValueSize\");\n    org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration();\n    java.lang.String oldMaxSize = conf.get(\"hbase.client.keyvalue.maxsize\");\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    byte[] value = new byte[(4 * 1024) * 1024];\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, value);\n    ht.put(put);\n    try {\n        conf.setInt(\"hbase.client.keyvalue.maxsize\", (2 * 1024) * 1024);\n        TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testMaxKeyValueSize2\");\n        ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n        put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n        put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, value);\n        ht.put(put);\n        org.junit.Assert.fail(\"Inserting a too large KeyValue worked, should throw exception\");\n    } catch (java.lang.Exception e) {\n    }\n    conf.set(\"hbase.client.keyvalue.maxsize\", oldMaxSize);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.TestZooKeeper.conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testMaxKeyValueSize",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 21,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    LOG.info(\"Starting testRegionServerSessionExpired\");\n    new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.TestZooKeeper.conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.expireRegionServerSession(0);\n    testSanity();\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.startMiniZKCluster();\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.startMiniCluster(2);\n    org.apache.hadoop.hbase.TestZooKeeper.conf = org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.getConfiguration();\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.ensureSomeRegionServersAvailable(2);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(this.conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    java.util.List<byte[]> rows = new java.util.ArrayList<byte[]>();\n    org.apache.hadoop.hbase.client.ResultScanner s = t.getScanner(new org.apache.hadoop.hbase.client.Scan());\n    for (org.apache.hadoop.hbase.client.Result result : s) {\n        org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(result.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER));\n        org.apache.hadoop.hbase.HTableDescriptor desc = info.getTableDesc();\n        if (org.apache.hadoop.hbase.util.Bytes.compareTo(desc.getName(), tableName) == 0) {\n            LOG.info(\"getMetaTableRows: row -> \" + org.apache.hadoop.hbase.util.Bytes.toStringBinary(result.getRow()));\n            rows.add(result.getRow());\n        }\n    }\n    s.close();\n    return rows;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getMetaTableRows",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 15,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        org.apache.hadoop.hbase.HRegionInfo hri = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(b);\n        byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(row);\n        p.add(org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily(), org.apache.hadoop.hbase.master.TestMasterTransitions.getTestQualifier(), row);\n        t.put(p);\n        rows++;\n    }\n    s.close();\n    org.junit.Assert.assertEquals(expected, rows);\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addToEachStartKey",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestZooKeeper",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testRegionServerSessionExpired",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 6,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.mapreduce.ImportTsv.TsvParser parser = new org.apache.hadoop.hbase.mapreduce.ImportTsv.TsvParser(\"col_a,col_b:qual,HBASE_ROW_KEY,col_d\", \"\\t\");\n    assertBytesEquals(org.apache.hadoop.hbase.util.Bytes.toBytes(\"col_a\"), parser.getFamily(0));\n    assertBytesEquals(org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY, parser.getQualifier(0));\n    assertBytesEquals(org.apache.hadoop.hbase.util.Bytes.toBytes(\"col_b\"), parser.getFamily(1));\n    assertBytesEquals(org.apache.hadoop.hbase.util.Bytes.toBytes(\"qual\"), parser.getQualifier(1));\n    org.junit.Assert.assertNull(parser.getFamily(2));\n    org.junit.Assert.assertNull(parser.getQualifier(2));\n    byte[] line = org.apache.hadoop.hbase.util.Bytes.toBytes(\"val_a\\tval_b\\tval_c\\tval_d\");\n    org.apache.hadoop.hbase.mapreduce.ImportTsv.TsvParser.ParsedLine parsed = parser.parse(line, line.length);\n    checkParsing(parsed, com.google.common.base.Splitter.on(\"\\t\").split(org.apache.hadoop.hbase.util.Bytes.toString(line)));\n    org.junit.Assert.assertEquals(2, parser.getRowKeyColumnIndex());\n}",
        "CUT_1": {
            "Body": "{\n    return new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(row), org.apache.hadoop.hbase.util.Bytes.toBytes(family), org.apache.hadoop.hbase.util.Bytes.toBytes(qualifier), timestamp, type, org.apache.hadoop.hbase.util.Bytes.toBytes(value));\n}",
            "ClassName": "KeyValueTestUtil",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "create",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.HRegionInfo a = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(\"a\"), null, null);\n    org.apache.hadoop.hbase.HRegionInfo b = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(\"b\"), null, null);\n    junit.framework.Assert.assertTrue(a.compareTo(b) != 0);\n    org.apache.hadoop.hbase.HTableDescriptor t = new org.apache.hadoop.hbase.HTableDescriptor(\"t\");\n    byte[] midway = org.apache.hadoop.hbase.util.Bytes.toBytes(\"midway\");\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, null, midway);\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, midway, null);\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n    junit.framework.Assert.assertTrue(b.compareTo(a) > 0);\n    junit.framework.Assert.assertEquals(a, a);\n    junit.framework.Assert.assertTrue(a.compareTo(a) == 0);\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"a\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"d\"));\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"e\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"g\"));\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"dddd\"));\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"e\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"g\"));\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"dddd\"));\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"eeee\"));\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n}",
            "ClassName": "TestCompare",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testHRegionInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.startsWith(org.apache.hadoop.hbase.util.Bytes.toBytes(\"hello\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"h\")));\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.startsWith(org.apache.hadoop.hbase.util.Bytes.toBytes(\"hello\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\")));\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.startsWith(org.apache.hadoop.hbase.util.Bytes.toBytes(\"hello\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"hello\")));\n    junit.framework.Assert.assertFalse(org.apache.hadoop.hbase.util.Bytes.startsWith(org.apache.hadoop.hbase.util.Bytes.toBytes(\"hello\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"helloworld\")));\n    junit.framework.Assert.assertFalse(org.apache.hadoop.hbase.util.Bytes.startsWith(org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"hello\")));\n}",
            "ClassName": "TestBytes",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testStartsWith",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.HBaseConfiguration.create();\n    java.lang.String[] otherArgs = new org.apache.hadoop.util.GenericOptionsParser(conf, args).getRemainingArgs();\n    if (otherArgs.length < 2) {\n        org.apache.hadoop.hbase.mapreduce.ImportTsv.usage(\"Wrong number of arguments: \" + otherArgs.length);\n        java.lang.System.exit(-1);\n    }\n    java.lang.String[] columns = conf.getStrings(org.apache.hadoop.hbase.mapreduce.ImportTsv.COLUMNS_CONF_KEY);\n    if (columns == null) {\n        org.apache.hadoop.hbase.mapreduce.ImportTsv.usage((\"No columns specified. Please specify with -D\" + org.apache.hadoop.hbase.mapreduce.ImportTsv.COLUMNS_CONF_KEY) + \"=...\");\n        java.lang.System.exit(-1);\n    }\n    int rowkeysFound = 0;\n    for (java.lang.String col : columns) {\n        if (col.equals(org.apache.hadoop.hbase.mapreduce.ImportTsv.TsvParser.ROWKEY_COLUMN_SPEC))\n            rowkeysFound++;\n\n    }\n    if (rowkeysFound != 1) {\n        org.apache.hadoop.hbase.mapreduce.ImportTsv.usage(\"Must specify exactly one column as \" + org.apache.hadoop.hbase.mapreduce.ImportTsv.TsvParser.ROWKEY_COLUMN_SPEC);\n        java.lang.System.exit(-1);\n    }\n    org.apache.hadoop.mapreduce.Job job = org.apache.hadoop.hbase.mapreduce.ImportTsv.createSubmittableJob(conf, otherArgs);\n    java.lang.System.exit(job.waitForCompletion(true) ? 0 : 1);\n}",
            "ClassName": "ImportTsv",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "main",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 25,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    long now = java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.KeyValue a = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\".META.,,99999999999999\"), now);\n    org.apache.hadoop.hbase.KeyValue b = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\".META.,,1\"), now);\n    org.apache.hadoop.hbase.KeyValue.KVComparator c = new org.apache.hadoop.hbase.KeyValue.RootComparator();\n    junit.framework.Assert.assertTrue(c.compare(b, a) < 0);\n    org.apache.hadoop.hbase.KeyValue aa = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\".META.,,1\"), now);\n    org.apache.hadoop.hbase.KeyValue bb = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\".META.,,1\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"info\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"regioninfo\"), 1235943454602L, ((byte[]) (null)));\n    junit.framework.Assert.assertTrue(c.compare(aa, bb) < 0);\n    org.apache.hadoop.hbase.KeyValue aaa = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"TestScanMultipleVersions,row_0500,1236020145502\"), now);\n    org.apache.hadoop.hbase.KeyValue bbb = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"TestScanMultipleVersions,,99999999999999\"), now);\n    c = new org.apache.hadoop.hbase.KeyValue.MetaComparator();\n    junit.framework.Assert.assertTrue(c.compare(bbb, aaa) < 0);\n    org.apache.hadoop.hbase.KeyValue aaaa = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"TestScanMultipleVersions,,1236023996656\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"info\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"regioninfo\"), 1236024396271L, ((byte[]) (null)));\n    junit.framework.Assert.assertTrue(c.compare(aaaa, bbb) < 0);\n    org.apache.hadoop.hbase.KeyValue x = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"TestScanMultipleVersions,row_0500,1236034574162\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"info\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), 9223372036854775807L, ((byte[]) (null)));\n    org.apache.hadoop.hbase.KeyValue y = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"TestScanMultipleVersions,row_0500,1236034574162\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"info\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"regioninfo\"), 1236034574912L, ((byte[]) (null)));\n    junit.framework.Assert.assertTrue(c.compare(x, y) < 0);\n    comparisons(new org.apache.hadoop.hbase.KeyValue.MetaComparator());\n    comparisons(new org.apache.hadoop.hbase.KeyValue.KVComparator());\n    metacomparisons(new org.apache.hadoop.hbase.KeyValue.RootComparator());\n    metacomparisons(new org.apache.hadoop.hbase.KeyValue.MetaComparator());\n}",
            "ClassName": "TestKeyValue",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testMoreComparisons",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestImportTsv",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testTsvParser",
        "NumberOfAsserts": 3,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 13,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    conf.setBoolean(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HBASE_SKIP_ERRORS, false);\n    generateHLogs(-1);\n    corruptHLog(new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + \"5\"), org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.Corruptions.APPEND_GARBAGE, true, fs);\n    fs.initialize(fs.getUri(), conf);\n    try {\n        org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    } catch (java.io.IOException e) {\n    }\n    org.junit.Assert.assertEquals(\"if skip.errors is false all files should remain in place\", org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.NUM_WRITERS, fs.listStatus(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir).length);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.removeAll(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions);\n    for (int i = 0; i < 100; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.add(\"region__\" + i);\n    }\n    generateHLogs(1, 100, -1);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    fs.rename(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir);\n    org.apache.hadoop.fs.Path firstSplitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME) + \".first\");\n    org.apache.hadoop.fs.Path splitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME));\n    fs.rename(splitPath, firstSplitPath);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    org.junit.Assert.assertEquals(0, compareHLogSplitDirs(firstSplitPath, splitPath));\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSplittingLargeNumberOfRegionsConsistency",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    for (int i = 0; i < writers; i++) {\n        writer[i] = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + i), conf);\n        for (int j = 0; j < entries; j++) {\n            int prefix = 0;\n            for (java.lang.String region : org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions) {\n                java.lang.String row_key = ((region + (prefix++)) + i) + j;\n                appendEntry(writer[i], org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, region.getBytes(), row_key.getBytes(), org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.FAMILY, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.QUALIFIER, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.VALUE, seq);\n            }\n        }\n        if (i != leaveOpen) {\n            writer[i].close();\n            flushToConsole(\"Closing writer \" + i);\n        }\n    }\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "generateHLogs",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Writer writer = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + suffix), conf);\n    if (closeFile)\n        writer.close();\n\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "injectEmptyFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.flushlogentries\", 1);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setStrings(\"hbase.rootdir\", org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir.toString());\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setClass(\"hbase.regionserver.hlog.writer.impl\", org.apache.hadoop.hbase.regionserver.wal.InstrumentedSequenceFileLogWriter.class, org.apache.hadoop.hbase.regionserver.wal.HLog.Writer.class);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.startMiniDFSCluster(2);\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.shutdownMiniDFSCluster();\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHLogSplit",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testCorruptedLogFilesSkipErrorsFalseDoesNotTouchLogs",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 11,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    testScan(null, \"bba\", \"baz\");\n}",
        "CUT_1": {
            "Body": "{\n    return ((x == null) && (y == null)) || ((x != null) && x.equals(y));\n}",
            "ClassName": "Pair",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "equals",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return null;\n}",
            "ClassName": "ScanWildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HalfStoreFileReader",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "midkey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "internalPreFlushcacheCommit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return null;\n}",
            "ClassName": "WildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableInputFormatScan",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testScanEmptyToBBA",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.util.DefaultEnvironmentEdge edge = new org.apache.hadoop.hbase.util.DefaultEnvironmentEdge();\n    long systemTime = java.lang.System.currentTimeMillis();\n    long edgeTime = edge.currentTimeMillis();\n    junit.framework.Assert.assertTrue(\"System time must be either the same or less than the edge time\", (systemTime < edgeTime) || (systemTime == edgeTime));\n    try {\n        java.lang.Thread.sleep(1);\n    } catch (java.lang.InterruptedException e) {\n        junit.framework.Assert.fail(e.getMessage());\n    }\n    long secondEdgeTime = edge.currentTimeMillis();\n    junit.framework.Assert.assertTrue(\"Second time must be greater than the first\", secondEdgeTime > edgeTime);\n}",
        "CUT_1": {
            "Body": "{\n    if (edge == null) {\n        org.apache.hadoop.hbase.util.EnvironmentEdgeManager.reset();\n    } else {\n        org.apache.hadoop.hbase.util.EnvironmentEdgeManager.delegate = edge;\n    }\n}",
            "ClassName": "EnvironmentEdgeManager",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "injectEdge",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.util.EnvironmentEdgeManager.injectEdge(edge);\n}",
            "ClassName": "EnvironmentEdgeManagerTestHelper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "injectEdge",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return java.lang.System.currentTimeMillis();\n}",
            "ClassName": "DefaultEnvironmentEdge",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "currentTimeMillis",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    long time = 0;\n    long currentTime = java.lang.System.currentTimeMillis();\n    java.lang.String[] parts = filePath.getName().split(\"\\\\.\");\n    try {\n        time = java.lang.Long.parseLong(parts[parts.length - 1]);\n    } catch (java.lang.NumberFormatException e) {\n        org.apache.hadoop.hbase.master.TimeToLiveLogCleaner.LOG.error((\"Unable to parse the timestamp in \" + filePath.getName()) + \", deleting it since it's invalid and may not be a hlog\", e);\n        return true;\n    }\n    long life = currentTime - time;\n    if (life < 0) {\n        org.apache.hadoop.hbase.master.TimeToLiveLogCleaner.LOG.warn(\"Found a log newer than current time, \" + \"probably a clock skew\");\n        return false;\n    }\n    return life > ttl;\n}",
            "ClassName": "TimeToLiveLogCleaner",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "isLogDeletable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 17,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    junit.framework.Assert.assertNotNull(model);\n    junit.framework.Assert.assertNotNull(model.getRESTVersion());\n    junit.framework.Assert.assertEquals(model.getRESTVersion(), org.apache.hadoop.hbase.rest.RESTServlet.VERSION_STRING);\n    java.lang.String osVersion = model.getOSVersion();\n    junit.framework.Assert.assertNotNull(osVersion);\n    junit.framework.Assert.assertTrue(osVersion.contains(java.lang.System.getProperty(\"os.name\")));\n    junit.framework.Assert.assertTrue(osVersion.contains(java.lang.System.getProperty(\"os.version\")));\n    junit.framework.Assert.assertTrue(osVersion.contains(java.lang.System.getProperty(\"os.arch\")));\n    java.lang.String jvmVersion = model.getJVMVersion();\n    junit.framework.Assert.assertNotNull(jvmVersion);\n    junit.framework.Assert.assertTrue(jvmVersion.contains(java.lang.System.getProperty(\"java.vm.vendor\")));\n    junit.framework.Assert.assertTrue(jvmVersion.contains(java.lang.System.getProperty(\"java.version\")));\n    junit.framework.Assert.assertTrue(jvmVersion.contains(java.lang.System.getProperty(\"java.vm.version\")));\n    junit.framework.Assert.assertNotNull(model.getServerVersion());\n    java.lang.String jerseyVersion = model.getJerseyVersion();\n    junit.framework.Assert.assertNotNull(jerseyVersion);\n    junit.framework.Assert.assertEquals(jerseyVersion, com.sun.jersey.spi.container.servlet.ServletContainer.class.getPackage().getImplementationVersion());\n}",
            "ClassName": "TestVersionResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "validate",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 19,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestDefaultEnvironmentEdge",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testGetCurrentTimeUsesSystemClock",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 1,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 13,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 2,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    java.lang.String tableName = \"tablename\";\n    final byte[] tn = org.apache.hadoop.hbase.util.Bytes.toBytes(tableName);\n    java.lang.String startKey = \"startkey\";\n    final byte[] sk = org.apache.hadoop.hbase.util.Bytes.toBytes(startKey);\n    java.lang.String id = \"id\";\n    byte[] name = org.apache.hadoop.hbase.HRegionInfo.createRegionName(tn, sk, id, false);\n    java.lang.String nameStr = org.apache.hadoop.hbase.util.Bytes.toString(name);\n    org.junit.Assert.assertEquals((((tableName + \",\") + startKey) + \",\") + id, nameStr);\n    java.lang.String md5HashInHex = org.apache.hadoop.hbase.util.MD5Hash.getMD5AsHex(name);\n    org.junit.Assert.assertEquals(org.apache.hadoop.hbase.HRegionInfo.MD5_HEX_LENGTH, md5HashInHex.length());\n    name = org.apache.hadoop.hbase.HRegionInfo.createRegionName(tn, sk, id, true);\n    nameStr = org.apache.hadoop.hbase.util.Bytes.toString(name);\n    org.junit.Assert.assertEquals(((((((tableName + \",\") + startKey) + \",\") + id) + \".\") + md5HashInHex) + \".\", nameStr);\n}",
        "CUT_1": {
            "Body": "{\n    return org.apache.hadoop.hbase.util.Bytes.toString(this.name);\n}",
            "ClassName": "HColumnDescriptor",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getNameAsString",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return org.apache.hadoop.hbase.HRegionInfo.createRegionName(tableName, startKey, java.lang.Long.toString(regionid), newFormat);\n}",
            "ClassName": "HRegionInfo",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createRegionName",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.HRegionInfo a = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(\"a\"), null, null);\n    org.apache.hadoop.hbase.HRegionInfo b = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(\"b\"), null, null);\n    junit.framework.Assert.assertTrue(a.compareTo(b) != 0);\n    org.apache.hadoop.hbase.HTableDescriptor t = new org.apache.hadoop.hbase.HTableDescriptor(\"t\");\n    byte[] midway = org.apache.hadoop.hbase.util.Bytes.toBytes(\"midway\");\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, null, midway);\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, midway, null);\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n    junit.framework.Assert.assertTrue(b.compareTo(a) > 0);\n    junit.framework.Assert.assertEquals(a, a);\n    junit.framework.Assert.assertTrue(a.compareTo(a) == 0);\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"a\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"d\"));\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"e\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"g\"));\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"dddd\"));\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"e\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"g\"));\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"dddd\"));\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"eeee\"));\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n}",
            "ClassName": "TestCompare",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testHRegionInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    this.id = id;\n}",
            "ClassName": "TableRegionModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setId",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return id;\n}",
            "ClassName": "TableRegionModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getId",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHRegionInfo",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testCreateHRegionInfoName",
        "NumberOfAsserts": 3,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 15,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[] t1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testListTables1\");\n    byte[] t2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testListTables2\");\n    byte[] t3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testListTables3\");\n    byte[][] tables = new byte[][]{ t1, t2, t3 };\n    for (int i = 0; i < tables.length; i++) {\n        org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(tables[i], org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    }\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration());\n    org.apache.hadoop.hbase.HTableDescriptor[] ts = admin.listTables();\n    java.util.HashSet<org.apache.hadoop.hbase.HTableDescriptor> result = new java.util.HashSet<org.apache.hadoop.hbase.HTableDescriptor>(ts.length);\n    for (int i = 0; i < ts.length; i++) {\n        result.add(ts[i]);\n    }\n    int size = result.size();\n    org.junit.Assert.assertTrue(size >= tables.length);\n    for (int i = 0; (i < tables.length) && (i < size); i++) {\n        boolean found = false;\n        for (int j = 0; j < ts.length; j++) {\n            if (org.apache.hadoop.hbase.util.Bytes.equals(ts[j].getName(), tables[i])) {\n                found = true;\n                break;\n            }\n        }\n        org.junit.Assert.assertTrue(\"Not found: \" + org.apache.hadoop.hbase.util.Bytes.toString(tables[i]), found);\n    }\n}",
        "CUT_1": {
            "Body": "{\n    boolean found = false;\n    java.util.Iterator<org.apache.hadoop.hbase.rest.model.TableModel> tables = model.getTables().iterator();\n    junit.framework.Assert.assertTrue(tables.hasNext());\n    while (tables.hasNext()) {\n        org.apache.hadoop.hbase.rest.model.TableModel table = tables.next();\n        if (table.getName().equals(org.apache.hadoop.hbase.rest.TestTableResource.TABLE)) {\n            found = true;\n            break;\n        }\n    } \n    junit.framework.Assert.assertTrue(found);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "checkTableList",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 13,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    this.tables = tables;\n}",
            "ClassName": "TableListModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setTables",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.TestZooKeeper.conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.TimestampTestBase.put(loader, org.apache.hadoop.hbase.util.Bytes.toBytes(ts), ts);\n}",
            "ClassName": "TimestampTestBase",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "put",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return tables;\n}",
            "ClassName": "TableListModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTables",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 5,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testListTables",
        "NumberOfAsserts": 2,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 27,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.conf.Configuration c = org.apache.hadoop.hbase.master.TestOldLogsCleaner.TEST_UTIL.getConfiguration();\n    org.apache.hadoop.fs.Path oldLogDir = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.master.TestOldLogsCleaner.TEST_UTIL.getTestDir(), org.apache.hadoop.hbase.HConstants.HREGION_OLDLOGDIR_NAME);\n    java.lang.String fakeMachineName = java.net.URLEncoder.encode(\"regionserver:60020\", \"UTF8\");\n    org.apache.hadoop.fs.FileSystem fs = org.apache.hadoop.fs.FileSystem.get(c);\n    java.util.concurrent.atomic.AtomicBoolean stop = new java.util.concurrent.atomic.AtomicBoolean(false);\n    org.apache.hadoop.hbase.master.OldLogsCleaner cleaner = new org.apache.hadoop.hbase.master.OldLogsCleaner(1000, stop, c, fs, oldLogDir);\n    long now = java.lang.System.currentTimeMillis();\n    fs.delete(oldLogDir, true);\n    fs.mkdirs(oldLogDir);\n    fs.createNewFile(new org.apache.hadoop.fs.Path(oldLogDir, \"a\"));\n    fs.createNewFile(new org.apache.hadoop.fs.Path(oldLogDir, (fakeMachineName + \".\") + \"a\"));\n    fs.createNewFile(new org.apache.hadoop.fs.Path(oldLogDir, (fakeMachineName + \".\") + now));\n    java.lang.System.out.println(\"Now is: \" + now);\n    for (int i = 0; i < 30; i++) {\n        fs.createNewFile(new org.apache.hadoop.fs.Path(oldLogDir, (fakeMachineName + \".\") + ((now - 6000000) - i)));\n    }\n    for (org.apache.hadoop.fs.FileStatus stat : fs.listStatus(oldLogDir)) {\n        java.lang.System.out.println(stat.getPath().toString());\n    }\n    fs.createNewFile(new org.apache.hadoop.fs.Path(oldLogDir, (fakeMachineName + \".\") + (now + 10000)));\n    org.junit.Assert.assertEquals(34, fs.listStatus(oldLogDir).length);\n    cleaner.chore();\n    org.junit.Assert.assertEquals(14, fs.listStatus(oldLogDir).length);\n    cleaner.chore();\n    org.junit.Assert.assertEquals(2, fs.listStatus(oldLogDir).length);\n}",
        "CUT_1": {
            "Body": "{\n    return new org.apache.hadoop.fs.Path(oldLogDir, p.getName());\n}",
            "ClassName": "HLog",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getHLogArchivePath",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    if (this.log == null) {\n        org.apache.hadoop.fs.Path logdir = new org.apache.hadoop.fs.Path(this.fs.getHomeDirectory(), (org.apache.hadoop.hbase.HConstants.HREGION_LOGDIR_NAME + \"_\") + java.lang.System.currentTimeMillis());\n        org.apache.hadoop.fs.Path oldLogDir = new org.apache.hadoop.fs.Path(this.fs.getHomeDirectory(), org.apache.hadoop.hbase.HConstants.HREGION_OLDLOGDIR_NAME);\n        this.log = new org.apache.hadoop.hbase.regionserver.wal.HLog(this.fs, logdir, oldLogDir, this.conf, null);\n    }\n    return this.log;\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getLog",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.fs.Path basedir = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.TestStore.DIR + methodName);\n    org.apache.hadoop.fs.Path logdir = new org.apache.hadoop.fs.Path((org.apache.hadoop.hbase.regionserver.TestStore.DIR + methodName) + \"/logs\");\n    org.apache.hadoop.fs.Path oldLogDir = new org.apache.hadoop.fs.Path(basedir, org.apache.hadoop.hbase.HConstants.HREGION_OLDLOGDIR_NAME);\n    org.apache.hadoop.hbase.HColumnDescriptor hcd = new org.apache.hadoop.hbase.HColumnDescriptor(family);\n    org.apache.hadoop.hbase.HBaseConfiguration conf = new org.apache.hadoop.hbase.HBaseConfiguration();\n    org.apache.hadoop.fs.FileSystem fs = org.apache.hadoop.fs.FileSystem.get(conf);\n    org.apache.hadoop.fs.Path reconstructionLog = null;\n    org.apache.hadoop.util.Progressable reporter = null;\n    fs.delete(logdir, true);\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(table);\n    htd.addFamily(hcd);\n    org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(htd, null, null, false);\n    org.apache.hadoop.hbase.regionserver.wal.HLog hlog = new org.apache.hadoop.hbase.regionserver.wal.HLog(fs, logdir, oldLogDir, conf, null);\n    org.apache.hadoop.hbase.regionserver.HRegion region = new org.apache.hadoop.hbase.regionserver.HRegion(basedir, hlog, fs, conf, info, null);\n    store = new org.apache.hadoop.hbase.regionserver.Store(basedir, region, hcd, fs, reconstructionLog, conf, reporter);\n}",
            "ClassName": "TestStore",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "init",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 17,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return fs;\n}",
            "ClassName": "HRegionServer",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getFileSystem",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.fs.Path path = master.getRootDir();\n    org.apache.hadoop.fs.FileSystem fs = path.getFileSystem(master.getConfiguration());\n    return org.apache.hadoop.hbase.util.FSUtils.getTableFragmentation(fs, path);\n}",
            "ClassName": "FSUtils",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableFragmentation",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestOldLogsCleaner",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 2,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testLogCleaning",
        "NumberOfAsserts": 3,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 27,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 5,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    java.lang.String name = \"testTableNameClash\";\n    admin.createTable(new org.apache.hadoop.hbase.HTableDescriptor(name + \"SOMEUPPERCASE\"));\n    admin.createTable(new org.apache.hadoop.hbase.HTableDescriptor(name));\n    new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration(), name);\n}",
        "CUT_1": {
            "Body": "{\n    this.admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration());\n}",
            "ClassName": "TestAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(getName());\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.client.TestTimestamp.COLUMN_NAME));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    return new org.apache.hadoop.hbase.client.HTable(conf, getName());\n}",
            "ClassName": "TestTimestamp",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    this.name = name;\n}",
            "ClassName": "ColumnSchemaModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setName",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    this.name = name;\n}",
            "ClassName": "TableModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setName",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    this.name = name;\n}",
            "ClassName": "HTableDescriptor",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setName",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testTableNameClash",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 6,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    conf.setBoolean(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HBASE_SKIP_ERRORS, true);\n    org.apache.hadoop.fs.Path c1 = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + \"0\");\n    org.apache.hadoop.fs.Path c2 = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + \"5\");\n    org.apache.hadoop.fs.Path c3 = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + (org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.NUM_WRITERS - 1));\n    generateHLogs(-1);\n    corruptHLog(c1, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.Corruptions.INSERT_GARBAGE_IN_THE_MIDDLE, false, fs);\n    corruptHLog(c2, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.Corruptions.APPEND_GARBAGE, true, fs);\n    corruptHLog(c3, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.Corruptions.INSERT_GARBAGE_ON_FIRST_LINE, true, fs);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    org.apache.hadoop.fs.FileStatus[] archivedLogs = fs.listStatus(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.corruptDir);\n    org.junit.Assert.assertEquals(\"expected a different file\", c1.getName(), archivedLogs[0].getPath().getName());\n    org.junit.Assert.assertEquals(\"expected a different file\", c2.getName(), archivedLogs[1].getPath().getName());\n    org.junit.Assert.assertEquals(\"expected a different file\", c3.getName(), archivedLogs[2].getPath().getName());\n    org.junit.Assert.assertEquals(archivedLogs.length, 3);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.removeAll(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions);\n    for (int i = 0; i < 100; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.add(\"region__\" + i);\n    }\n    generateHLogs(1, 100, -1);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    fs.rename(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir);\n    org.apache.hadoop.fs.Path firstSplitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME) + \".first\");\n    org.apache.hadoop.fs.Path splitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME));\n    fs.rename(splitPath, firstSplitPath);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    org.junit.Assert.assertEquals(0, compareHLogSplitDirs(firstSplitPath, splitPath));\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSplittingLargeNumberOfRegionsConsistency",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    for (int i = 0; i < writers; i++) {\n        writer[i] = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + i), conf);\n        for (int j = 0; j < entries; j++) {\n            int prefix = 0;\n            for (java.lang.String region : org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions) {\n                java.lang.String row_key = ((region + (prefix++)) + i) + j;\n                appendEntry(writer[i], org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, region.getBytes(), row_key.getBytes(), org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.FAMILY, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.QUALIFIER, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.VALUE, seq);\n            }\n        }\n        if (i != leaveOpen) {\n            writer[i].close();\n            flushToConsole(\"Closing writer \" + i);\n        }\n    }\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "generateHLogs",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Writer writer = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + suffix), conf);\n    if (closeFile)\n        writer.close();\n\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "injectEmptyFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.flushlogentries\", 1);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setStrings(\"hbase.rootdir\", org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir.toString());\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setClass(\"hbase.regionserver.hlog.writer.impl\", org.apache.hadoop.hbase.regionserver.wal.InstrumentedSequenceFileLogWriter.class, org.apache.hadoop.hbase.regionserver.wal.HLog.Writer.class);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.startMiniDFSCluster(2);\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    conf = org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration();\n    fs = org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getDFSCluster().getFileSystem();\n    org.apache.hadoop.fs.FileStatus[] entries = fs.listStatus(new org.apache.hadoop.fs.Path(\"/\"));\n    for (org.apache.hadoop.fs.FileStatus dir : entries) {\n        fs.delete(dir.getPath(), true);\n    }\n    seq = 0;\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions = new java.util.ArrayList<java.lang.String>();\n    java.util.Collections.addAll(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions, \"bbb\", \"ccc\");\n    org.apache.hadoop.hbase.regionserver.wal.InstrumentedSequenceFileLogWriter.activateFailure = false;\n    org.apache.hadoop.hdfs.MiniDFSCluster dfsCluster = org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getDFSCluster();\n    java.lang.reflect.Field field = dfsCluster.getClass().getDeclaredField(\"nameNode\");\n    field.setAccessible(true);\n    org.apache.hadoop.hdfs.server.namenode.NameNode nn = ((org.apache.hadoop.hdfs.server.namenode.NameNode) (field.get(dfsCluster)));\n    nn.namesystem.leaseManager.setLeasePeriod(100, 50000);\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 17,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHLogSplit",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testCorruptedFileGetsArchivedIfSkipErrors",
        "NumberOfAsserts": 4,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 17,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testDeletes\");\n    byte[][] ROWS = makeNAscii(org.apache.hadoop.hbase.client.TestFromClientSide.ROW, 6);\n    byte[][] FAMILIES = makeNAscii(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, 3);\n    byte[][] VALUES = makeN(org.apache.hadoop.hbase.client.TestFromClientSide.VALUE, 5);\n    long[] ts = new long[]{ 1000, 2000, 3000, 4000, 5000 };\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, FAMILIES);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[0], VALUES[0]);\n    put.add(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[1], VALUES[1]);\n    ht.put(put);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    delete.deleteFamily(FAMILIES[0], ts[0]);\n    ht.delete(delete);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addFamily(FAMILIES[0]);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ ts[1] }, new byte[][]{ VALUES[1] }, 0, 0);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addFamily(FAMILIES[0]);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ ts[1] }, new byte[][]{ VALUES[1] }, 0, 0);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[4], VALUES[4]);\n    put.add(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[2], VALUES[2]);\n    put.add(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[3], VALUES[3]);\n    put.add(FAMILIES[0], null, ts[4], VALUES[4]);\n    put.add(FAMILIES[0], null, ts[2], VALUES[2]);\n    put.add(FAMILIES[0], null, ts[3], VALUES[3]);\n    ht.put(put);\n    delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    delete.deleteColumn(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    ht.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ ts[1], ts[2], ts[3] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3] }, 0, 2);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ ts[1], ts[2], ts[3] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3] }, 0, 2);\n    delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    delete.deleteColumn(FAMILIES[0], null);\n    ht.delete(delete);\n    delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    delete.deleteColumns(FAMILIES[0], null);\n    ht.delete(delete);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[0], VALUES[0]);\n    put.add(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[4], VALUES[4]);\n    ht.put(put);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addFamily(FAMILIES[0]);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ ts[1], ts[2], ts[3] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3] }, 0, 2);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addFamily(FAMILIES[0]);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ ts[1], ts[2], ts[3] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3] }, 0, 2);\n    put = new org.apache.hadoop.hbase.client.Put(ROWS[0]);\n    put.add(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[0], VALUES[0]);\n    put.add(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[1], VALUES[1]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[2], VALUES[2]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[3], VALUES[3]);\n    ht.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(ROWS[1]);\n    put.add(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[0], VALUES[0]);\n    put.add(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[1], VALUES[1]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[2], VALUES[2]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[3], VALUES[3]);\n    ht.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(ROWS[2]);\n    put.add(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[0], VALUES[0]);\n    put.add(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[1], VALUES[1]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[2], VALUES[2]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, ts[3], VALUES[3]);\n    ht.put(put);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[2]);\n    get.addFamily(FAMILIES[1]);\n    get.addFamily(FAMILIES[2]);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    org.junit.Assert.assertTrue(((\"Expected 4 key but received \" + result.size()) + \": \") + result, result.size() == 4);\n    delete = new org.apache.hadoop.hbase.client.Delete(ROWS[0]);\n    delete.deleteFamily(FAMILIES[2]);\n    ht.delete(delete);\n    delete = new org.apache.hadoop.hbase.client.Delete(ROWS[1]);\n    delete.deleteColumns(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    ht.delete(delete);\n    delete = new org.apache.hadoop.hbase.client.Delete(ROWS[2]);\n    delete.deleteColumn(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    delete.deleteColumn(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    delete.deleteColumn(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    ht.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(FAMILIES[1]);\n    get.addFamily(FAMILIES[2]);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    org.junit.Assert.assertTrue(\"Expected 2 keys but received \" + result.size(), result.size() == 2);\n    assertNResult(result, ROWS[0], FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ ts[0], ts[1] }, new byte[][]{ VALUES[0], VALUES[1] }, 0, 1);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[0]);\n    scan.addFamily(FAMILIES[1]);\n    scan.addFamily(FAMILIES[2]);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    org.junit.Assert.assertTrue(\"Expected 2 keys but received \" + result.size(), result.size() == 2);\n    assertNResult(result, ROWS[0], FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ ts[0], ts[1] }, new byte[][]{ VALUES[0], VALUES[1] }, 0, 1);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[1]);\n    get.addFamily(FAMILIES[1]);\n    get.addFamily(FAMILIES[2]);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    org.junit.Assert.assertTrue(\"Expected 2 keys but received \" + result.size(), result.size() == 2);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[1]);\n    scan.addFamily(FAMILIES[1]);\n    scan.addFamily(FAMILIES[2]);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    org.junit.Assert.assertTrue(\"Expected 2 keys but received \" + result.size(), result.size() == 2);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[2]);\n    get.addFamily(FAMILIES[1]);\n    get.addFamily(FAMILIES[2]);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    org.junit.Assert.assertEquals(1, result.size());\n    assertNResult(result, ROWS[2], FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ ts[2] }, new byte[][]{ VALUES[2] }, 0, 0);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[2]);\n    scan.addFamily(FAMILIES[1]);\n    scan.addFamily(FAMILIES[2]);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    org.junit.Assert.assertEquals(1, result.size());\n    assertNResult(result, ROWS[2], FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ ts[2] }, new byte[][]{ VALUES[2] }, 0, 0);\n    delete = new org.apache.hadoop.hbase.client.Delete(ROWS[3]);\n    delete.deleteFamily(FAMILIES[1]);\n    ht.delete(delete);\n    put = new org.apache.hadoop.hbase.client.Put(ROWS[3]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, VALUES[0]);\n    ht.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(ROWS[4]);\n    put.add(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, VALUES[1]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, VALUES[2]);\n    ht.put(put);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[3]);\n    get.addFamily(FAMILIES[1]);\n    get.addFamily(FAMILIES[2]);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    org.junit.Assert.assertTrue(\"Expected 1 key but received \" + result.size(), result.size() == 1);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[4]);\n    get.addFamily(FAMILIES[1]);\n    get.addFamily(FAMILIES[2]);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    org.junit.Assert.assertTrue(\"Expected 2 keys but received \" + result.size(), result.size() == 2);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[3]);\n    scan.addFamily(FAMILIES[1]);\n    scan.addFamily(FAMILIES[2]);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    org.apache.hadoop.hbase.client.ResultScanner scanner = ht.getScanner(scan);\n    result = scanner.next();\n    org.junit.Assert.assertTrue(\"Expected 1 key but received \" + result.size(), result.size() == 1);\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(result.sorted()[0].getRow(), ROWS[3]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(result.sorted()[0].getValue(), VALUES[0]));\n    result = scanner.next();\n    org.junit.Assert.assertTrue(\"Expected 2 keys but received \" + result.size(), result.size() == 2);\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(result.sorted()[0].getRow(), ROWS[4]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(result.sorted()[1].getRow(), ROWS[4]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(result.sorted()[0].getValue(), VALUES[1]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(result.sorted()[1].getValue(), VALUES[2]));\n    scanner.close();\n    for (int i = 0; i < 10; i++) {\n        byte[] bytes = org.apache.hadoop.hbase.util.Bytes.toBytes(i);\n        put = new org.apache.hadoop.hbase.client.Put(bytes);\n        put.add(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, bytes);\n        ht.put(put);\n    }\n    for (int i = 0; i < 10; i++) {\n        byte[] bytes = org.apache.hadoop.hbase.util.Bytes.toBytes(i);\n        get = new org.apache.hadoop.hbase.client.Get(bytes);\n        get.addFamily(FAMILIES[0]);\n        result = ht.get(get);\n        org.junit.Assert.assertTrue(result.size() == 1);\n    }\n    java.util.ArrayList<org.apache.hadoop.hbase.client.Delete> deletes = new java.util.ArrayList<org.apache.hadoop.hbase.client.Delete>();\n    for (int i = 0; i < 10; i++) {\n        byte[] bytes = org.apache.hadoop.hbase.util.Bytes.toBytes(i);\n        delete = new org.apache.hadoop.hbase.client.Delete(bytes);\n        delete.deleteFamily(FAMILIES[0]);\n        deletes.add(delete);\n    }\n    ht.delete(deletes);\n    for (int i = 0; i < 10; i++) {\n        byte[] bytes = org.apache.hadoop.hbase.util.Bytes.toBytes(i);\n        get = new org.apache.hadoop.hbase.client.Get(bytes);\n        get.addFamily(FAMILIES[0]);\n        result = ht.get(get);\n        org.junit.Assert.assertTrue(result.size() == 0);\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] fam = org.apache.hadoop.hbase.util.Bytes.toBytes(\"info\");\n    byte[][] families = new byte[][]{ fam };\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, families);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"table_name\");\n    byte[] serverinfo = org.apache.hadoop.hbase.util.Bytes.toBytes(\"serverinfo\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(fam, serverinfo, org.apache.hadoop.hbase.HConstants.LATEST_TIMESTAMP - 5, org.apache.hadoop.hbase.util.Bytes.toBytes(\"value\"));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(row);\n    region.delete(delete, null, true);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row).addColumn(fam, serverinfo);\n    org.apache.hadoop.hbase.client.Result result = region.get(get, null);\n    junit.framework.Assert.assertEquals(1, result.size());\n    delete = new org.apache.hadoop.hbase.client.Delete(row, org.apache.hadoop.hbase.HConstants.LATEST_TIMESTAMP - 3, null);\n    region.delete(delete, null, true);\n    get = new org.apache.hadoop.hbase.client.Get(row).addColumn(fam, serverinfo);\n    result = region.get(get, null);\n    junit.framework.Assert.assertEquals(0, result.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDeleteRowWithFutureTs",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(family, qualifier);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertNResult(result, row, family, qualifier, stamps, values, start, end);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getAllVersionsAndVerify",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] fam = org.apache.hadoop.hbase.util.Bytes.toBytes(\"info\");\n    byte[][] families = new byte[][]{ fam };\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, families);\n    org.apache.hadoop.hbase.util.EnvironmentEdgeManagerTestHelper.injectEdge(new org.apache.hadoop.hbase.util.IncrementingEnvironmentEdge());\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"table_name\");\n    byte[] serverinfo = org.apache.hadoop.hbase.util.Bytes.toBytes(\"serverinfo\");\n    byte[] splitA = org.apache.hadoop.hbase.util.Bytes.toBytes(\"splitA\");\n    byte[] splitB = org.apache.hadoop.hbase.util.Bytes.toBytes(\"splitB\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(fam, splitA, org.apache.hadoop.hbase.util.Bytes.toBytes(\"reference_A\"));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(fam, splitB, org.apache.hadoop.hbase.util.Bytes.toBytes(\"reference_B\"));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(fam, serverinfo, org.apache.hadoop.hbase.util.Bytes.toBytes(\"ip_address\"));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(row);\n    delete.deleteColumns(fam, splitA);\n    region.delete(delete, null, true);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row).addColumn(fam, serverinfo);\n    org.apache.hadoop.hbase.client.Result result = region.get(get, null);\n    junit.framework.Assert.assertEquals(1, result.size());\n    get = new org.apache.hadoop.hbase.client.Get(row).addColumn(fam, splitA);\n    result = region.get(get, null);\n    junit.framework.Assert.assertEquals(0, result.size());\n    get = new org.apache.hadoop.hbase.client.Get(row).addColumn(fam, splitB);\n    result = region.get(get, null);\n    junit.framework.Assert.assertEquals(1, result.size());\n    put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(fam, splitA, org.apache.hadoop.hbase.util.Bytes.toBytes(\"reference_A\"));\n    region.put(put);\n    get = new org.apache.hadoop.hbase.client.Get(row);\n    result = region.get(get, null);\n    junit.framework.Assert.assertEquals(3, result.size());\n    delete = new org.apache.hadoop.hbase.client.Delete(row);\n    region.delete(delete, null, false);\n    junit.framework.Assert.assertEquals(0, region.get(get, null).size());\n    region.put(new org.apache.hadoop.hbase.client.Put(row).add(fam, splitA, org.apache.hadoop.hbase.util.Bytes.toBytes(\"reference_A\")));\n    result = region.get(get, null);\n    junit.framework.Assert.assertEquals(1, result.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDelete_mixed",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 45,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[0]);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[4], QUALIFIERS[0], VALUES[0]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[2], QUALIFIERS[2]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[2], QUALIFIERS[2], VALUES[2]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(FAMILIES[7]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[7], QUALIFIERS[7], VALUES[7]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(FAMILIES[4]);\n    result = ht.get(get);\n    assertDoubleResult(result, ROWS[0], FAMILIES[4], QUALIFIERS[0], VALUES[0], FAMILIES[4], QUALIFIERS[4], VALUES[4]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[4]);\n    result = ht.get(get);\n    assertDoubleResult(result, ROWS[0], FAMILIES[4], QUALIFIERS[0], VALUES[0], FAMILIES[4], QUALIFIERS[4], VALUES[4]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(FAMILIES[4]);\n    get.addFamily(FAMILIES[7]);\n    result = ht.get(get);\n    assertNResult(result, ROWS[0], FAMILIES, QUALIFIERS, VALUES, new int[][]{ new int[]{ 4, 0, 0 }, new int[]{ 4, 4, 4 }, new int[]{ 7, 7, 7 } });\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(FAMILIES[2]);\n    get.addFamily(FAMILIES[4]);\n    get.addFamily(FAMILIES[6]);\n    get.addFamily(FAMILIES[7]);\n    result = ht.get(get);\n    assertNResult(result, ROWS[0], FAMILIES, QUALIFIERS, VALUES, new int[][]{ new int[]{ 2, 2, 2 }, new int[]{ 2, 4, 4 }, new int[]{ 4, 0, 0 }, new int[]{ 4, 4, 4 }, new int[]{ 6, 6, 6 }, new int[]{ 6, 7, 7 }, new int[]{ 7, 7, 7 } });\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[2], QUALIFIERS[2]);\n    get.addColumn(FAMILIES[2], QUALIFIERS[4]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[4]);\n    get.addColumn(FAMILIES[6], QUALIFIERS[6]);\n    get.addColumn(FAMILIES[6], QUALIFIERS[7]);\n    get.addColumn(FAMILIES[7], QUALIFIERS[7]);\n    get.addColumn(FAMILIES[7], QUALIFIERS[8]);\n    result = ht.get(get);\n    assertNResult(result, ROWS[0], FAMILIES, QUALIFIERS, VALUES, new int[][]{ new int[]{ 2, 2, 2 }, new int[]{ 2, 4, 4 }, new int[]{ 4, 0, 0 }, new int[]{ 4, 4, 4 }, new int[]{ 6, 6, 6 }, new int[]{ 6, 7, 7 }, new int[]{ 7, 7, 7 } });\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    result = ht.get(get);\n    assertNResult(result, ROWS[0], FAMILIES, QUALIFIERS, VALUES, new int[][]{ new int[]{ 2, 2, 2 }, new int[]{ 2, 4, 4 }, new int[]{ 4, 0, 0 }, new int[]{ 4, 4, 4 }, new int[]{ 6, 6, 6 }, new int[]{ 6, 7, 7 }, new int[]{ 7, 7, 7 }, new int[]{ 9, 0, 0 } });\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[1]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[3]);\n    get.addColumn(FAMILIES[2], QUALIFIERS[3]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "singleRowGetTest",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 57,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 4,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testDeletes",
        "NumberOfAsserts": 19,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 207,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    final int times = 100;\n    org.apache.hadoop.hbase.HColumnDescriptor fam1 = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam1\");\n    org.apache.hadoop.hbase.HColumnDescriptor fam2 = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam2\");\n    org.apache.hadoop.hbase.HColumnDescriptor fam3 = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam3\");\n    for (int i = 0; i < times; i++) {\n        org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(\"table\" + i);\n        htd.addFamily(fam1);\n        htd.addFamily(fam2);\n        htd.addFamily(fam3);\n        this.admin.createTable(htd);\n    }\n    for (int i = 0; i < times; i++) {\n        java.lang.String tableName = \"table\" + i;\n        this.admin.disableTable(tableName);\n        this.admin.enableTable(tableName);\n        this.admin.disableTable(tableName);\n        this.admin.deleteTable(tableName);\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(name);\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HBaseTestCase.fam1, versions, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_COMPRESSION, false, false, java.lang.Integer.MAX_VALUE, org.apache.hadoop.hbase.HConstants.FOREVER, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_BLOOMFILTER, org.apache.hadoop.hbase.HConstants.REPLICATION_SCOPE_LOCAL));\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HBaseTestCase.fam2, versions, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_COMPRESSION, false, false, java.lang.Integer.MAX_VALUE, org.apache.hadoop.hbase.HConstants.FOREVER, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_BLOOMFILTER, org.apache.hadoop.hbase.HConstants.REPLICATION_SCOPE_LOCAL));\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HBaseTestCase.fam3, versions, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_COMPRESSION, false, false, java.lang.Integer.MAX_VALUE, org.apache.hadoop.hbase.HConstants.FOREVER, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_BLOOMFILTER, org.apache.hadoop.hbase.HConstants.REPLICATION_SCOPE_LOCAL));\n    return htd;\n}",
            "ClassName": "HBaseTestCase",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableDescriptor",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    int i = 0;\n    for (byte[] family : families) {\n        org.apache.hadoop.hbase.HColumnDescriptor hcd = new org.apache.hadoop.hbase.HColumnDescriptor(family, numVersions[i], org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_COMPRESSION, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_IN_MEMORY, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_BLOCKCACHE, java.lang.Integer.MAX_VALUE, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_TTL, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_BLOOMFILTER, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_REPLICATION_SCOPE);\n        desc.addFamily(hcd);\n        i++;\n    }\n    new org.apache.hadoop.hbase.client.HBaseAdmin(getConfiguration()).createTable(desc);\n    return new org.apache.hadoop.hbase.client.HTable(getConfiguration(), tableName);\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 11,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    super.setUp();\n    context = javax.xml.bind.JAXBContext.newInstance(org.apache.hadoop.hbase.rest.model.CellModel.class, org.apache.hadoop.hbase.rest.model.CellSetModel.class, org.apache.hadoop.hbase.rest.model.RowModel.class);\n    marshaller = context.createMarshaller();\n    unmarshaller = context.createUnmarshaller();\n    client = new org.apache.hadoop.hbase.rest.client.Client(new org.apache.hadoop.hbase.rest.client.Cluster().add(\"localhost\", testServletPort));\n    admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    if (admin.tableExists(org.apache.hadoop.hbase.rest.TestRowResource.TABLE)) {\n        return;\n    }\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.rest.TestRowResource.TABLE);\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.rest.TestRowResource.CFA));\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.rest.TestRowResource.CFB));\n    admin.createTable(htd);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 15,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    for (byte[] family : families) {\n        htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(family));\n    }\n    org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(htd, null, null, false);\n    org.apache.hadoop.fs.Path path = new org.apache.hadoop.fs.Path(DIR + callingMethod);\n    region = org.apache.hadoop.hbase.regionserver.HRegion.createHRegion(info, path, conf);\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "initHRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    super.setUp();\n    testUtil = new org.apache.hadoop.hbase.HBaseTestingUtility();\n    testVals = makeTestVals();\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(getName());\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.filter.TestDependentColumnFilter.FAMILIES[0]));\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.filter.TestDependentColumnFilter.FAMILIES[1]));\n    org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(htd, null, null, false);\n    this.region = org.apache.hadoop.hbase.regionserver.HRegion.createHRegion(info, testUtil.getTestDir(), testUtil.getConfiguration());\n    addData();\n}",
            "ClassName": "TestDependentColumnFilter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 11,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 2,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testHundredsOfTable",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 20,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testJiraTest1014\");\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, 10);\n    long manualStamp = 12345;\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, manualStamp, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n    ht.put(put);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, manualStamp, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, manualStamp - 1);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, manualStamp + 1);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration());\n    admin.split(t.getTableName());\n    java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> regions = waitOnSplit(t);\n    org.junit.Assert.assertTrue(regions.size() > 1);\n    return regions;\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "splitTable",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.TestZooKeeper.conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testJiraTest1014",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 11,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.HColumnDescriptor fam1 = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam1\");\n    org.apache.hadoop.hbase.HColumnDescriptor fam2 = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam2\");\n    org.apache.hadoop.hbase.HColumnDescriptor fam3 = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam3\");\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(\"myTestTable\");\n    htd.addFamily(fam1);\n    htd.addFamily(fam2);\n    htd.addFamily(fam3);\n    this.admin.createTable(htd);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(\"myTestTable\");\n    org.apache.hadoop.hbase.HTableDescriptor confirmedHtd = table.getTableDescriptor();\n    org.junit.Assert.assertEquals(htd.compareTo(confirmedHtd), 0);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(name);\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HBaseTestCase.fam1, versions, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_COMPRESSION, false, false, java.lang.Integer.MAX_VALUE, org.apache.hadoop.hbase.HConstants.FOREVER, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_BLOOMFILTER, org.apache.hadoop.hbase.HConstants.REPLICATION_SCOPE_LOCAL));\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HBaseTestCase.fam2, versions, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_COMPRESSION, false, false, java.lang.Integer.MAX_VALUE, org.apache.hadoop.hbase.HConstants.FOREVER, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_BLOOMFILTER, org.apache.hadoop.hbase.HConstants.REPLICATION_SCOPE_LOCAL));\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HBaseTestCase.fam3, versions, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_COMPRESSION, false, false, java.lang.Integer.MAX_VALUE, org.apache.hadoop.hbase.HConstants.FOREVER, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_BLOOMFILTER, org.apache.hadoop.hbase.HConstants.REPLICATION_SCOPE_LOCAL));\n    return htd;\n}",
            "ClassName": "HBaseTestCase",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableDescriptor",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    super.setUp();\n    testUtil = new org.apache.hadoop.hbase.HBaseTestingUtility();\n    testVals = makeTestVals();\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(getName());\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.filter.TestDependentColumnFilter.FAMILIES[0]));\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.filter.TestDependentColumnFilter.FAMILIES[1]));\n    org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(htd, null, null, false);\n    this.region = org.apache.hadoop.hbase.regionserver.HRegion.createHRegion(info, testUtil.getTestDir(), testUtil.getConfiguration());\n    addData();\n}",
            "ClassName": "TestDependentColumnFilter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 11,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    for (byte[] family : families) {\n        htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(family));\n    }\n    org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(htd, null, null, false);\n    org.apache.hadoop.fs.Path path = new org.apache.hadoop.fs.Path(DIR + callingMethod);\n    region = org.apache.hadoop.hbase.regionserver.HRegion.createHRegion(info, path, conf);\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "initHRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    int i = 0;\n    for (byte[] family : families) {\n        org.apache.hadoop.hbase.HColumnDescriptor hcd = new org.apache.hadoop.hbase.HColumnDescriptor(family, numVersions[i], org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_COMPRESSION, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_IN_MEMORY, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_BLOCKCACHE, java.lang.Integer.MAX_VALUE, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_TTL, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_BLOOMFILTER, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_REPLICATION_SCOPE);\n        desc.addFamily(hcd);\n        i++;\n    }\n    new org.apache.hadoop.hbase.client.HBaseAdmin(getConfiguration()).createTable(desc);\n    return new org.apache.hadoop.hbase.client.HTable(getConfiguration(), tableName);\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 11,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    super.setUp();\n    context = javax.xml.bind.JAXBContext.newInstance(org.apache.hadoop.hbase.rest.model.CellModel.class, org.apache.hadoop.hbase.rest.model.CellSetModel.class, org.apache.hadoop.hbase.rest.model.RowModel.class);\n    marshaller = context.createMarshaller();\n    unmarshaller = context.createUnmarshaller();\n    client = new org.apache.hadoop.hbase.rest.client.Client(new org.apache.hadoop.hbase.rest.client.Cluster().add(\"localhost\", testServletPort));\n    admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    if (admin.tableExists(org.apache.hadoop.hbase.rest.TestRowResource.TABLE)) {\n        return;\n    }\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.rest.TestRowResource.TABLE);\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.rest.TestRowResource.CFA));\n    htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.rest.TestRowResource.CFB));\n    admin.createTable(htd);\n}",
            "ClassName": "TestRowResource",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 15,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testGetTableDescriptor",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 13,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.fs.Path dir = org.apache.hadoop.hbase.HBaseTestingUtility.getTestDir(\"testSplitHFile\");\n    org.apache.hadoop.fs.FileSystem fs = util.getTestFileSystem();\n    org.apache.hadoop.fs.Path testIn = new org.apache.hadoop.fs.Path(dir, \"testhfile\");\n    org.apache.hadoop.hbase.HColumnDescriptor familyDesc = new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles.FAMILY);\n    org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles.createHFile(fs, testIn, org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles.FAMILY, org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles.QUALIFIER, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"zzz\"), 1000);\n    org.apache.hadoop.fs.Path bottomOut = new org.apache.hadoop.fs.Path(dir, \"bottom.out\");\n    org.apache.hadoop.fs.Path topOut = new org.apache.hadoop.fs.Path(dir, \"top.out\");\n    org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.splitStoreFile(util.getConfiguration(), testIn, familyDesc, org.apache.hadoop.hbase.util.Bytes.toBytes(\"ggg\"), bottomOut, topOut);\n    int rowCount = verifyHFile(bottomOut);\n    rowCount += verifyHFile(topOut);\n    org.junit.Assert.assertEquals(1000, rowCount);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.fs.Path dir = org.apache.hadoop.hbase.HBaseTestingUtility.getTestDir(testName);\n    org.apache.hadoop.fs.FileSystem fs = util.getTestFileSystem();\n    dir = dir.makeQualified(fs);\n    org.apache.hadoop.fs.Path familyDir = new org.apache.hadoop.fs.Path(dir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles.FAMILY));\n    int hfileIdx = 0;\n    for (byte[][] range : hfileRanges) {\n        byte[] from = range[0];\n        byte[] to = range[1];\n        org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles.createHFile(fs, new org.apache.hadoop.fs.Path(familyDir, \"hfile_\" + (hfileIdx++)), org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles.FAMILY, org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles.QUALIFIER, from, to, 1000);\n    }\n    int expectedRows = hfileIdx * 1000;\n    util.startMiniCluster();\n    try {\n        org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(util.getConfiguration());\n        org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles.TABLE);\n        htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles.FAMILY));\n        admin.createTable(htd, org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles.SPLIT_KEYS);\n        org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles.TABLE);\n        util.waitTableAvailable(org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles.TABLE, 30000);\n        org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles loader = new org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles(util.getConfiguration());\n        loader.doBulkLoad(dir, table);\n        org.junit.Assert.assertEquals(expectedRows, util.countRows(table));\n    } finally {\n        util.shutdownMiniCluster();\n    }\n}",
            "ClassName": "TestLoadIncrementalHFiles",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "runTest",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 27,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return new org.apache.hadoop.fs.Path(tabledir, new org.apache.hadoop.fs.Path(encodedName, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.util.Bytes.toString(family))));\n}",
            "ClassName": "Store",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getStoreHomedir",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.fs.Path basedir = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.TestStore.DIR + methodName);\n    org.apache.hadoop.fs.Path logdir = new org.apache.hadoop.fs.Path((org.apache.hadoop.hbase.regionserver.TestStore.DIR + methodName) + \"/logs\");\n    org.apache.hadoop.fs.Path oldLogDir = new org.apache.hadoop.fs.Path(basedir, org.apache.hadoop.hbase.HConstants.HREGION_OLDLOGDIR_NAME);\n    org.apache.hadoop.hbase.HColumnDescriptor hcd = new org.apache.hadoop.hbase.HColumnDescriptor(family);\n    org.apache.hadoop.hbase.HBaseConfiguration conf = new org.apache.hadoop.hbase.HBaseConfiguration();\n    org.apache.hadoop.fs.FileSystem fs = org.apache.hadoop.fs.FileSystem.get(conf);\n    org.apache.hadoop.fs.Path reconstructionLog = null;\n    org.apache.hadoop.util.Progressable reporter = null;\n    fs.delete(logdir, true);\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(table);\n    htd.addFamily(hcd);\n    org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(htd, null, null, false);\n    org.apache.hadoop.hbase.regionserver.wal.HLog hlog = new org.apache.hadoop.hbase.regionserver.wal.HLog(fs, logdir, oldLogDir, conf, null);\n    org.apache.hadoop.hbase.regionserver.HRegion region = new org.apache.hadoop.hbase.regionserver.HRegion(basedir, hlog, fs, conf, info, null);\n    store = new org.apache.hadoop.hbase.regionserver.Store(basedir, region, hcd, fs, reconstructionLog, conf, reporter);\n}",
            "ClassName": "TestStore",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "init",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 17,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.fs.Path path = master.getRootDir();\n    org.apache.hadoop.fs.FileSystem fs = path.getFileSystem(master.getConfiguration());\n    return org.apache.hadoop.hbase.util.FSUtils.getTableFragmentation(fs, path);\n}",
            "ClassName": "FSUtils",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableFragmentation",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.fs.Path tableDir = org.apache.hadoop.hbase.HTableDescriptor.getTableDir(rootDir, info.getTableDesc().getName());\n    org.apache.hadoop.fs.Path regionDir = org.apache.hadoop.hbase.regionserver.HRegion.getRegionDir(tableDir, info.getEncodedName());\n    org.apache.hadoop.fs.FileSystem fs = org.apache.hadoop.fs.FileSystem.get(conf);\n    fs.mkdirs(regionDir);\n    org.apache.hadoop.hbase.regionserver.HRegion region = org.apache.hadoop.hbase.regionserver.HRegion.newHRegion(tableDir, new org.apache.hadoop.hbase.regionserver.wal.HLog(fs, new org.apache.hadoop.fs.Path(regionDir, org.apache.hadoop.hbase.HConstants.HREGION_LOGDIR_NAME), new org.apache.hadoop.fs.Path(regionDir, org.apache.hadoop.hbase.HConstants.HREGION_OLDLOGDIR_NAME), conf, null), fs, conf, info, null);\n    region.initialize(null, null);\n    return region;\n}",
            "ClassName": "HRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createHRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestLoadIncrementalHFiles",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testSplitStoreFile",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 13,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = new org.apache.hadoop.hbase.client.HTablePool(((org.apache.hadoop.hbase.HBaseConfiguration) (null)), java.lang.Integer.MAX_VALUE);\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testTable\");\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    junit.framework.Assert.assertNotNull(table);\n    pool.putTable(table);\n    org.apache.hadoop.hbase.client.HTableInterface sameTable = pool.getTable(tableName);\n    junit.framework.Assert.assertSame(table, sameTable);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    try {\n        return ((org.apache.hadoop.hbase.client.HTable) (table)).getRegionsInfo();\n    } finally {\n        pool.putTable(table);\n    }\n}",
            "ClassName": "RegionsResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    try {\n        return table.getTableDescriptor();\n    } finally {\n        pool.putTable(table);\n    }\n}",
            "ClassName": "SchemaResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableSchema",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return pool;\n}",
            "ClassName": "RESTServlet",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTablePool",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    servlet.getMetrics().incrementRequests(1);\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = null;\n    try {\n        java.util.List<org.apache.hadoop.hbase.rest.model.RowModel> rows = model.getRows();\n        table = pool.getTable(tableName);\n        ((org.apache.hadoop.hbase.client.HTable) (table)).setAutoFlush(false);\n        for (org.apache.hadoop.hbase.rest.model.RowModel row : rows) {\n            byte[] key = row.getKey();\n            org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(key);\n            for (org.apache.hadoop.hbase.rest.model.CellModel cell : row.getCells()) {\n                byte[][] parts = org.apache.hadoop.hbase.KeyValue.parseColumn(cell.getColumn());\n                if ((parts.length == 2) && (parts[1].length > 0)) {\n                    put.add(parts[0], parts[1], cell.getTimestamp(), cell.getValue());\n                } else {\n                    put.add(parts[0], null, cell.getTimestamp(), cell.getValue());\n                }\n            }\n            table.put(put);\n            if (org.apache.hadoop.hbase.rest.RowResource.LOG.isDebugEnabled()) {\n                org.apache.hadoop.hbase.rest.RowResource.LOG.debug(\"PUT \" + put.toString());\n            }\n        }\n        ((org.apache.hadoop.hbase.client.HTable) (table)).setAutoFlush(true);\n        table.flushCommits();\n        javax.ws.rs.core.Response.ResponseBuilder response = javax.ws.rs.core.Response.ok();\n        return response.build();\n    } catch (java.io.IOException e) {\n        throw new javax.ws.rs.WebApplicationException(e, Response.Status.SERVICE_UNAVAILABLE);\n    } finally {\n        if (table != null) {\n            pool.putTable(table);\n        }\n    }\n}",
            "ClassName": "RowResource",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "update",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 36,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.TestZooKeeper.conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHTablePool",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testTableWithByteArrayName",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 9,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.util.EnvironmentEdge mock = mock(org.apache.hadoop.hbase.util.EnvironmentEdge.class);\n    org.apache.hadoop.hbase.util.EnvironmentEdgeManager.injectEdge(mock);\n    long expectation = 3456;\n    when(mock.currentTimeMillis()).thenReturn(expectation);\n    long result = org.apache.hadoop.hbase.util.EnvironmentEdgeManager.currentTimeMillis();\n    verify(mock).currentTimeMillis();\n    org.junit.Assert.assertEquals(expectation, result);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.StoreFile mock = org.mockito.Mockito.mock(org.apache.hadoop.hbase.regionserver.StoreFile.class);\n    org.mockito.Mockito.doReturn(bulkLoad).when(mock).isBulkLoadResult();\n    org.mockito.Mockito.doReturn(bulkTimestamp).when(mock).getBulkLoadTimestamp();\n    if (bulkLoad) {\n        org.mockito.Mockito.doThrow(new java.lang.IllegalAccessError(\"bulk load\")).when(mock).getMaxSequenceId();\n    } else {\n        org.mockito.Mockito.doReturn(seqId).when(mock).getMaxSequenceId();\n    }\n    org.mockito.Mockito.doReturn(new org.apache.hadoop.fs.Path(path)).when(mock).getPath();\n    java.lang.String name = ((((((\"mock storefile, bulkLoad=\" + bulkLoad) + \" bulkTimestamp=\") + bulkTimestamp) + \" seqId=\") + seqId) + \" path=\") + path;\n    org.mockito.Mockito.doReturn(name).when(mock).toString();\n    return mock;\n}",
            "ClassName": "TestStoreFile",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "mockStoreFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 14,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.util.EnvironmentEdgeManager.injectEdge(new org.apache.hadoop.hbase.util.DefaultEnvironmentEdge());\n}",
            "ClassName": "EnvironmentEdgeManager",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "reset",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return org.apache.hadoop.hbase.util.EnvironmentEdgeManager.getDelegate().currentTimeMillis();\n}",
            "ClassName": "EnvironmentEdgeManager",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "currentTimeMillis",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.util.EnvironmentEdgeManager.injectEdge(edge);\n}",
            "ClassName": "EnvironmentEdgeManagerTestHelper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "injectEdge",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.util.EnvironmentEdgeManager.reset();\n}",
            "ClassName": "EnvironmentEdgeManagerTestHelper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "reset",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestEnvironmentEdgeManager",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testCurrentTimeInMillis",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 9,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[] familyName = org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY;\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testForceSplit\");\n    final org.apache.hadoop.hbase.client.HTable table = org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.createTable(tableName, familyName);\n    byte[] k = new byte[3];\n    int rowCount = 0;\n    for (byte b1 = 'a'; b1 < 'z'; b1++) {\n        for (byte b2 = 'a'; b2 < 'z'; b2++) {\n            for (byte b3 = 'a'; b3 < 'z'; b3++) {\n                k[0] = b1;\n                k[1] = b2;\n                k[2] = b3;\n                org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(k);\n                put.add(familyName, new byte[0], k);\n                table.put(put);\n                rowCount++;\n            }\n        }\n    }\n    java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> m = table.getRegionsInfo();\n    java.lang.System.out.println(((\"Initial regions (\" + m.size()) + \"): \") + m);\n    org.junit.Assert.assertTrue(m.size() == 1);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.ResultScanner scanner = table.getScanner(scan);\n    int rows = 0;\n    for (@java.lang.SuppressWarnings(\"unused\")\n    org.apache.hadoop.hbase.client.Result result : scanner) {\n        rows++;\n    }\n    scanner.close();\n    org.junit.Assert.assertEquals(rowCount, rows);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scanner = table.getScanner(scan);\n    scanner.next();\n    final java.util.concurrent.atomic.AtomicInteger count = new java.util.concurrent.atomic.AtomicInteger(0);\n    java.lang.Thread t = new java.lang.Thread(\"CheckForSplit\") {\n        public void run() {\n            for (int i = 0; i < 20; i++) {\n                try {\n                    java.lang.Thread.sleep(1000);\n                } catch (java.lang.InterruptedException e) {\n                    continue;\n                }\n                java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> regions = null;\n                try {\n                    regions = table.getRegionsInfo();\n                } catch (java.io.IOException e) {\n                    e.printStackTrace();\n                }\n                if (regions == null)\n                    continue;\n\n                count.set(regions.size());\n                if (count.get() >= 2)\n                    break;\n\n                LOG.debug(\"Cycle waiting on split\");\n            }\n        }\n    };\n    t.start();\n    admin.split(org.apache.hadoop.hbase.util.Bytes.toString(tableName));\n    t.join();\n    rows = 1;\n    for (@java.lang.SuppressWarnings(\"unused\")\n    org.apache.hadoop.hbase.client.Result result : scanner) {\n        rows++;\n        if (rows > rowCount) {\n            scanner.close();\n            org.junit.Assert.assertTrue((\"Scanned more than expected (\" + rowCount) + \")\", false);\n        }\n    }\n    scanner.close();\n    org.junit.Assert.assertEquals(rowCount, rows);\n}",
        "CUT_1": {
            "Body": "{\n    byte[] k = new byte[3];\n    int rowCount = 0;\n    for (byte b1 = 'a'; b1 < 'z'; b1++) {\n        for (byte b2 = 'a'; b2 < 'z'; b2++) {\n            for (byte b3 = 'a'; b3 < 'z'; b3++) {\n                k[0] = b1;\n                k[1] = b2;\n                k[2] = b3;\n                org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(k);\n                put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, new byte[0], k);\n                t.put(put);\n                rowCount++;\n            }\n        }\n    }\n    return rowCount;\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "loadTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 18,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    java.lang.Runnable runnable = new java.lang.Runnable() {\n        public void run() {\n            try {\n                org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n                org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n                scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n                org.apache.hadoop.hbase.client.ResultScanner s = t.getScanner(scan);\n                s.close();\n            } catch (java.io.IOException e) {\n                LOG.fatal(\"could not re-open meta table because\", e);\n                junit.framework.Assert.fail();\n            }\n            org.apache.hadoop.hbase.client.ResultScanner scanner = null;\n            try {\n                org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n                scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n                scanner = table.getScanner(scan);\n                LOG.info(\"Obtained scanner \" + scanner);\n                for (org.apache.hadoop.hbase.client.Result r : scanner) {\n                    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(r.getRow(), row));\n                    junit.framework.Assert.assertEquals(1, r.size());\n                    byte[] bytes = r.value();\n                    junit.framework.Assert.assertNotNull(bytes);\n                    junit.framework.Assert.assertTrue(tableName.equals(org.apache.hadoop.hbase.util.Bytes.toString(bytes)));\n                }\n                LOG.info(\"Success!\");\n            } catch (java.lang.Exception e) {\n                e.printStackTrace();\n                junit.framework.Assert.fail();\n            } finally {\n                if (scanner != null) {\n                    LOG.info(\"Closing scanner \" + scanner);\n                    scanner.close();\n                }\n            }\n        }\n    };\n    return new java.lang.Thread(runnable);\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startVerificationThread",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 40,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_3": {
            "Body": "{\n    byte[] k = new byte[3];\n    int rowCount = 0;\n    for (byte b1 = 'a'; b1 <= 'z'; b1++) {\n        for (byte b2 = 'a'; b2 <= 'z'; b2++) {\n            for (byte b3 = 'a'; b3 <= 'z'; b3++) {\n                k[0] = b1;\n                k[1] = b2;\n                k[2] = b3;\n                org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(k);\n                put.add(f, null, k);\n                t.put(put);\n                rowCount++;\n            }\n        }\n    }\n    return rowCount;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "loadTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 18,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    java.util.Random rng = new java.util.Random();\n    int count = 0;\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] k = new byte[3];\n    byte[][] famAndQf = org.apache.hadoop.hbase.KeyValue.parseColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(column));\n    for (byte b1 = 'a'; b1 < 'z'; b1++) {\n        for (byte b2 = 'a'; b2 < 'z'; b2++) {\n            for (byte b3 = 'a'; b3 < 'z'; b3++) {\n                if (rng.nextDouble() < prob) {\n                    k[0] = b1;\n                    k[1] = b2;\n                    k[2] = b3;\n                    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(k);\n                    put.add(famAndQf[0], famAndQf[1], k);\n                    table.put(put);\n                    count++;\n                }\n            }\n        }\n    }\n    table.flushCommits();\n    return count;\n}",
            "ClassName": "TestScannerResource",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "insertData",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 24,
            "NumberOfRandoms": 5,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.util.TestMergeTool.FAMILY);\n    org.apache.hadoop.hbase.regionserver.InternalScanner scanner = merged.getScanner(scan);\n    try {\n        java.util.List<org.apache.hadoop.hbase.KeyValue> testRes = null;\n        while (true) {\n            testRes = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n            boolean hasNext = scanner.next(testRes);\n            if (!hasNext) {\n                break;\n            }\n        } \n    } finally {\n        scanner.close();\n    }\n    for (int i = 0; i < upperbound; i++) {\n        for (int j = 0; j < rows[i].length; j++) {\n            org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(rows[i][j]);\n            get.addFamily(org.apache.hadoop.hbase.util.TestMergeTool.FAMILY);\n            org.apache.hadoop.hbase.client.Result result = merged.get(get, null);\n            junit.framework.Assert.assertEquals(1, result.size());\n            byte[] bytes = result.sorted()[0].getValue();\n            junit.framework.Assert.assertNotNull(org.apache.hadoop.hbase.util.Bytes.toStringBinary(rows[i][j]), bytes);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(bytes, rows[i][j]));\n        }\n    }\n}",
            "ClassName": "TestMergeTool",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "verifyMerge",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 28,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 9,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testForceSplit",
        "NumberOfAsserts": 4,
        "NumberOfAsynchronousWaits": 1,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 75,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 17,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = new org.apache.hadoop.hbase.client.HTablePool(((org.apache.hadoop.hbase.HBaseConfiguration) (null)), java.lang.Integer.MAX_VALUE);\n    java.lang.String tableName1 = \"testTable1\";\n    java.lang.String tableName2 = \"testTable2\";\n    org.apache.hadoop.hbase.client.HTableInterface table1 = pool.getTable(tableName1);\n    org.apache.hadoop.hbase.client.HTableInterface table2 = pool.getTable(tableName2);\n    junit.framework.Assert.assertNotNull(table2);\n    pool.putTable(table1);\n    pool.putTable(table2);\n    org.apache.hadoop.hbase.client.HTableInterface sameTable1 = pool.getTable(tableName1);\n    org.apache.hadoop.hbase.client.HTableInterface sameTable2 = pool.getTable(tableName2);\n    junit.framework.Assert.assertSame(table1, sameTable1);\n    junit.framework.Assert.assertSame(table2, sameTable2);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    try {\n        return ((org.apache.hadoop.hbase.client.HTable) (table)).getRegionsInfo();\n    } finally {\n        pool.putTable(table);\n    }\n}",
            "ClassName": "RegionsResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    try {\n        return table.getTableDescriptor();\n    } finally {\n        pool.putTable(table);\n    }\n}",
            "ClassName": "SchemaResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableSchema",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return pool;\n}",
            "ClassName": "RESTServlet",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTablePool",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    servlet.getMetrics().incrementRequests(1);\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = null;\n    try {\n        java.util.List<org.apache.hadoop.hbase.rest.model.RowModel> rows = model.getRows();\n        table = pool.getTable(tableName);\n        ((org.apache.hadoop.hbase.client.HTable) (table)).setAutoFlush(false);\n        for (org.apache.hadoop.hbase.rest.model.RowModel row : rows) {\n            byte[] key = row.getKey();\n            org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(key);\n            for (org.apache.hadoop.hbase.rest.model.CellModel cell : row.getCells()) {\n                byte[][] parts = org.apache.hadoop.hbase.KeyValue.parseColumn(cell.getColumn());\n                if ((parts.length == 2) && (parts[1].length > 0)) {\n                    put.add(parts[0], parts[1], cell.getTimestamp(), cell.getValue());\n                } else {\n                    put.add(parts[0], null, cell.getTimestamp(), cell.getValue());\n                }\n            }\n            table.put(put);\n            if (org.apache.hadoop.hbase.rest.RowResource.LOG.isDebugEnabled()) {\n                org.apache.hadoop.hbase.rest.RowResource.LOG.debug(\"PUT \" + put.toString());\n            }\n        }\n        ((org.apache.hadoop.hbase.client.HTable) (table)).setAutoFlush(true);\n        table.flushCommits();\n        javax.ws.rs.core.Response.ResponseBuilder response = javax.ws.rs.core.Response.ok();\n        return response.build();\n    } catch (java.io.IOException e) {\n        throw new javax.ws.rs.WebApplicationException(e, Response.Status.SERVICE_UNAVAILABLE);\n    } finally {\n        if (table != null) {\n            pool.putTable(table);\n        }\n    }\n}",
            "ClassName": "RowResource",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "update",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 36,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    java.lang.String schemaPath = (\"/\" + org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2) + \"/schema\";\n    org.apache.hadoop.hbase.rest.model.TableSchemaModel model;\n    org.apache.hadoop.hbase.rest.client.Response response;\n    junit.framework.Assert.assertFalse(admin.tableExists(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2));\n    model = org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.buildTestModel(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2);\n    org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.checkModel(model, org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2);\n    response = client.put(schemaPath, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF, model.createProtobufOutput());\n    junit.framework.Assert.assertEquals(response.getCode(), 201);\n    admin.enableTable(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2);\n    response = client.get(schemaPath, org.apache.hadoop.hbase.rest.Constants.MIMETYPE_PROTOBUF);\n    junit.framework.Assert.assertEquals(response.getCode(), 200);\n    model = new org.apache.hadoop.hbase.rest.model.TableSchemaModel();\n    model.getObjectFromMessage(response.getBody());\n    org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.checkModel(model, org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2);\n    client.delete(schemaPath);\n    junit.framework.Assert.assertFalse(admin.tableExists(org.apache.hadoop.hbase.rest.TestSchemaResource.TABLE2));\n}",
            "ClassName": "TestSchemaResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "doTestTableCreateAndDeletePB",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 18,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHTablePool",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testTablesWithDifferentNames",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 14,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testNull\");\n    try {\n        org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(null, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n        org.junit.Assert.fail(\"Creating a table with null name passed, should have failed\");\n    } catch (java.lang.Exception e) {\n    }\n    try {\n        org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, ((byte[]) (null)));\n        org.junit.Assert.fail(\"Creating a table with a null family passed, should fail\");\n    } catch (java.lang.Exception e) {\n    }\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    try {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(((byte[]) (null)));\n        put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n        ht.put(put);\n        org.junit.Assert.fail(\"Inserting a null row worked, should throw exception\");\n    } catch (java.lang.Exception e) {\n    }\n    {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n        put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, null, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n        ht.put(put);\n        getTestNull(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n        scanTestNull(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n        org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n        delete.deleteColumns(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, null);\n        ht.delete(delete);\n        org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n        org.apache.hadoop.hbase.client.Result result = ht.get(get);\n        assertEmptyResult(result);\n    }\n    byte[] TABLE2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testNull2\");\n    ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE2, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    try {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n        put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n        ht.put(put);\n        getTestNull(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n        scanTestNull(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n        org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n        getTestNull(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n        scanTestNull(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n        org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n        delete.deleteColumns(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY);\n        ht.delete(delete);\n        org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n        org.apache.hadoop.hbase.client.Result result = ht.get(get);\n        assertEmptyResult(result);\n    } catch (java.lang.Exception e) {\n        throw new java.io.IOException(\"Using a row with null qualifier threw exception, should \");\n    }\n    try {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n        put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, null);\n        ht.put(put);\n        org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n        get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n        org.apache.hadoop.hbase.client.Result result = ht.get(get);\n        assertSingleResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, null);\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n        result = getSingleScanResult(ht, scan);\n        assertSingleResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, null);\n        org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n        delete.deleteColumns(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n        ht.delete(delete);\n        get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n        result = ht.get(get);\n        assertEmptyResult(result);\n    } catch (java.lang.Exception e) {\n        throw new java.io.IOException(\"Null values should be allowed, but threw exception\");\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(c, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.Result res = t.get(get);\n    org.apache.hadoop.hbase.KeyValue[] kvs = res.raw();\n    if (kvs.length <= 0) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    byte[] value = kvs[0].getValue();\n    if (value == null) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(value);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    info.setOffline(onlineOffline);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(info));\n    t.put(put);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(row);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER);\n    t.delete(delete);\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "changeOnlineStatus",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] fam = org.apache.hadoop.hbase.util.Bytes.toBytes(\"info\");\n    byte[][] families = new byte[][]{ fam };\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, families);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"table_name\");\n    byte[] serverinfo = org.apache.hadoop.hbase.util.Bytes.toBytes(\"serverinfo\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(fam, serverinfo, org.apache.hadoop.hbase.HConstants.LATEST_TIMESTAMP - 5, org.apache.hadoop.hbase.util.Bytes.toBytes(\"value\"));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(row);\n    region.delete(delete, null, true);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row).addColumn(fam, serverinfo);\n    org.apache.hadoop.hbase.client.Result result = region.get(get, null);\n    junit.framework.Assert.assertEquals(1, result.size());\n    delete = new org.apache.hadoop.hbase.client.Delete(row, org.apache.hadoop.hbase.HConstants.LATEST_TIMESTAMP - 3, null);\n    region.delete(delete, null, true);\n    get = new org.apache.hadoop.hbase.client.Get(row).addColumn(fam, serverinfo);\n    result = region.get(get, null);\n    junit.framework.Assert.assertEquals(0, result.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testDeleteRowWithFutureTs",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testNull",
        "NumberOfAsserts": 3,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 75,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    java.util.Set<java.lang.String> deadServers = new java.util.HashSet<java.lang.String>();\n    final java.lang.String hostname123 = \"one,123,3\";\n    org.junit.Assert.assertFalse(org.apache.hadoop.hbase.master.ServerManager.isDead(deadServers, hostname123, false));\n    org.junit.Assert.assertFalse(org.apache.hadoop.hbase.master.ServerManager.isDead(deadServers, hostname123, true));\n    deadServers.add(hostname123);\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.master.ServerManager.isDead(deadServers, hostname123, false));\n    org.junit.Assert.assertFalse(org.apache.hadoop.hbase.master.ServerManager.isDead(deadServers, \"one:1\", true));\n    org.junit.Assert.assertFalse(org.apache.hadoop.hbase.master.ServerManager.isDead(deadServers, \"one:1234\", true));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.master.ServerManager.isDead(deadServers, \"one:123\", true));\n}",
        "CUT_1": {
            "Body": "{\n    this.deadServers = deadServers;\n}",
            "ClassName": "ClusterStatus",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setDeadServers",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return this.deadServers;\n}",
            "ClassName": "ServerManager",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getDeadServers",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return deadServers.size();\n}",
            "ClassName": "ClusterStatus",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getDeadServers",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.executor.HBaseEventHandler.serverManager = serverManager;\n}",
            "ClassName": "HBaseEventHandler",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "init",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return this.serverManager;\n}",
            "ClassName": "HMaster",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getServerManager",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestServerManager",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testIsDead",
        "NumberOfAsserts": 6,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 11,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    try {\n        org.apache.hadoop.hbase.client.HTable localMeta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.TestZooKeeper.conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n        org.apache.hadoop.conf.Configuration otherConf = org.apache.hadoop.hbase.HBaseConfiguration.create(org.apache.hadoop.hbase.TestZooKeeper.conf);\n        otherConf.set(org.apache.hadoop.hbase.HConstants.ZOOKEEPER_QUORUM, \"127.0.0.1\");\n        org.apache.hadoop.hbase.client.HTable ipMeta = new org.apache.hadoop.hbase.client.HTable(otherConf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n        localMeta.exists(new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.HConstants.LAST_ROW));\n        ipMeta.exists(new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.HConstants.LAST_ROW));\n        org.junit.Assert.assertFalse(org.apache.hadoop.hbase.client.HConnectionManager.getClientZooKeeperWatcher(org.apache.hadoop.hbase.TestZooKeeper.conf).getZooKeeperWrapper() == org.apache.hadoop.hbase.client.HConnectionManager.getClientZooKeeperWatcher(otherConf).getZooKeeperWrapper());\n        org.junit.Assert.assertFalse(org.apache.hadoop.hbase.client.HConnectionManager.getConnection(org.apache.hadoop.hbase.TestZooKeeper.conf).getZooKeeperWrapper().getQuorumServers().equals(org.apache.hadoop.hbase.client.HConnectionManager.getConnection(otherConf).getZooKeeperWrapper().getQuorumServers()));\n    } catch (java.lang.Exception e) {\n        e.printStackTrace();\n        org.junit.Assert.fail();\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        org.apache.hadoop.hbase.HRegionInfo hri = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(b);\n        byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(row);\n        p.add(org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily(), org.apache.hadoop.hbase.master.TestMasterTransitions.getTestQualifier(), row);\n        t.put(p);\n        rows++;\n    }\n    s.close();\n    org.junit.Assert.assertEquals(expected, rows);\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addToEachStartKey",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.TABLENAME);\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        org.apache.hadoop.hbase.HRegionInfo hri = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(b);\n        byte[] row = org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.getStartKey(hri);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(row);\n        p.add(org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.getTestFamily(), org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.getTestQualifier(), row);\n        t.put(p);\n        rows++;\n    }\n    s.close();\n    org.junit.Assert.assertEquals(expected, rows);\n    return rows;\n}",
            "ClassName": "TestZKBasedReopenRegion",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addToEachStartKey",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.TABLENAME);\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        org.apache.hadoop.hbase.HRegionInfo hri = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(b);\n        byte[] row = org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.getStartKey(hri);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(row);\n        p.add(org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.getTestFamily(), org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.getTestQualifier(), row);\n        t.put(p);\n        rows++;\n    }\n    s.close();\n    org.junit.Assert.assertEquals(expected, rows);\n    return rows;\n}",
            "ClassName": "TestZKBasedCloseRegion",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addToEachStartKey",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    this.server = cluster.getRegionServerThreads().get(0).getRegionServer();\n    this.log = server.getLog();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    for (int i = 1; i <= 256; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + java.lang.String.format(\"%1$04d\", i)));\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, value);\n        table.put(put);\n        if ((i % 32) == 0) {\n            try {\n                java.lang.Thread.sleep(2000);\n            } catch (java.lang.InterruptedException e) {\n            }\n        }\n    }\n}",
            "ClassName": "TestLogRolling",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startAndWriteData",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(c, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.Result res = t.get(get);\n    org.apache.hadoop.hbase.KeyValue[] kvs = res.raw();\n    if (kvs.length <= 0) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    byte[] value = kvs[0].getValue();\n    if (value == null) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(value);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    info.setOffline(onlineOffline);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(info));\n    t.put(put);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(row);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER);\n    t.delete(delete);\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "changeOnlineStatus",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestZooKeeper",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testMultipleZK",
        "NumberOfAsserts": 3,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 15,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    final byte[] tableAname = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testMiscHTableStuffA\");\n    final byte[] tableBname = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testMiscHTableStuffB\");\n    final byte[] attrName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"TESTATTR\");\n    final byte[] attrValue = org.apache.hadoop.hbase.util.Bytes.toBytes(\"somevalue\");\n    byte[] value = org.apache.hadoop.hbase.util.Bytes.toBytes(\"value\");\n    org.apache.hadoop.hbase.client.HTable a = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(tableAname, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.apache.hadoop.hbase.client.HTable b = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(tableBname, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, value);\n    a.put(put);\n    org.apache.hadoop.hbase.client.HTable newA = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration(), tableAname);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.apache.hadoop.hbase.client.ResultScanner s = newA.getScanner(scan);\n    try {\n        for (org.apache.hadoop.hbase.client.Result r : s) {\n            put = new org.apache.hadoop.hbase.client.Put(r.getRow());\n            for (org.apache.hadoop.hbase.KeyValue kv : r.sorted()) {\n                put.add(kv);\n            }\n            b.put(put);\n        }\n    } finally {\n        s.close();\n    }\n    org.apache.hadoop.hbase.client.HTable anotherA = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration(), tableAname);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    anotherA.get(get);\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration());\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(a.getTableDescriptor());\n    admin.disableTable(tableAname);\n    desc.setValue(attrName, attrValue);\n    for (org.apache.hadoop.hbase.HColumnDescriptor c : desc.getFamilies())\n        c.setValue(attrName, attrValue);\n\n    admin.modifyTable(tableAname, org.apache.hadoop.hbase.HConstants.Modify.TABLE_SET_HTD, desc);\n    admin.enableTable(tableAname);\n    desc = a.getTableDescriptor();\n    org.junit.Assert.assertTrue(\"wrong table descriptor returned\", org.apache.hadoop.hbase.util.Bytes.compareTo(desc.getName(), tableAname) == 0);\n    value = desc.getValue(attrName);\n    org.junit.Assert.assertFalse(\"missing HTD attribute value\", value == null);\n    org.junit.Assert.assertFalse(\"HTD attribute value is incorrect\", org.apache.hadoop.hbase.util.Bytes.compareTo(value, attrValue) != 0);\n    for (org.apache.hadoop.hbase.HColumnDescriptor c : desc.getFamilies()) {\n        value = c.getValue(attrName);\n        org.junit.Assert.assertFalse(\"missing HCD attribute value\", value == null);\n        org.junit.Assert.assertFalse(\"HCD attribute value is incorrect\", org.apache.hadoop.hbase.util.Bytes.compareTo(value, attrValue) != 0);\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    this.server = cluster.getRegionServerThreads().get(0).getRegionServer();\n    this.log = server.getLog();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    for (int i = 1; i <= 256; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + java.lang.String.format(\"%1$04d\", i)));\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, value);\n        table.put(put);\n        if ((i % 32) == 0) {\n            try {\n                java.lang.Thread.sleep(2000);\n            } catch (java.lang.InterruptedException e) {\n            }\n        }\n    }\n}",
            "ClassName": "TestLogRolling",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startAndWriteData",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.TestZooKeeper.conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(c, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.Result res = t.get(get);\n    org.apache.hadoop.hbase.KeyValue[] kvs = res.raw();\n    if (kvs.length <= 0) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    byte[] value = kvs[0].getValue();\n    if (value == null) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(value);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    info.setOffline(onlineOffline);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(info));\n    t.put(put);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(row);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER);\n    t.delete(delete);\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "changeOnlineStatus",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        org.apache.hadoop.hbase.HRegionInfo hri = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(b);\n        byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(row);\n        p.add(org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily(), org.apache.hadoop.hbase.master.TestMasterTransitions.getTestQualifier(), row);\n        t.put(p);\n        rows++;\n    }\n    s.close();\n    org.junit.Assert.assertEquals(expected, rows);\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addToEachStartKey",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 4,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testMiscHTableStuff",
        "NumberOfAsserts": 5,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 50,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.avro.AvroServer.HBaseImpl impl = new org.apache.hadoop.hbase.avro.AvroServer.HBaseImpl();\n    org.apache.hadoop.hbase.avro.generated.ATableDescriptor tableA = new org.apache.hadoop.hbase.avro.generated.ATableDescriptor();\n    tableA.name = org.apache.hadoop.hbase.avro.TestAvroServer.tableAname;\n    org.apache.hadoop.hbase.avro.generated.AFamilyDescriptor familyA = new org.apache.hadoop.hbase.avro.generated.AFamilyDescriptor();\n    familyA.name = org.apache.hadoop.hbase.avro.TestAvroServer.familyAname;\n    org.apache.avro.Schema familyArraySchema = org.apache.avro.Schema.createArray(org.apache.hadoop.hbase.avro.generated.AFamilyDescriptor.SCHEMA$);\n    org.apache.avro.generic.GenericArray<org.apache.hadoop.hbase.avro.generated.AFamilyDescriptor> families = new org.apache.avro.generic.GenericData.Array<org.apache.hadoop.hbase.avro.generated.AFamilyDescriptor>(1, familyArraySchema);\n    families.add(familyA);\n    tableA.families = families;\n    impl.createTable(tableA);\n    org.junit.Assert.assertEquals(impl.describeTable(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname).families.size(), 1);\n    impl.disableTable(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname);\n    org.junit.Assert.assertFalse(impl.isTableEnabled(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname));\n    familyA.maxVersions = 123456;\n    impl.modifyFamily(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname, org.apache.hadoop.hbase.avro.TestAvroServer.familyAname, familyA);\n    org.junit.Assert.assertEquals(((int) (impl.describeFamily(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname, org.apache.hadoop.hbase.avro.TestAvroServer.familyAname).maxVersions)), 123456);\n    impl.deleteFamily(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname, org.apache.hadoop.hbase.avro.TestAvroServer.familyAname);\n    org.junit.Assert.assertEquals(impl.describeTable(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname).families.size(), 0);\n    impl.disableTable(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname);\n    impl.deleteTable(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.avro.TestAvroServer.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestAvroServer",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.avro.TestAvroServer.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestAvroServer",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return org.apache.hadoop.hbase.avro.generated.AFamilyDescriptor.SCHEMA$;\n}",
            "ClassName": "AFamilyDescriptor",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getSchema",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.avro.Schema s = org.apache.avro.Schema.createArray(org.apache.hadoop.hbase.avro.generated.AResult.SCHEMA$);\n    org.apache.avro.generic.GenericData.Array<org.apache.hadoop.hbase.avro.generated.AResult> aresults = null;\n    if ((results != null) && (results.length > 0)) {\n        aresults = new org.apache.avro.generic.GenericData.Array<org.apache.hadoop.hbase.avro.generated.AResult>(results.length, s);\n        for (org.apache.hadoop.hbase.client.Result result : results) {\n            aresults.add(org.apache.hadoop.hbase.avro.AvroUtil.resultToAResult(result));\n        }\n    } else {\n        aresults = new org.apache.avro.generic.GenericData.Array<org.apache.hadoop.hbase.avro.generated.AResult>(0, s);\n    }\n    return aresults;\n}",
            "ClassName": "AvroUtil",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "resultsToAResults",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 13,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return org.apache.hadoop.hbase.avro.generated.ATableDescriptor.SCHEMA$;\n}",
            "ClassName": "ATableDescriptor",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getSchema",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAvroServer",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testFamilyAdminAndMetadata",
        "NumberOfAsserts": 4,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 22,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    testScan(\"yyx\", null, \"zzz\");\n}",
        "CUT_1": {
            "Body": "{\n    return ((x == null) && (y == null)) || ((x != null) && x.equals(y));\n}",
            "ClassName": "Pair",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "equals",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HalfStoreFileReader",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "midkey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return null;\n}",
            "ClassName": "WildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return null;\n}",
            "ClassName": "ScanWildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "internalPreFlushcacheCommit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableInputFormatScan",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testScanYYXToEmpty",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration(), \"testTableNotFoundExceptionWithoutAnyTables\");\n}",
        "CUT_1": {
            "Body": "{\n    this.admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration());\n}",
            "ClassName": "TestAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.msginterval\", 100);\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration().setInt(\"hbase.client.pause\", 250);\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration().setInt(\"hbase.client.retries.number\", 6);\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.Get g = new org.apache.hadoop.hbase.client.Get(row);\n    org.junit.Assert.assertTrue(t.get(g).size() > 0);\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "assertRegionIsBackOnline",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        org.apache.hadoop.hbase.HRegionInfo hri = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(b);\n        byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(row);\n        p.add(org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily(), org.apache.hadoop.hbase.master.TestMasterTransitions.getTestQualifier(), row);\n        t.put(p);\n        rows++;\n    }\n    s.close();\n    org.junit.Assert.assertEquals(expected, rows);\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addToEachStartKey",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testTableNotFoundExceptionWithoutAnyTables",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testMasterAdmin\");\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.createTable(tableName, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    this.admin.disableTable(tableName);\n    try {\n        new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration(), tableName);\n    } catch (org.apache.hadoop.hbase.client.RegionOfflineException e) {\n    }\n    this.admin.addColumn(tableName, new org.apache.hadoop.hbase.HColumnDescriptor(\"col2\"));\n    this.admin.enableTable(tableName);\n    try {\n        this.admin.deleteColumn(tableName, org.apache.hadoop.hbase.util.Bytes.toBytes(\"col2\"));\n    } catch (org.apache.hadoop.hbase.TableNotDisabledException e) {\n    }\n    this.admin.disableTable(tableName);\n    this.admin.deleteColumn(tableName, org.apache.hadoop.hbase.util.Bytes.toBytes(\"col2\"));\n    this.admin.deleteTable(tableName);\n}",
        "CUT_1": {
            "Body": "{\n    this.admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration());\n}",
            "ClassName": "TestAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    disableTable(org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n}",
            "ClassName": "HBaseAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "disableTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.TestZooKeeper.conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return this.tableName;\n}",
            "ClassName": "HTable",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableName",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testEnableDisableAddColumnDeleteColumn",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 18,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor[] tables = admin.listTables();\n    int numTables = tables.length;\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.createTable(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testCreateTable\"), org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    tables = this.admin.listTables();\n    org.junit.Assert.assertEquals(numTables + 1, tables.length);\n}",
        "CUT_1": {
            "Body": "{\n    this.tables = tables;\n}",
            "ClassName": "TableListModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setTables",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return tables;\n}",
            "ClassName": "TableListModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTables",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    tables.add(table);\n}",
            "ClassName": "TableListModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "add",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return tables.get(index);\n}",
            "ClassName": "TableListModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "get",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    boolean found = false;\n    java.util.Iterator<org.apache.hadoop.hbase.rest.model.TableModel> tables = model.getTables().iterator();\n    junit.framework.Assert.assertTrue(tables.hasNext());\n    while (tables.hasNext()) {\n        org.apache.hadoop.hbase.rest.model.TableModel table = tables.next();\n        if (table.getName().equals(org.apache.hadoop.hbase.rest.TestTableResource.TABLE)) {\n            found = true;\n            break;\n        }\n    } \n    junit.framework.Assert.assertTrue(found);\n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "checkTableList",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 13,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testCreateTable",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 7,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    doIncrementalLoadTest(true);\n}",
        "CUT_1": {
            "Body": "{\n    return true;\n}",
            "ClassName": "DependentColumnFilter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "hasFilterRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return true;\n}",
            "ClassName": "Chore",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "initialChore",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return true;\n}",
            "ClassName": "PlainTextMessageBodyProducer",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "isWriteable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return true;\n}",
            "ClassName": "TableInputFormatBase",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "includeRegionInSplit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return true;\n}",
            "ClassName": "BoundedRangeFileInputStream",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "markSupported",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHFileOutputFormat",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testMRIncrementalLoadWithSplit",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    testScan(\"yzy\", null, \"zzz\");\n}",
        "CUT_1": {
            "Body": "{\n    return ((x == null) && (y == null)) || ((x != null) && x.equals(y));\n}",
            "ClassName": "Pair",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "equals",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HalfStoreFileReader",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "midkey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return null;\n}",
            "ClassName": "WildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return null;\n}",
            "ClassName": "ScanWildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "internalPreFlushcacheCommit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableInputFormatScan",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testScanYZYToEmpty",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    final byte[] tableAname = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testGetClosestRowBefore\");\n    final byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\");\n    byte[] firstRow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"ro\");\n    byte[] beforeFirstRow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"rn\");\n    byte[] beforeSecondRow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"rov\");\n    org.apache.hadoop.hbase.client.HTable table = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(tableAname, new byte[][]{ org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.util.Bytes.toBytes(\"info2\") });\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(firstRow);\n    org.apache.hadoop.hbase.client.Put put2 = new org.apache.hadoop.hbase.client.Put(row);\n    byte[] zero = new byte[]{ 0 };\n    byte[] one = new byte[]{ 1 };\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, zero);\n    put2.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, one);\n    table.put(put);\n    table.put(put2);\n    org.apache.hadoop.hbase.client.Result result = null;\n    result = table.getRowOrBefore(beforeFirstRow, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.junit.Assert.assertTrue(result == null);\n    result = table.getRowOrBefore(firstRow, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.junit.Assert.assertTrue(result.containsColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(result.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null), zero));\n    result = table.getRowOrBefore(beforeSecondRow, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.junit.Assert.assertTrue(result.containsColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(result.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null), zero));\n    result = table.getRowOrBefore(row, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.junit.Assert.assertTrue(result.containsColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(result.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null), one));\n    result = table.getRowOrBefore(org.apache.hadoop.hbase.util.Bytes.add(row, one), org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.junit.Assert.assertTrue(result.containsColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(result.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null), one));\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(c, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.Result res = t.get(get);\n    org.apache.hadoop.hbase.KeyValue[] kvs = res.raw();\n    if (kvs.length <= 0) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    byte[] value = kvs[0].getValue();\n    if (value == null) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(value);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    info.setOffline(onlineOffline);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(info));\n    t.put(put);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(row);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER);\n    t.delete(delete);\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "changeOnlineStatus",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    final int COUNT = 5;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(getName());\n    for (int i = 0; i < COUNT; i++) {\n        byte[] regionName = org.apache.hadoop.hbase.HRegionInfo.createRegionName(tableName, org.apache.hadoop.hbase.util.Bytes.toBytes(i == 0 ? \"\" : java.lang.Integer.toString(i)), java.lang.Long.toString(java.lang.System.currentTimeMillis()), true);\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(regionName);\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER, org.apache.hadoop.hbase.util.Bytes.toBytes(\"localhost:1234\"));\n        t.put(put);\n    }\n    long sleepTime = conf.getLong(\"hbase.master.meta.thread.rescanfrequency\", 10000);\n    int tries = conf.getInt(\"hbase.client.retries.number\", 5);\n    int count = 0;\n    do {\n        tries -= 1;\n        try {\n            java.lang.Thread.sleep(sleepTime);\n        } catch (java.lang.InterruptedException e) {\n        }\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITA_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITB_QUALIFIER);\n        org.apache.hadoop.hbase.client.ResultScanner scanner = t.getScanner(scan);\n        try {\n            count = 0;\n            org.apache.hadoop.hbase.client.Result r;\n            while ((r = scanner.next()) != null) {\n                if (!r.isEmpty()) {\n                    count += 1;\n                }\n            } \n        } finally {\n            scanner.close();\n        }\n    } while ((count != 0) && (tries >= 0) );\n    junit.framework.Assert.assertTrue(tries >= 0);\n    junit.framework.Assert.assertEquals(0, count);\n}",
            "ClassName": "TestEmptyMetaInfo",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testEmptyMetaInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 41,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(this.conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    java.util.List<byte[]> rows = new java.util.ArrayList<byte[]>();\n    org.apache.hadoop.hbase.client.ResultScanner s = t.getScanner(new org.apache.hadoop.hbase.client.Scan());\n    for (org.apache.hadoop.hbase.client.Result result : s) {\n        org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(result.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER));\n        org.apache.hadoop.hbase.HTableDescriptor desc = info.getTableDesc();\n        if (org.apache.hadoop.hbase.util.Bytes.compareTo(desc.getName(), tableName) == 0) {\n            LOG.info(\"getMetaTableRows: row -> \" + org.apache.hadoop.hbase.util.Bytes.toStringBinary(result.getRow()));\n            rows.add(result.getRow());\n        }\n    }\n    s.close();\n    return rows;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getMetaTableRows",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 15,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    if (org.apache.hadoop.hbase.util.MetaUtils.LOG.isDebugEnabled()) {\n        org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(hri.getRegionName());\n        get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        org.apache.hadoop.hbase.client.Result res = r.get(get, null);\n        org.apache.hadoop.hbase.KeyValue[] kvs = res.raw();\n        if (kvs.length <= 0) {\n            return;\n        }\n        byte[] value = kvs[0].getValue();\n        if (value == null) {\n            return;\n        }\n        org.apache.hadoop.hbase.HRegionInfo h = org.apache.hadoop.hbase.util.Writables.getHRegionInfoOrNull(value);\n        org.apache.hadoop.hbase.util.MetaUtils.LOG.debug(((((((((\"Old \" + org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY)) + \":\") + org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER)) + \" for \") + hri.toString()) + \" in \") + r.toString()) + \" is: \") + h.toString());\n    }\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(hri.getRegionName());\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(hri));\n    r.put(put);\n    if (org.apache.hadoop.hbase.util.MetaUtils.LOG.isDebugEnabled()) {\n        org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(hri.getRegionName());\n        get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        org.apache.hadoop.hbase.client.Result res = r.get(get, null);\n        org.apache.hadoop.hbase.KeyValue[] kvs = res.raw();\n        if (kvs.length <= 0) {\n            return;\n        }\n        byte[] value = kvs[0].getValue();\n        if (value == null) {\n            return;\n        }\n        org.apache.hadoop.hbase.HRegionInfo h = org.apache.hadoop.hbase.util.Writables.getHRegionInfoOrNull(value);\n        org.apache.hadoop.hbase.util.MetaUtils.LOG.debug(((((((((\"New \" + org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY)) + \":\") + org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER)) + \" for \") + hri.toString()) + \" in \") + r.toString()) + \" is: \") + h.toString());\n    }\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 6,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "updateMETARegionInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 35,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testGetClosestRowBefore",
        "NumberOfAsserts": 9,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 31,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.HTable table = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testGet_EmptyTable\"), org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.apache.hadoop.hbase.client.Result r = table.get(get);\n    org.junit.Assert.assertTrue(r.isEmpty());\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.regionserver.TestScanner.ROW_KEY);\n    get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.Result result = region.get(get, null);\n    byte[] bytes = result.value();\n    validateRegionInfo(bytes);\n}",
            "ClassName": "TestScanner",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getRegionInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.Get g = new org.apache.hadoop.hbase.client.Get(row);\n    org.junit.Assert.assertTrue(t.get(g).size() > 0);\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "assertRegionIsBackOnline",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testGet_EmptyTable",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 7,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    conf.setBoolean(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HBASE_SKIP_ERRORS, false);\n    generateHLogs(java.lang.Integer.MAX_VALUE);\n    corruptHLog(new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + \"5\"), org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.Corruptions.APPEND_GARBAGE, true, fs);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.removeAll(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions);\n    for (int i = 0; i < 100; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.add(\"region__\" + i);\n    }\n    generateHLogs(1, 100, -1);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    fs.rename(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir);\n    org.apache.hadoop.fs.Path firstSplitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME) + \".first\");\n    org.apache.hadoop.fs.Path splitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME));\n    fs.rename(splitPath, firstSplitPath);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    org.junit.Assert.assertEquals(0, compareHLogSplitDirs(firstSplitPath, splitPath));\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSplittingLargeNumberOfRegionsConsistency",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    for (int i = 0; i < writers; i++) {\n        writer[i] = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + i), conf);\n        for (int j = 0; j < entries; j++) {\n            int prefix = 0;\n            for (java.lang.String region : org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions) {\n                java.lang.String row_key = ((region + (prefix++)) + i) + j;\n                appendEntry(writer[i], org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, region.getBytes(), row_key.getBytes(), org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.FAMILY, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.QUALIFIER, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.VALUE, seq);\n            }\n        }\n        if (i != leaveOpen) {\n            writer[i].close();\n            flushToConsole(\"Closing writer \" + i);\n        }\n    }\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "generateHLogs",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Writer writer = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + suffix), conf);\n    if (closeFile)\n        writer.close();\n\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "injectEmptyFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.flushlogentries\", 1);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setStrings(\"hbase.rootdir\", org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir.toString());\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setClass(\"hbase.regionserver.hlog.writer.impl\", org.apache.hadoop.hbase.regionserver.wal.InstrumentedSequenceFileLogWriter.class, org.apache.hadoop.hbase.regionserver.wal.HLog.Writer.class);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.startMiniDFSCluster(2);\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.shutdownMiniDFSCluster();\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHLogSplit",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testTrailingGarbageCorruptionLogFileSkipErrorsFalseThrows",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 7,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testVersions\");\n    long[] STAMPS = makeStamps(20);\n    byte[][] VALUES = makeNAscii(org.apache.hadoop.hbase.client.TestFromClientSide.VALUE, 20);\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, 10);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    ht.put(put);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6]);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(2);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[4], STAMPS[5] }, new byte[][]{ VALUES[4], VALUES[5] }, 0, 1);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(2);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[4], STAMPS[5] }, new byte[][]{ VALUES[4], VALUES[5] }, 0, 1);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6]);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(2);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[4], STAMPS[5] }, new byte[][]{ VALUES[4], VALUES[5] }, 0, 1);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(2);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[4], STAMPS[5] }, new byte[][]{ VALUES[4], VALUES[5] }, 0, 1);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3], VALUES[3]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6], VALUES[6]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[7], VALUES[7]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[8], VALUES[8]);\n    ht.put(put);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions();\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1], STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[7], STAMPS[8] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3], VALUES[4], VALUES[5], VALUES[6], VALUES[7], VALUES[8] }, 0, 7);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions();\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1], STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[7], STAMPS[8] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3], VALUES[4], VALUES[5], VALUES[6], VALUES[7], VALUES[8] }, 0, 7);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.setMaxVersions();\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1], STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[7], STAMPS[8] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3], VALUES[4], VALUES[5], VALUES[6], VALUES[7], VALUES[8] }, 0, 7);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.setMaxVersions();\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1], STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[7], STAMPS[8] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3], VALUES[4], VALUES[5], VALUES[6], VALUES[7], VALUES[8] }, 0, 7);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[7], VALUES[7]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[7], VALUES[7]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[9]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[9]);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[9], VALUES[9]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[11], VALUES[11]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[13], VALUES[13]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[15], VALUES[15]);\n    ht.put(put);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[7], STAMPS[8], STAMPS[9], STAMPS[11], STAMPS[13], STAMPS[15] }, new byte[][]{ VALUES[3], VALUES[4], VALUES[5], VALUES[6], VALUES[7], VALUES[8], VALUES[9], VALUES[11], VALUES[13], VALUES[15] }, 0, 9);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[7], STAMPS[8], STAMPS[9], STAMPS[11], STAMPS[13], STAMPS[15] }, new byte[][]{ VALUES[3], VALUES[4], VALUES[5], VALUES[6], VALUES[7], VALUES[8], VALUES[9], VALUES[11], VALUES[13], VALUES[15] }, 0, 9);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    delete.deleteColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[11]);\n    delete.deleteColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[7]);\n    ht.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1], STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[8], STAMPS[9], STAMPS[13], STAMPS[15] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3], VALUES[4], VALUES[5], VALUES[6], VALUES[8], VALUES[9], VALUES[13], VALUES[15] }, 0, 9);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1], STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[8], STAMPS[9], STAMPS[13], STAMPS[15] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3], VALUES[4], VALUES[5], VALUES[6], VALUES[8], VALUES[9], VALUES[13], VALUES[15] }, 0, 9);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(family, qualifier);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    get.setTimeRange(stamps[start], stamps[end] + 1);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertNResult(result, row, family, qualifier, stamps, values, start, end);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getVersionRangeAndVerify",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row);\n    scan.addColumn(family, qualifier);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    scan.setTimeRange(stamps[start], stamps[end] + 1);\n    org.apache.hadoop.hbase.client.Result result = getSingleScanResult(ht, scan);\n    assertNResult(result, row, family, qualifier, stamps, values, start, end);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "scanVersionRangeAndVerify",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testVersions",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 128,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testSimpleMissing\");\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    byte[][] ROWS = makeN(org.apache.hadoop.hbase.client.TestFromClientSide.ROW, 4);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[0]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[0], ROWS[1]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(ROWS[2]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n    ht.put(put);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[1]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[3]);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[3]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[0], ROWS[2]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[2]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[2], org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[2]);\n    get.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[2], org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[2]);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[2], org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[2], org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[0], ROWS[3]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[2], org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[2], ROWS[3]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[2], org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(ROWS[ROWIDX + 1]);\n    org.apache.hadoop.hbase.client.Result result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[ROWIDX + 1], ROWS[ROWIDX + 2]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, ROWS[ROWIDX]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX + 1]);\n    scan.addFamily(FAMILIES[FAMILYIDX - 1]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "scanVerifySingleEmpty",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(family, null);\n    org.apache.hadoop.hbase.client.Result result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, row, family, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY, value);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(family, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, row, family, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY, value);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(family);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, row, family, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY, value);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, row, family, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY, value);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "scanTestNull",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 17,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row);\n    scan.addColumn(family, qualifier);\n    scan.setTimeStamp(stamp);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    org.apache.hadoop.hbase.client.Result result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "scanVersionAndVerifyMissing",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(family, null);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertSingleResult(result, row, family, null, value);\n    get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(family, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY);\n    result = ht.get(get);\n    assertSingleResult(result, row, family, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY, value);\n    get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addFamily(family);\n    result = ht.get(get);\n    assertSingleResult(result, row, family, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY, value);\n    get = new org.apache.hadoop.hbase.client.Get(row);\n    result = ht.get(get);\n    assertSingleResult(result, row, family, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY, value);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTestNull",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 17,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testSimpleMissing",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 73,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.avro.AvroServer.HBaseImpl impl = new org.apache.hadoop.hbase.avro.AvroServer.HBaseImpl();\n    org.junit.Assert.assertEquals(impl.listTables().size(), 0);\n    org.apache.hadoop.hbase.avro.generated.ATableDescriptor tableA = new org.apache.hadoop.hbase.avro.generated.ATableDescriptor();\n    tableA.name = org.apache.hadoop.hbase.avro.TestAvroServer.tableAname;\n    impl.createTable(tableA);\n    org.junit.Assert.assertEquals(impl.listTables().size(), 1);\n    org.junit.Assert.assertTrue(impl.isTableEnabled(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname));\n    org.junit.Assert.assertTrue(impl.tableExists(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname));\n    org.apache.hadoop.hbase.avro.generated.ATableDescriptor tableB = new org.apache.hadoop.hbase.avro.generated.ATableDescriptor();\n    tableB.name = org.apache.hadoop.hbase.avro.TestAvroServer.tableBname;\n    impl.createTable(tableB);\n    org.junit.Assert.assertEquals(impl.listTables().size(), 2);\n    impl.disableTable(org.apache.hadoop.hbase.avro.TestAvroServer.tableBname);\n    org.junit.Assert.assertFalse(impl.isTableEnabled(org.apache.hadoop.hbase.avro.TestAvroServer.tableBname));\n    impl.deleteTable(org.apache.hadoop.hbase.avro.TestAvroServer.tableBname);\n    org.junit.Assert.assertEquals(impl.listTables().size(), 1);\n    impl.disableTable(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname);\n    org.junit.Assert.assertFalse(impl.isTableEnabled(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname));\n    tableA.maxFileSize = 123456L;\n    impl.modifyTable(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname, tableA);\n    org.junit.Assert.assertEquals(((long) (impl.describeTable(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname).maxFileSize)), 123456L);\n    impl.enableTable(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname);\n    org.junit.Assert.assertTrue(impl.isTableEnabled(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname));\n    impl.disableTable(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname);\n    impl.deleteTable(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.avro.TestAvroServer.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestAvroServer",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.avro.TestAvroServer.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestAvroServer",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return org.apache.hadoop.hbase.avro.generated.ATableDescriptor.SCHEMA$;\n}",
            "ClassName": "ATableDescriptor",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getSchema",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    if (args.length < 1) {\n        org.apache.hadoop.hbase.avro.AvroServer.printUsageAndExit();\n    }\n    int port = 9090;\n    final java.lang.String portArgKey = \"--port=\";\n    for (java.lang.String cmd : args) {\n        if (cmd.startsWith(portArgKey)) {\n            port = java.lang.Integer.parseInt(cmd.substring(portArgKey.length()));\n            continue;\n        } else\n            if (cmd.equals(\"--help\") || cmd.equals(\"-h\")) {\n                org.apache.hadoop.hbase.avro.AvroServer.printUsageAndExit();\n            } else\n                if (cmd.equals(\"start\")) {\n                    continue;\n                } else\n                    if (cmd.equals(\"stop\")) {\n                        org.apache.hadoop.hbase.avro.AvroServer.printUsageAndExit(\"To shutdown the Avro server run \" + (\"bin/hbase-daemon.sh stop avro or send a kill signal to \" + \"the Avro server pid\"));\n                    }\n\n\n\n        org.apache.hadoop.hbase.avro.AvroServer.printUsageAndExit();\n    }\n    org.apache.commons.logging.Log LOG = org.apache.commons.logging.LogFactory.getLog(\"AvroServer\");\n    LOG.info(\"starting HBase Avro server on port \" + java.lang.Integer.toString(port));\n    org.apache.avro.specific.SpecificResponder r = new org.apache.avro.specific.SpecificResponder(org.apache.hadoop.hbase.avro.generated.HBase.class, new org.apache.hadoop.hbase.avro.AvroServer.HBaseImpl());\n    org.apache.avro.ipc.HttpServer server = new org.apache.avro.ipc.HttpServer(r, 9090);\n    java.lang.Thread.sleep(1000000);\n}",
            "ClassName": "AvroServer",
            "CyclomaticComplexity": 6,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "doMain",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 31,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_5": {
            "Body": "{\n    org.apache.avro.Schema s = org.apache.avro.Schema.createArray(org.apache.hadoop.hbase.avro.generated.AResult.SCHEMA$);\n    org.apache.avro.generic.GenericData.Array<org.apache.hadoop.hbase.avro.generated.AResult> aresults = null;\n    if ((results != null) && (results.length > 0)) {\n        aresults = new org.apache.avro.generic.GenericData.Array<org.apache.hadoop.hbase.avro.generated.AResult>(results.length, s);\n        for (org.apache.hadoop.hbase.client.Result result : results) {\n            aresults.add(org.apache.hadoop.hbase.avro.AvroUtil.resultToAResult(result));\n        }\n    } else {\n        aresults = new org.apache.avro.generic.GenericData.Array<org.apache.hadoop.hbase.avro.generated.AResult>(0, s);\n    }\n    return aresults;\n}",
            "ClassName": "AvroUtil",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "resultsToAResults",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 13,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAvroServer",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testTableAdminAndMetadata",
        "NumberOfAsserts": 10,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 27,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    this.admin.createTable(new org.apache.hadoop.hbase.HTableDescriptor());\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(getName());\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.client.TestTimestamp.COLUMN_NAME));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    return new org.apache.hadoop.hbase.client.HTable(conf, getName());\n}",
            "ClassName": "TestTimestamp",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.HBaseConfiguration.create();\n    org.apache.hadoop.hbase.LocalHBaseCluster cluster = new org.apache.hadoop.hbase.LocalHBaseCluster(conf);\n    cluster.startup();\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.util.Bytes.toBytes(cluster.getClass().getName()));\n    admin.createTable(htd);\n    cluster.shutdown();\n}",
            "ClassName": "LocalHBaseCluster",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "main",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    super.setUp();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.client.TestGetRowVersions.TABLE_NAME);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.client.TestGetRowVersions.CONTENTS));\n    this.admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    this.admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.client.TestGetRowVersions.TABLE_NAME);\n}",
            "ClassName": "TestGetRowVersions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    setValue(org.apache.hadoop.hbase.HTableDescriptor.READONLY_KEY, readOnly ? org.apache.hadoop.hbase.HTableDescriptor.TRUE : org.apache.hadoop.hbase.HTableDescriptor.FALSE);\n}",
            "ClassName": "HTableDescriptor",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setReadOnly",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testEmptyHHTableDescriptor",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.MiniHBaseCluster.MiniHBaseClusterRegionServer firstServer = ((org.apache.hadoop.hbase.MiniHBaseCluster.MiniHBaseClusterRegionServer) (org.apache.hadoop.hbase.master.TestKillingServersFromMaster.cluster.getRegionServer(0)));\n    org.apache.hadoop.hbase.HServerInfo hsi = firstServer.getServerInfo();\n    firstServer.setHServerInfo(new org.apache.hadoop.hbase.HServerInfo(hsi.getServerAddress(), hsi.getInfoPort(), hsi.getHostname()));\n    org.apache.hadoop.hbase.master.TestKillingServersFromMaster.cluster.waitOnRegionServer(0);\n    org.junit.Assert.assertEquals(1, org.apache.hadoop.hbase.master.TestKillingServersFromMaster.cluster.getLiveRegionServerThreads().size());\n}",
        "CUT_1": {
            "Body": "{\n    return new org.apache.hadoop.hbase.HServerInfo(hsi.getServerAddress(), hsi.getInfoPort(), hsi.getHostname());\n}",
            "ClassName": "HRegionServer",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createServerInfoWithNewStartCode",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestKillingServersFromMaster.TEST_UTIL.startMiniCluster(2);\n    org.apache.hadoop.hbase.master.TestKillingServersFromMaster.cluster = org.apache.hadoop.hbase.master.TestKillingServersFromMaster.TEST_UTIL.getHBaseCluster();\n}",
            "ClassName": "TestKillingServersFromMaster",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "beforeAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.HServerInfo hsi = new org.apache.hadoop.hbase.HServerInfo(new org.apache.hadoop.hbase.HServerAddress(\"0.0.0.0:123\"), -1, 1245, \"default name\");\n    byte[] b = org.apache.hadoop.hbase.util.Writables.getBytes(hsi);\n    org.apache.hadoop.hbase.HServerInfo deserializedHsi = ((org.apache.hadoop.hbase.HServerInfo) (org.apache.hadoop.hbase.util.Writables.getWritable(b, new org.apache.hadoop.hbase.HServerInfo())));\n    junit.framework.Assert.assertTrue(hsi.equals(deserializedHsi));\n}",
            "ClassName": "TestSerialization",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testServerInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestKillingServersFromMaster.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestKillingServersFromMaster",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "afterAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestKillingServersFromMaster.TEST_UTIL.ensureSomeRegionServersAvailable(2);\n}",
            "ClassName": "TestKillingServersFromMaster",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setup",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestKillingServersFromMaster",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 1,
        "Label": 0,
        "MethodName": "testRsReportsWrongStartCode",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 7,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    final byte[] CONTENTS_FAMILY = org.apache.hadoop.hbase.util.Bytes.toBytes(\"contents\");\n    final byte[] SMALL_FAMILY = org.apache.hadoop.hbase.util.Bytes.toBytes(\"smallfam\");\n    final byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    final byte[] row2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row2\");\n    final byte[] value = org.apache.hadoop.hbase.util.Bytes.toBytes(\"abcd\");\n    org.apache.hadoop.hbase.client.HTable table = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testPut\"), new byte[][]{ CONTENTS_FAMILY, SMALL_FAMILY });\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(CONTENTS_FAMILY, null, value);\n    table.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row2);\n    put.add(CONTENTS_FAMILY, null, value);\n    org.junit.Assert.assertEquals(put.size(), 1);\n    org.junit.Assert.assertEquals(put.getFamilyMap().get(CONTENTS_FAMILY).size(), 1);\n    org.apache.hadoop.hbase.KeyValue kv = put.getFamilyMap().get(CONTENTS_FAMILY).get(0);\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(kv.getFamily(), CONTENTS_FAMILY));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(kv.getQualifier(), new byte[0]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(kv.getValue(), value));\n    table.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(CONTENTS_FAMILY, null);\n    org.apache.hadoop.hbase.client.ResultScanner scanner = table.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r : scanner) {\n        for (org.apache.hadoop.hbase.KeyValue key : r.sorted()) {\n            java.lang.System.out.println((org.apache.hadoop.hbase.util.Bytes.toString(r.getRow()) + \": \") + key.toString());\n        }\n    }\n}",
        "CUT_1": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"test_table\");\n    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testFamily\");\n    initHRegion(tableName, getName(), family);\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row111\");\n    byte[] row2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row222\");\n    byte[] row3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row333\");\n    byte[] row4 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row444\");\n    byte[] row5 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row555\");\n    byte[] col1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub111\");\n    byte[] col2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub222\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(10L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row2);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(15L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row3);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(20L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row4);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(30L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row5);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(40L));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row3, row4);\n    scan.setMaxVersions();\n    scan.addColumn(family, col1);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    junit.framework.Assert.assertEquals(false, s.next(results));\n    junit.framework.Assert.assertEquals(0, results.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_StopRow1542",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 34,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.TestZooKeeper.conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"test_table\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"columnA\");\n    byte[] fam2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"columnB\");\n    initHRegion(tableName, getName(), fam1, fam2);\n    byte[] rowA = org.apache.hadoop.hbase.util.Bytes.toBytes(\"rowA\");\n    byte[] rowB = org.apache.hadoop.hbase.util.Bytes.toBytes(\"rowB\");\n    byte[] value = org.apache.hadoop.hbase.util.Bytes.toBytes(\"value\");\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(rowA);\n    delete.deleteFamily(fam1);\n    region.delete(delete, null, true);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(rowA);\n    put.add(fam2, null, value);\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(rowB);\n    put.add(fam1, null, value);\n    put.add(fam2, null, value);\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(fam1).addFamily(fam2);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    s.next(results);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(rowA, results.get(0).getRow()));\n    results.clear();\n    s.next(results);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(rowB, results.get(0).getRow()));\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_DeleteOneFamilyNotAnother",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 28,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam1\");\n    byte[] fam2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam2\");\n    byte[] qf1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"qualifier\");\n    byte[] val1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"value1\");\n    byte[] val2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"value2\");\n    java.lang.Integer lockId = null;\n    byte[][] families = new byte[][]{ fam1, fam2 };\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, families);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(fam1, qf1, val1);\n    region.put(put);\n    long ts = java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.KeyValue kv = new org.apache.hadoop.hbase.KeyValue(row1, fam2, qf1, ts, org.apache.hadoop.hbase.KeyValue.Type.Put, val2);\n    put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(kv);\n    org.apache.hadoop.hbase.regionserver.Store store = region.getStore(fam1);\n    store.memstore.kvset.size();\n    boolean res = region.checkAndMutate(row1, fam1, qf1, val1, put, lockId, true);\n    junit.framework.Assert.assertEquals(true, res);\n    store.memstore.kvset.size();\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row1);\n    get.addColumn(fam2, qf1);\n    org.apache.hadoop.hbase.KeyValue[] actual = region.get(get, null).raw();\n    org.apache.hadoop.hbase.KeyValue[] expected = new org.apache.hadoop.hbase.KeyValue[]{ kv };\n    junit.framework.Assert.assertEquals(expected.length, actual.length);\n    for (int i = 0; i < actual.length; i++) {\n        junit.framework.Assert.assertEquals(expected[i], actual[i]);\n    }\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testCheckAndPut_ThatPutWasWritten",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 33,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 2,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testPut",
        "NumberOfAsserts": 5,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 28,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    final byte[] FAM1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam1\");\n    final byte[] FAM2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam2\");\n    org.apache.hadoop.hbase.client.HTable table = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testHBase737\"), new byte[][]{ FAM1, FAM2 });\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(FAM1, org.apache.hadoop.hbase.util.Bytes.toBytes(\"letters\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"abcdefg\"));\n    table.put(put);\n    try {\n        java.lang.Thread.sleep(1000);\n    } catch (java.lang.InterruptedException i) {\n    }\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(FAM1, org.apache.hadoop.hbase.util.Bytes.toBytes(\"numbers\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"123456\"));\n    table.put(put);\n    try {\n        java.lang.Thread.sleep(1000);\n    } catch (java.lang.InterruptedException i) {\n    }\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(FAM2, org.apache.hadoop.hbase.util.Bytes.toBytes(\"letters\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"hijklmnop\"));\n    table.put(put);\n    long[] times = new long[3];\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(FAM1);\n    scan.addFamily(FAM2);\n    org.apache.hadoop.hbase.client.ResultScanner s = table.getScanner(scan);\n    try {\n        int index = 0;\n        org.apache.hadoop.hbase.client.Result r = null;\n        while ((r = s.next()) != null) {\n            for (org.apache.hadoop.hbase.KeyValue key : r.sorted()) {\n                times[index++] = key.getTimestamp();\n            }\n        } \n    } finally {\n        s.close();\n    }\n    for (int i = 0; i < (times.length - 1); i++) {\n        for (int j = i + 1; j < times.length; j++) {\n            org.junit.Assert.assertTrue(times[j] > times[i]);\n        }\n    }\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    for (int i = 0; i < times.length; i++) {\n        times[i] = 0;\n    }\n    try {\n        java.lang.Thread.sleep(1000);\n    } catch (java.lang.InterruptedException i) {\n    }\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(FAM1);\n    scan.addFamily(FAM2);\n    s = table.getScanner(scan);\n    try {\n        int index = 0;\n        org.apache.hadoop.hbase.client.Result r = null;\n        while ((r = s.next()) != null) {\n            for (org.apache.hadoop.hbase.KeyValue key : r.sorted()) {\n                times[index++] = key.getTimestamp();\n            }\n        } \n    } finally {\n        s.close();\n    }\n    for (int i = 0; i < (times.length - 1); i++) {\n        for (int j = i + 1; j < times.length; j++) {\n            org.junit.Assert.assertTrue(times[j] > times[i]);\n        }\n    }\n}",
        "CUT_1": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"test_table\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"columnA\");\n    byte[] fam2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"columnB\");\n    initHRegion(tableName, getName(), fam1, fam2);\n    byte[] rowA = org.apache.hadoop.hbase.util.Bytes.toBytes(\"rowA\");\n    byte[] rowB = org.apache.hadoop.hbase.util.Bytes.toBytes(\"rowB\");\n    byte[] value = org.apache.hadoop.hbase.util.Bytes.toBytes(\"value\");\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(rowA);\n    delete.deleteFamily(fam1);\n    region.delete(delete, null, true);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(rowA);\n    put.add(fam2, null, value);\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(rowB);\n    put.add(fam1, null, value);\n    put.add(fam2, null, value);\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(fam1).addFamily(fam2);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    s.next(results);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(rowA, results.get(0).getRow()));\n    results.clear();\n    s.next(results);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(rowB, results.get(0).getRow()));\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_DeleteOneFamilyNotAnother",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 28,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"test_table\");\n    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testFamily\");\n    initHRegion(tableName, getName(), family);\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row111\");\n    byte[] row2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row222\");\n    byte[] row3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row333\");\n    byte[] row4 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row444\");\n    byte[] row5 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row555\");\n    byte[] col1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub111\");\n    byte[] col2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub222\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(10L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row2);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(15L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row3);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(20L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row4);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(30L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row5);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(40L));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row3, row4);\n    scan.setMaxVersions();\n    scan.addColumn(family, col1);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    junit.framework.Assert.assertEquals(false, s.next(results));\n    junit.framework.Assert.assertEquals(0, results.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_StopRow1542",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 34,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.TestZooKeeper.conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam1\");\n    byte[] fam2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam2\");\n    byte[] fam3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam3\");\n    byte[] fam4 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam4\");\n    byte[][] families = new byte[][]{ fam1, fam2, fam3, fam4 };\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, families);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(fam1, null, null);\n    put.add(fam2, null, null);\n    put.add(fam3, null, null);\n    put.add(fam4, null, null);\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = null;\n    org.apache.hadoop.hbase.regionserver.HRegion.RegionScanner is = null;\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(fam2);\n    scan.addFamily(fam4);\n    is = ((org.apache.hadoop.hbase.regionserver.HRegion.RegionScanner) (region.getScanner(scan)));\n    junit.framework.Assert.assertEquals(1, ((org.apache.hadoop.hbase.regionserver.HRegion.RegionScanner) (is)).storeHeap.getHeap().size());\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    is = ((org.apache.hadoop.hbase.regionserver.HRegion.RegionScanner) (region.getScanner(scan)));\n    junit.framework.Assert.assertEquals(families.length - 1, ((org.apache.hadoop.hbase.regionserver.HRegion.RegionScanner) (is)).storeHeap.getHeap().size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testGetScanner_WithNoFamilies",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 27,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 9,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testHBase737",
        "NumberOfAsserts": 2,
        "NumberOfAsynchronousWaits": 3,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 71,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 6,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.fs.Path hfilePath = new org.apache.hadoop.fs.Path(new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.HBaseTestingUtility.getTestDir(\"internalScannerExposesErrors\"), \"regionname\"), \"familyname\");\n    org.apache.hadoop.hbase.regionserver.TestFSErrorsExposed.FaultyFileSystem fs = new org.apache.hadoop.hbase.regionserver.TestFSErrorsExposed.FaultyFileSystem(util.getTestFileSystem());\n    org.apache.hadoop.hbase.regionserver.StoreFile.Writer writer = org.apache.hadoop.hbase.regionserver.StoreFile.createWriter(fs, hfilePath, 2 * 1024);\n    org.apache.hadoop.hbase.regionserver.TestStoreFile.writeStoreFile(writer, org.apache.hadoop.hbase.util.Bytes.toBytes(\"cf\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"qual\"));\n    org.apache.hadoop.hbase.regionserver.StoreFile sf = new org.apache.hadoop.hbase.regionserver.StoreFile(fs, writer.getPath(), false, util.getConfiguration(), org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE, false);\n    org.apache.hadoop.hbase.io.hfile.HFile.Reader reader = sf.createReader();\n    org.apache.hadoop.hbase.io.hfile.HFileScanner scanner = reader.getScanner(false, true);\n    org.apache.hadoop.hbase.regionserver.TestFSErrorsExposed.FaultyInputStream inStream = fs.inStreams.get(0).get();\n    org.junit.Assert.assertNotNull(inStream);\n    scanner.seekTo();\n    org.junit.Assert.assertTrue(scanner.next());\n    inStream.startFaults();\n    try {\n        int scanned = 0;\n        while (scanner.next()) {\n            scanned++;\n        } \n        org.junit.Assert.fail(\"Scanner didn't throw after faults injected\");\n    } catch (java.io.IOException ioe) {\n        org.apache.hadoop.hbase.regionserver.TestFSErrorsExposed.LOG.info(\"Got expected exception\", ioe);\n        org.junit.Assert.assertTrue(ioe.getMessage().contains(\"Fault\"));\n    }\n    reader.close();\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.fs.Path storedir = new org.apache.hadoop.fs.Path(new org.apache.hadoop.fs.Path(this.testDir, \"regionname\"), \"familyname\");\n    org.apache.hadoop.fs.Path dir = new org.apache.hadoop.fs.Path(storedir, \"1234567890\");\n    org.apache.hadoop.hbase.io.hfile.HFile.Writer writer = org.apache.hadoop.hbase.regionserver.StoreFile.createWriter(this.fs, dir, 8 * 1024);\n    writeStoreFile(writer);\n    org.apache.hadoop.hbase.regionserver.StoreFile hsf = new org.apache.hadoop.hbase.regionserver.StoreFile(this.fs, writer.getPath(), true, conf, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE, false);\n    org.apache.hadoop.hbase.io.hfile.HFile.Reader reader = hsf.createReader();\n    org.apache.hadoop.hbase.KeyValue kv = org.apache.hadoop.hbase.KeyValue.createKeyValueFromKey(reader.midkey());\n    byte[] midRow = kv.getRow();\n    kv = org.apache.hadoop.hbase.KeyValue.createKeyValueFromKey(reader.getLastKey());\n    byte[] finalRow = kv.getRow();\n    org.apache.hadoop.fs.Path refPath = org.apache.hadoop.hbase.regionserver.StoreFile.split(fs, dir, hsf, midRow, org.apache.hadoop.hbase.io.Reference.Range.top);\n    org.apache.hadoop.hbase.regionserver.StoreFile refHsf = new org.apache.hadoop.hbase.regionserver.StoreFile(this.fs, refPath, true, conf, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE, false);\n    org.apache.hadoop.hbase.io.hfile.HFileScanner s = refHsf.createReader().getScanner(false, false);\n    for (boolean first = true; ((!s.isSeeked()) && s.seekTo()) || s.next();) {\n        java.nio.ByteBuffer bb = s.getKey();\n        kv = org.apache.hadoop.hbase.KeyValue.createKeyValueFromKey(bb);\n        if (first) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(kv.getRow(), midRow));\n            first = false;\n        }\n    }\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(kv.getRow(), finalRow));\n}",
            "ClassName": "TestStoreFile",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testReference",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 24,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.io.hfile.HFile.Writer writer = org.apache.hadoop.hbase.regionserver.StoreFile.createWriter(this.fs, new org.apache.hadoop.fs.Path(new org.apache.hadoop.fs.Path(this.testDir, \"regionname\"), \"familyname\"), 2 * 1024);\n    writeStoreFile(writer);\n    checkHalfHFile(new org.apache.hadoop.hbase.regionserver.StoreFile(this.fs, writer.getPath(), true, conf, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE, false));\n}",
            "ClassName": "TestStoreFile",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testBasicHalfMapFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.fs.FileSystem fs = org.apache.hadoop.fs.FileSystem.getLocal(conf);\n    conf.setFloat(\"io.hfile.bloom.error.rate\", ((float) (0.01)));\n    conf.setBoolean(\"io.hfile.bloom.enabled\", true);\n    org.apache.hadoop.fs.Path f = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.TestStoreFile.ROOT_DIR, getName());\n    org.apache.hadoop.hbase.regionserver.StoreFile.Writer writer = new org.apache.hadoop.hbase.regionserver.StoreFile.Writer(fs, f, org.apache.hadoop.hbase.regionserver.StoreFile.DEFAULT_BLOCKSIZE_SMALL, org.apache.hadoop.hbase.io.hfile.HFile.DEFAULT_COMPRESSION_ALGORITHM, conf, org.apache.hadoop.hbase.KeyValue.COMPARATOR, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.ROW, 2000);\n    long now = java.lang.System.currentTimeMillis();\n    for (int i = 0; i < 2000; i += 2) {\n        java.lang.String row = java.lang.String.format(org.apache.hadoop.hbase.regionserver.TestStoreFile.localFormatter, java.lang.Integer.valueOf(i));\n        org.apache.hadoop.hbase.KeyValue kv = new org.apache.hadoop.hbase.KeyValue(row.getBytes(), \"family\".getBytes(), \"col\".getBytes(), now, \"value\".getBytes());\n        writer.append(kv);\n    }\n    writer.close();\n    org.apache.hadoop.hbase.regionserver.StoreFile.Reader reader = new org.apache.hadoop.hbase.regionserver.StoreFile.Reader(fs, f, null, false);\n    reader.loadFileInfo();\n    reader.loadBloomfilter();\n    org.apache.hadoop.hbase.io.hfile.HFileScanner scanner = reader.getScanner(false, false);\n    int falsePos = 0;\n    int falseNeg = 0;\n    for (int i = 0; i < 2000; i++) {\n        java.lang.String row = java.lang.String.format(org.apache.hadoop.hbase.regionserver.TestStoreFile.localFormatter, java.lang.Integer.valueOf(i));\n        java.util.TreeSet<byte[]> columns = new java.util.TreeSet<byte[]>();\n        columns.add(\"family:col\".getBytes());\n        boolean exists = scanner.shouldSeek(row.getBytes(), columns);\n        if ((i % 2) == 0) {\n            if (!exists)\n                falseNeg++;\n\n        } else {\n            if (exists)\n                falsePos++;\n\n        }\n    }\n    reader.close();\n    fs.delete(f, true);\n    java.lang.System.out.println(\"False negatives: \" + falseNeg);\n    junit.framework.Assert.assertEquals(0, falseNeg);\n    java.lang.System.out.println(\"False positives: \" + falsePos);\n    junit.framework.Assert.assertTrue(falsePos < 2);\n}",
            "ClassName": "TestStoreFile",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testBloomFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 41,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.fs.Path basedir = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.TestStore.DIR + methodName);\n    org.apache.hadoop.fs.Path logdir = new org.apache.hadoop.fs.Path((org.apache.hadoop.hbase.regionserver.TestStore.DIR + methodName) + \"/logs\");\n    org.apache.hadoop.fs.Path oldLogDir = new org.apache.hadoop.fs.Path(basedir, org.apache.hadoop.hbase.HConstants.HREGION_OLDLOGDIR_NAME);\n    org.apache.hadoop.hbase.HColumnDescriptor hcd = new org.apache.hadoop.hbase.HColumnDescriptor(family);\n    org.apache.hadoop.hbase.HBaseConfiguration conf = new org.apache.hadoop.hbase.HBaseConfiguration();\n    org.apache.hadoop.fs.FileSystem fs = org.apache.hadoop.fs.FileSystem.get(conf);\n    org.apache.hadoop.fs.Path reconstructionLog = null;\n    org.apache.hadoop.util.Progressable reporter = null;\n    fs.delete(logdir, true);\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(table);\n    htd.addFamily(hcd);\n    org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(htd, null, null, false);\n    org.apache.hadoop.hbase.regionserver.wal.HLog hlog = new org.apache.hadoop.hbase.regionserver.wal.HLog(fs, logdir, oldLogDir, conf, null);\n    org.apache.hadoop.hbase.regionserver.HRegion region = new org.apache.hadoop.hbase.regionserver.HRegion(basedir, hlog, fs, conf, info, null);\n    store = new org.apache.hadoop.hbase.regionserver.Store(basedir, region, hcd, fs, reconstructionLog, conf, reporter);\n}",
            "ClassName": "TestStore",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "init",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 17,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.fs.Path ncTFile = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.io.hfile.TestHFile.ROOT_DIR, \"basic.hfile\");\n    org.apache.hadoop.fs.FSDataOutputStream fout = createFSOutput(ncTFile);\n    org.apache.hadoop.hbase.io.hfile.HFile.Writer writer = new org.apache.hadoop.hbase.io.hfile.HFile.Writer(fout, minBlockSize, org.apache.hadoop.hbase.io.hfile.Compression.getCompressionAlgorithmByName(codec), null);\n    org.apache.hadoop.hbase.io.hfile.TestHFile.LOG.info(writer);\n    writeRecords(writer);\n    fout.close();\n    org.apache.hadoop.fs.FSDataInputStream fin = fs.open(ncTFile);\n    org.apache.hadoop.hbase.io.hfile.HFile.Reader reader = new org.apache.hadoop.hbase.io.hfile.HFile.Reader(fs.open(ncTFile), fs.getFileStatus(ncTFile).getLen(), null, false);\n    reader.loadFileInfo();\n    org.apache.hadoop.hbase.io.hfile.HFileScanner scanner = reader.getScanner(true, false);\n    scanner.seekTo();\n    readAllRecords(scanner);\n    scanner.seekTo(getSomeKey(50));\n    junit.framework.Assert.assertTrue(\"location lookup failed\", scanner.seekTo(getSomeKey(50)) == 0);\n    java.nio.ByteBuffer readKey = scanner.getKey();\n    junit.framework.Assert.assertTrue(\"seeked key does not match\", java.util.Arrays.equals(getSomeKey(50), org.apache.hadoop.hbase.util.Bytes.toBytes(readKey)));\n    scanner.seekTo(new byte[0]);\n    java.nio.ByteBuffer val1 = scanner.getValue();\n    scanner.seekTo(new byte[0]);\n    java.nio.ByteBuffer val2 = scanner.getValue();\n    junit.framework.Assert.assertTrue(java.util.Arrays.equals(org.apache.hadoop.hbase.util.Bytes.toBytes(val1), org.apache.hadoop.hbase.util.Bytes.toBytes(val2)));\n    reader.close();\n    fin.close();\n    fs.delete(ncTFile, true);\n}",
            "ClassName": "TestHFile",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "basicWithSomeCodec",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 26,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFSErrorsExposed",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 1,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testHFileScannerThrowsErrors",
        "NumberOfAsserts": 4,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 25,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    LOG.info(\"Starting testMasterSessionExpired\");\n    new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.TestZooKeeper.conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.expireMasterSession();\n    testSanity();\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.startMiniZKCluster();\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.startMiniCluster(2);\n    org.apache.hadoop.hbase.TestZooKeeper.conf = org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.getConfiguration();\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.ensureSomeRegionServersAvailable(2);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(this.conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    java.util.List<byte[]> rows = new java.util.ArrayList<byte[]>();\n    org.apache.hadoop.hbase.client.ResultScanner s = t.getScanner(new org.apache.hadoop.hbase.client.Scan());\n    for (org.apache.hadoop.hbase.client.Result result : s) {\n        org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(result.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER));\n        org.apache.hadoop.hbase.HTableDescriptor desc = info.getTableDesc();\n        if (org.apache.hadoop.hbase.util.Bytes.compareTo(desc.getName(), tableName) == 0) {\n            LOG.info(\"getMetaTableRows: row -> \" + org.apache.hadoop.hbase.util.Bytes.toStringBinary(result.getRow()));\n            rows.add(result.getRow());\n        }\n    }\n    s.close();\n    return rows;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getMetaTableRows",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 15,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        org.apache.hadoop.hbase.HRegionInfo hri = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(b);\n        byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(row);\n        p.add(org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily(), org.apache.hadoop.hbase.master.TestMasterTransitions.getTestQualifier(), row);\n        t.put(p);\n        rows++;\n    }\n    s.close();\n    org.junit.Assert.assertEquals(expected, rows);\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addToEachStartKey",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestZooKeeper",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testMasterSessionExpired",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 6,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    LOG.info(\"Starting testMetaScanner\");\n    final byte[] TABLENAME = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testMetaScanner\");\n    final byte[] FAMILY = org.apache.hadoop.hbase.util.Bytes.toBytes(\"family\");\n    org.apache.hadoop.hbase.client.TestMetaScanner.TEST_UTIL.createTable(TABLENAME, FAMILY);\n    org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.client.TestMetaScanner.TEST_UTIL.getConfiguration();\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, TABLENAME);\n    org.apache.hadoop.hbase.client.TestMetaScanner.TEST_UTIL.createMultiRegions(conf, table, FAMILY, new byte[][]{ org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.util.Bytes.toBytes(\"region_a\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"region_b\") });\n    org.apache.hadoop.hbase.client.TestMetaScanner.TEST_UTIL.countRows(table);\n    org.apache.hadoop.hbase.client.MetaScanner.MetaScannerVisitor visitor = mock(org.apache.hadoop.hbase.client.MetaScanner.MetaScannerVisitor.class);\n    doReturn(true).when(visitor).processRow(((org.apache.hadoop.hbase.client.Result) (anyObject())));\n    org.apache.hadoop.hbase.client.MetaScanner.metaScan(conf, visitor, TABLENAME);\n    verify(visitor, times(3)).processRow(((org.apache.hadoop.hbase.client.Result) (anyObject())));\n    reset(visitor);\n    doReturn(true).when(visitor).processRow(((org.apache.hadoop.hbase.client.Result) (anyObject())));\n    org.apache.hadoop.hbase.client.MetaScanner.metaScan(conf, visitor, TABLENAME, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY, 1000);\n    verify(visitor, times(3)).processRow(((org.apache.hadoop.hbase.client.Result) (anyObject())));\n    reset(visitor);\n    doReturn(true).when(visitor).processRow(((org.apache.hadoop.hbase.client.Result) (anyObject())));\n    org.apache.hadoop.hbase.client.MetaScanner.metaScan(conf, visitor, TABLENAME, org.apache.hadoop.hbase.util.Bytes.toBytes(\"region_ac\"), 1000);\n    verify(visitor, times(2)).processRow(((org.apache.hadoop.hbase.client.Result) (anyObject())));\n    reset(visitor);\n    doReturn(true).when(visitor).processRow(((org.apache.hadoop.hbase.client.Result) (anyObject())));\n    org.apache.hadoop.hbase.client.MetaScanner.metaScan(conf, visitor, TABLENAME, org.apache.hadoop.hbase.util.Bytes.toBytes(\"region_ac\"), 1);\n    verify(visitor, times(1)).processRow(((org.apache.hadoop.hbase.client.Result) (anyObject())));\n}",
        "CUT_1": {
            "Body": "{\n    final java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> regionMap = new java.util.TreeMap<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress>();\n    org.apache.hadoop.hbase.client.MetaScanner.MetaScannerVisitor visitor = new org.apache.hadoop.hbase.client.MetaScanner.MetaScannerVisitor() {\n        public boolean processRow(org.apache.hadoop.hbase.client.Result rowResult) throws java.io.IOException {\n            org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(rowResult.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER));\n            if (!org.apache.hadoop.hbase.util.Bytes.equals(info.getTableDesc().getName(), getTableName())) {\n                return false;\n            }\n            org.apache.hadoop.hbase.HServerAddress server = new org.apache.hadoop.hbase.HServerAddress();\n            byte[] value = rowResult.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n            if ((value != null) && (value.length > 0)) {\n                java.lang.String address = org.apache.hadoop.hbase.util.Bytes.toString(value);\n                server = new org.apache.hadoop.hbase.HServerAddress(address);\n            }\n            if (!(info.isOffline() || info.isSplit())) {\n                regionMap.put(new org.apache.hadoop.hbase.client.UnmodifyableHRegionInfo(info), server);\n            }\n            return true;\n        }\n    };\n    org.apache.hadoop.hbase.client.MetaScanner.metaScan(configuration, visitor, tableName);\n    return regionMap;\n}",
            "ClassName": "HTable",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getRegionsInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    final java.util.ArrayList<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress>> result = com.google.common.collect.Lists.newArrayList();\n    org.apache.hadoop.hbase.client.MetaScanner.MetaScannerVisitor visitor = new org.apache.hadoop.hbase.client.MetaScanner.MetaScannerVisitor() {\n        @java.lang.Override\n        public boolean processRow(org.apache.hadoop.hbase.client.Result data) throws java.io.IOException {\n            if ((data == null) || (data.size() <= 0))\n                return true;\n\n            org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> pair = metaRowToRegionPair(data);\n            if (pair == null)\n                return false;\n\n            if (!org.apache.hadoop.hbase.util.Bytes.equals(pair.getFirst().getTableDesc().getName(), tableName)) {\n                return false;\n            }\n            result.add(pair);\n            return true;\n        }\n    };\n    org.apache.hadoop.hbase.client.MetaScanner.metaScan(conf, visitor, tableName);\n    return result;\n}",
            "ClassName": "HMaster",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    final java.util.concurrent.atomic.AtomicReference<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress>> result = new java.util.concurrent.atomic.AtomicReference<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress>>(null);\n    org.apache.hadoop.hbase.client.MetaScanner.MetaScannerVisitor visitor = new org.apache.hadoop.hbase.client.MetaScanner.MetaScannerVisitor() {\n        @java.lang.Override\n        public boolean processRow(org.apache.hadoop.hbase.client.Result data) throws java.io.IOException {\n            if ((data == null) || (data.size() <= 0))\n                return true;\n\n            org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> pair = metaRowToRegionPair(data);\n            if (pair == null)\n                return false;\n\n            if (!org.apache.hadoop.hbase.util.Bytes.equals(pair.getFirst().getTableDesc().getName(), tableName)) {\n                return false;\n            }\n            result.set(pair);\n            return true;\n        }\n    };\n    org.apache.hadoop.hbase.client.MetaScanner.metaScan(conf, visitor, tableName, rowKey, 1);\n    return result.get();\n}",
            "ClassName": "HMaster",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableRegionForRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 7
        },
        "CUT_4": {
            "Body": "{\n    final java.util.List<byte[]> startKeyList = new java.util.ArrayList<byte[]>();\n    final java.util.List<byte[]> endKeyList = new java.util.ArrayList<byte[]>();\n    org.apache.hadoop.hbase.client.MetaScanner.MetaScannerVisitor visitor = new org.apache.hadoop.hbase.client.MetaScanner.MetaScannerVisitor() {\n        public boolean processRow(org.apache.hadoop.hbase.client.Result rowResult) throws java.io.IOException {\n            org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(rowResult.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER));\n            if (org.apache.hadoop.hbase.util.Bytes.equals(info.getTableDesc().getName(), getTableName())) {\n                if (!(info.isOffline() || info.isSplit())) {\n                    startKeyList.add(info.getStartKey());\n                    endKeyList.add(info.getEndKey());\n                }\n            }\n            return true;\n        }\n    };\n    org.apache.hadoop.hbase.client.MetaScanner.metaScan(configuration, visitor, this.tableName);\n    return new org.apache.hadoop.hbase.util.Pair(startKeyList.toArray(new byte[startKeyList.size()][]), endKeyList.toArray(new byte[endKeyList.size()][]));\n}",
            "ClassName": "HTable",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getStartEndKeys",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 18,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(this.conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    java.util.List<byte[]> rows = new java.util.ArrayList<byte[]>();\n    org.apache.hadoop.hbase.client.ResultScanner s = t.getScanner(new org.apache.hadoop.hbase.client.Scan());\n    for (org.apache.hadoop.hbase.client.Result result : s) {\n        org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(result.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER));\n        org.apache.hadoop.hbase.HTableDescriptor desc = info.getTableDesc();\n        if (org.apache.hadoop.hbase.util.Bytes.compareTo(desc.getName(), tableName) == 0) {\n            LOG.info(\"getMetaTableRows: row -> \" + org.apache.hadoop.hbase.util.Bytes.toStringBinary(result.getRow()));\n            rows.add(result.getRow());\n        }\n    }\n    s.close();\n    return rows;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getMetaTableRows",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 15,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestMetaScanner",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testMetaScanner",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 26,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    final java.lang.String REGION = \"region__1\";\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.removeAll(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.add(REGION);\n    generateHLogs(1, 10, -1);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    org.apache.hadoop.fs.Path originalLog = fs.listStatus(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir)[0].getPath();\n    org.apache.hadoop.fs.Path splitLog = getLogForRegion(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, REGION);\n    org.junit.Assert.assertEquals(\"edits differ after split\", true, logsAreEqual(originalLog, splitLog));\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.removeAll(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions);\n    for (int i = 0; i < 100; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.add(\"region__\" + i);\n    }\n    generateHLogs(1, 100, -1);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    fs.rename(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir);\n    org.apache.hadoop.fs.Path firstSplitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME) + \".first\");\n    org.apache.hadoop.fs.Path splitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME));\n    fs.rename(splitPath, firstSplitPath);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    org.junit.Assert.assertEquals(0, compareHLogSplitDirs(firstSplitPath, splitPath));\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSplittingLargeNumberOfRegionsConsistency",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    for (int i = 0; i < writers; i++) {\n        writer[i] = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + i), conf);\n        for (int j = 0; j < entries; j++) {\n            int prefix = 0;\n            for (java.lang.String region : org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions) {\n                java.lang.String row_key = ((region + (prefix++)) + i) + j;\n                appendEntry(writer[i], org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, region.getBytes(), row_key.getBytes(), org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.FAMILY, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.QUALIFIER, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.VALUE, seq);\n            }\n        }\n        if (i != leaveOpen) {\n            writer[i].close();\n            flushToConsole(\"Closing writer \" + i);\n        }\n    }\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "generateHLogs",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.flushlogentries\", 1);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setStrings(\"hbase.rootdir\", org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir.toString());\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setClass(\"hbase.regionserver.hlog.writer.impl\", org.apache.hadoop.hbase.regionserver.wal.InstrumentedSequenceFileLogWriter.class, org.apache.hadoop.hbase.regionserver.wal.HLog.Writer.class);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.startMiniDFSCluster(2);\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Writer writer = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + suffix), conf);\n    if (closeFile)\n        writer.close();\n\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "injectEmptyFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.shutdownMiniDFSCluster();\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHLogSplit",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testSplitPreservesEdits",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 11,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[] name = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testReadOnlyTable\");\n    org.apache.hadoop.hbase.client.HTable table = org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.createTable(name, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    byte[] value = org.apache.hadoop.hbase.util.Bytes.toBytes(\"somedata\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(value);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, value);\n    table.put(put);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    i.setOffline(!online);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(i.getRegionName());\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(i));\n    return put;\n}",
            "ClassName": "ChangeTableState",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "updateRegionInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    this.server = cluster.getRegionServerThreads().get(0).getRegionServer();\n    this.log = server.getLog();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    for (int i = 1; i <= 256; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + java.lang.String.format(\"%1$04d\", i)));\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, value);\n        table.put(put);\n        if ((i % 32) == 0) {\n            try {\n                java.lang.Thread.sleep(2000);\n            } catch (java.lang.InterruptedException e) {\n            }\n        }\n    }\n}",
            "ClassName": "TestLogRolling",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startAndWriteData",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(c, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.Result res = t.get(get);\n    org.apache.hadoop.hbase.KeyValue[] kvs = res.raw();\n    if (kvs.length <= 0) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    byte[] value = kvs[0].getValue();\n    if (value == null) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(value);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    info.setOffline(onlineOffline);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(info));\n    t.put(put);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(row);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER);\n    t.delete(delete);\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "changeOnlineStatus",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(i.getRegionName());\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(i));\n    server.put(regionName, put);\n    org.apache.hadoop.hbase.master.ModifyTableMeta.LOG.debug(\"updated HTableDescriptor for region \" + i.getRegionNameAsString());\n}",
            "ClassName": "ModifyTableMeta",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "updateRegionInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testReadOnlyTable",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 8,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    runTest(\"testRegionCrossingLoad\", new byte[][][]{ new byte[][]{ org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"eee\") }, new byte[][]{ org.apache.hadoop.hbase.util.Bytes.toBytes(\"fff\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"zzz\") } });\n}",
        "CUT_1": {
            "Body": "{\n    return new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(row), org.apache.hadoop.hbase.util.Bytes.toBytes(family), org.apache.hadoop.hbase.util.Bytes.toBytes(qualifier), timestamp, type, org.apache.hadoop.hbase.util.Bytes.toBytes(value));\n}",
            "ClassName": "KeyValueTestUtil",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "create",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    super.setUp();\n    data = org.apache.hadoop.hbase.util.Bytes.toBytes(\"data\");\n    row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam1\");\n    col1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"col1\");\n    row2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row2\");\n    fam2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam2\");\n    col2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"col2\");\n    col3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"col3\");\n    col4 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"col4\");\n    col5 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"col5\");\n}",
            "ClassName": "TestKeyValueHeap",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 13,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    setValue(org.apache.hadoop.hbase.util.Bytes.toBytes(key), org.apache.hadoop.hbase.util.Bytes.toBytes(value));\n}",
            "ClassName": "HColumnDescriptor",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setValue",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.HRegionInfo a = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(\"a\"), null, null);\n    org.apache.hadoop.hbase.HRegionInfo b = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(\"b\"), null, null);\n    junit.framework.Assert.assertTrue(a.compareTo(b) != 0);\n    org.apache.hadoop.hbase.HTableDescriptor t = new org.apache.hadoop.hbase.HTableDescriptor(\"t\");\n    byte[] midway = org.apache.hadoop.hbase.util.Bytes.toBytes(\"midway\");\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, null, midway);\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, midway, null);\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n    junit.framework.Assert.assertTrue(b.compareTo(a) > 0);\n    junit.framework.Assert.assertEquals(a, a);\n    junit.framework.Assert.assertTrue(a.compareTo(a) == 0);\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"a\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"d\"));\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"e\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"g\"));\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"dddd\"));\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"e\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"g\"));\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"dddd\"));\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"eeee\"));\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n}",
            "ClassName": "TestCompare",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testHRegionInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.KeyValue rowA = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable,www.hbase.org/,1234\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), java.lang.Long.MAX_VALUE, ((byte[]) (null)));\n    org.apache.hadoop.hbase.KeyValue rowB = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable,www.hbase.org/%20,99999\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), java.lang.Long.MAX_VALUE, ((byte[]) (null)));\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.META_COMPARATOR.compare(rowA, rowB) < 0);\n    rowA = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable,,1234\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), java.lang.Long.MAX_VALUE, ((byte[]) (null)));\n    rowB = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable,$www.hbase.org/,99999\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), java.lang.Long.MAX_VALUE, ((byte[]) (null)));\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.META_COMPARATOR.compare(rowA, rowB) < 0);\n    rowA = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\".META.,testtable,www.hbase.org/,1234,4321\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), java.lang.Long.MAX_VALUE, ((byte[]) (null)));\n    rowB = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\".META.,testtable,www.hbase.org/%20,99999,99999\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), java.lang.Long.MAX_VALUE, ((byte[]) (null)));\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.ROOT_COMPARATOR.compare(rowA, rowB) < 0);\n}",
            "ClassName": "TestKeyValue",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testKeyValueBorderCases",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 11,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestLoadIncrementalHFiles",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testRegionCrossingLoad",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    util.startMiniCluster(1);\n    try {\n        runTestAtomicity(20000, 5, 2, 2, 3);\n    } finally {\n        util.shutdownMiniCluster();\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "afterAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestZKBasedCloseRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "afterAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestZKBasedReopenRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "afterAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAcidGuarantees",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testMixedAtomicity",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 8,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = new org.apache.hadoop.hbase.client.HTablePool(((org.apache.hadoop.hbase.HBaseConfiguration) (null)), 2);\n    java.lang.String tableName = \"testTable\";\n    org.apache.hadoop.hbase.client.HTableInterface table1 = pool.getTable(tableName);\n    org.apache.hadoop.hbase.client.HTableInterface table2 = pool.getTable(tableName);\n    org.apache.hadoop.hbase.client.HTableInterface table3 = pool.getTable(tableName);\n    pool.putTable(table1);\n    pool.putTable(table2);\n    pool.putTable(table3);\n    org.apache.hadoop.hbase.client.HTableInterface sameTable1 = pool.getTable(tableName);\n    org.apache.hadoop.hbase.client.HTableInterface sameTable2 = pool.getTable(tableName);\n    org.apache.hadoop.hbase.client.HTableInterface sameTable3 = pool.getTable(tableName);\n    junit.framework.Assert.assertSame(table1, sameTable1);\n    junit.framework.Assert.assertSame(table2, sameTable2);\n    junit.framework.Assert.assertNotSame(table3, sameTable3);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    try {\n        return ((org.apache.hadoop.hbase.client.HTable) (table)).getRegionsInfo();\n    } finally {\n        pool.putTable(table);\n    }\n}",
            "ClassName": "RegionsResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    try {\n        return table.getTableDescriptor();\n    } finally {\n        pool.putTable(table);\n    }\n}",
            "ClassName": "SchemaResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableSchema",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return pool;\n}",
            "ClassName": "RESTServlet",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTablePool",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    java.lang.Integer i = maxAgeMap.get(tableName);\n    if (i != null) {\n        return i.intValue();\n    }\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    try {\n        int maxAge = org.apache.hadoop.hbase.rest.Constants.DEFAULT_MAX_AGE;\n        for (org.apache.hadoop.hbase.HColumnDescriptor family : table.getTableDescriptor().getFamilies()) {\n            int ttl = family.getTimeToLive();\n            if (ttl < 0) {\n                continue;\n            }\n            if (ttl < maxAge) {\n                maxAge = ttl;\n            }\n        }\n        maxAgeMap.put(tableName, maxAge);\n        return maxAge;\n    } finally {\n        pool.putTable(table);\n    }\n}",
            "ClassName": "RESTServlet",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getMaxAge",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    servlet.getMetrics().incrementRequests(1);\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = null;\n    try {\n        java.util.List<org.apache.hadoop.hbase.rest.model.RowModel> rows = model.getRows();\n        table = pool.getTable(tableName);\n        ((org.apache.hadoop.hbase.client.HTable) (table)).setAutoFlush(false);\n        for (org.apache.hadoop.hbase.rest.model.RowModel row : rows) {\n            byte[] key = row.getKey();\n            org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(key);\n            for (org.apache.hadoop.hbase.rest.model.CellModel cell : row.getCells()) {\n                byte[][] parts = org.apache.hadoop.hbase.KeyValue.parseColumn(cell.getColumn());\n                if ((parts.length == 2) && (parts[1].length > 0)) {\n                    put.add(parts[0], parts[1], cell.getTimestamp(), cell.getValue());\n                } else {\n                    put.add(parts[0], null, cell.getTimestamp(), cell.getValue());\n                }\n            }\n            table.put(put);\n            if (org.apache.hadoop.hbase.rest.RowResource.LOG.isDebugEnabled()) {\n                org.apache.hadoop.hbase.rest.RowResource.LOG.debug(\"PUT \" + put.toString());\n            }\n        }\n        ((org.apache.hadoop.hbase.client.HTable) (table)).setAutoFlush(true);\n        table.flushCommits();\n        javax.ws.rs.core.Response.ResponseBuilder response = javax.ws.rs.core.Response.ok();\n        return response.build();\n    } catch (java.io.IOException e) {\n        throw new javax.ws.rs.WebApplicationException(e, Response.Status.SERVICE_UNAVAILABLE);\n    } finally {\n        if (table != null) {\n            pool.putTable(table);\n        }\n    }\n}",
            "ClassName": "RowResource",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "update",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 36,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHTablePool",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testTableWithMaxSize",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 16,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.ipc.HBaseServer.Call call = org.apache.hadoop.hbase.ipc.HBaseServer.CurCall.get();\n    if (call != null) {\n        return call.connection.socket.getInetAddress();\n    }\n    return null;\n}",
            "ClassName": "HBaseServer",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getRemoteIp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    this.writer.append(entry.getKey(), entry.getEdit());\n}",
            "ClassName": "SequenceFileLogWriter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "append",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return cacheBlocks;\n}",
            "ClassName": "Scan",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getCacheBlocks",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.HRegion r = org.apache.hadoop.hbase.regionserver.HRegion.newHRegion(org.apache.hadoop.hbase.HTableDescriptor.getTableDir(rootDir, regionInfo.getTableDesc().getName()), this.hlog, this.fs, conf, regionInfo, this.cacheFlusher);\n    r.initialize(null, new org.apache.hadoop.util.Progressable() {\n        public void progress() {\n            addProcessingMessage(regionInfo);\n        }\n    });\n    return r;\n}",
            "ClassName": "HRegionServer",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "instantiateRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    final org.apache.hadoop.hbase.HRegionInfo oldRegionInfo = region.getRegionInfo();\n    final long startTime = java.lang.System.currentTimeMillis();\n    final org.apache.hadoop.hbase.regionserver.HRegion[] newRegions = region.splitRegion(midKey);\n    if (newRegions == null) {\n        return;\n    }\n    org.apache.hadoop.hbase.client.HTable t = null;\n    if (region.getRegionInfo().isMetaTable()) {\n        if (this.root == null) {\n            this.root = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.ROOT_TABLE_NAME);\n        }\n        t = root;\n    } else {\n        if (meta == null) {\n            meta = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n        }\n        t = meta;\n    }\n    oldRegionInfo.setOffline(true);\n    oldRegionInfo.setSplit(true);\n    this.server.removeFromOnlineRegions(oldRegionInfo);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(oldRegionInfo.getRegionName());\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(oldRegionInfo));\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITA_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(newRegions[0].getRegionInfo()));\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITB_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(newRegions[1].getRegionInfo()));\n    t.put(put);\n    for (int i = 0; i < newRegions.length; i++) {\n        put = new org.apache.hadoop.hbase.client.Put(newRegions[i].getRegionName());\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(newRegions[i].getRegionInfo()));\n        t.put(put);\n    }\n    server.reportSplit(oldRegionInfo, newRegions[0].getRegionInfo(), newRegions[1].getRegionInfo());\n    org.apache.hadoop.hbase.regionserver.CompactSplitThread.LOG.info((((((((\"region split, META updated, and report to master all\" + \" successful. Old region=\") + oldRegionInfo.toString()) + \", new regions: \") + newRegions[0].toString()) + \", \") + newRegions[1].toString()) + \". Split took \") + org.apache.hadoop.util.StringUtils.formatTimeDiff(java.lang.System.currentTimeMillis(), startTime));\n}",
            "ClassName": "CompactSplitThread",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "split",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 37,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestRegionServerOperationQueue",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testNothing",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 2,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.avro.AvroServer.HBaseImpl impl = new org.apache.hadoop.hbase.avro.AvroServer.HBaseImpl();\n    org.apache.hadoop.hbase.avro.generated.ATableDescriptor tableA = new org.apache.hadoop.hbase.avro.generated.ATableDescriptor();\n    tableA.name = org.apache.hadoop.hbase.avro.TestAvroServer.tableAname;\n    org.apache.hadoop.hbase.avro.generated.AFamilyDescriptor familyA = new org.apache.hadoop.hbase.avro.generated.AFamilyDescriptor();\n    familyA.name = org.apache.hadoop.hbase.avro.TestAvroServer.familyAname;\n    org.apache.avro.Schema familyArraySchema = org.apache.avro.Schema.createArray(org.apache.hadoop.hbase.avro.generated.AFamilyDescriptor.SCHEMA$);\n    org.apache.avro.generic.GenericArray<org.apache.hadoop.hbase.avro.generated.AFamilyDescriptor> families = new org.apache.avro.generic.GenericData.Array<org.apache.hadoop.hbase.avro.generated.AFamilyDescriptor>(1, familyArraySchema);\n    families.add(familyA);\n    tableA.families = families;\n    impl.createTable(tableA);\n    org.junit.Assert.assertEquals(impl.describeTable(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname).families.size(), 1);\n    org.apache.hadoop.hbase.avro.generated.AGet getA = new org.apache.hadoop.hbase.avro.generated.AGet();\n    getA.row = org.apache.hadoop.hbase.avro.TestAvroServer.rowAname;\n    org.apache.avro.Schema columnsSchema = org.apache.avro.Schema.createArray(org.apache.hadoop.hbase.avro.generated.AColumn.SCHEMA$);\n    org.apache.avro.generic.GenericArray<org.apache.hadoop.hbase.avro.generated.AColumn> columns = new org.apache.avro.generic.GenericData.Array<org.apache.hadoop.hbase.avro.generated.AColumn>(1, columnsSchema);\n    org.apache.hadoop.hbase.avro.generated.AColumn column = new org.apache.hadoop.hbase.avro.generated.AColumn();\n    column.family = org.apache.hadoop.hbase.avro.TestAvroServer.familyAname;\n    column.qualifier = org.apache.hadoop.hbase.avro.TestAvroServer.qualifierAname;\n    columns.add(column);\n    getA.columns = columns;\n    org.junit.Assert.assertFalse(impl.exists(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname, getA));\n    org.apache.hadoop.hbase.avro.generated.APut putA = new org.apache.hadoop.hbase.avro.generated.APut();\n    putA.row = org.apache.hadoop.hbase.avro.TestAvroServer.rowAname;\n    org.apache.avro.Schema columnValuesSchema = org.apache.avro.Schema.createArray(org.apache.hadoop.hbase.avro.generated.AColumnValue.SCHEMA$);\n    org.apache.avro.generic.GenericArray<org.apache.hadoop.hbase.avro.generated.AColumnValue> columnValues = new org.apache.avro.generic.GenericData.Array<org.apache.hadoop.hbase.avro.generated.AColumnValue>(1, columnValuesSchema);\n    org.apache.hadoop.hbase.avro.generated.AColumnValue acv = new org.apache.hadoop.hbase.avro.generated.AColumnValue();\n    acv.family = org.apache.hadoop.hbase.avro.TestAvroServer.familyAname;\n    acv.qualifier = org.apache.hadoop.hbase.avro.TestAvroServer.qualifierAname;\n    acv.value = org.apache.hadoop.hbase.avro.TestAvroServer.valueA;\n    columnValues.add(acv);\n    putA.columnValues = columnValues;\n    impl.put(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname, putA);\n    org.junit.Assert.assertTrue(impl.exists(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname, getA));\n    org.junit.Assert.assertEquals(impl.get(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname, getA).entries.size(), 1);\n    impl.disableTable(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname);\n    impl.deleteTable(org.apache.hadoop.hbase.avro.TestAvroServer.tableAname);\n}",
        "CUT_1": {
            "Body": "{\n    return org.apache.hadoop.hbase.avro.generated.AColumn.SCHEMA$;\n}",
            "ClassName": "AColumn",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getSchema",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return org.apache.hadoop.hbase.avro.generated.AFamilyDescriptor.SCHEMA$;\n}",
            "ClassName": "AFamilyDescriptor",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getSchema",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return org.apache.hadoop.hbase.avro.generated.AColumnValue.SCHEMA$;\n}",
            "ClassName": "AColumnValue",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getSchema",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.avro.Schema s = org.apache.avro.Schema.createArray(org.apache.hadoop.hbase.avro.generated.AResult.SCHEMA$);\n    org.apache.avro.generic.GenericData.Array<org.apache.hadoop.hbase.avro.generated.AResult> aresults = null;\n    if ((results != null) && (results.length > 0)) {\n        aresults = new org.apache.avro.generic.GenericData.Array<org.apache.hadoop.hbase.avro.generated.AResult>(results.length, s);\n        for (org.apache.hadoop.hbase.client.Result result : results) {\n            aresults.add(org.apache.hadoop.hbase.avro.AvroUtil.resultToAResult(result));\n        }\n    } else {\n        aresults = new org.apache.avro.generic.GenericData.Array<org.apache.hadoop.hbase.avro.generated.AResult>(0, s);\n    }\n    return aresults;\n}",
            "ClassName": "AvroUtil",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "resultsToAResults",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 13,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.avro.TestAvroServer.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestAvroServer",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAvroServer",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testDML",
        "NumberOfAsserts": 4,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 38,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.MiniHBaseCluster cluster = org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.TEST_UTIL.getHBaseCluster();\n    org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.LOG.info(\"Number of region servers = \" + cluster.getLiveRegionServerThreads().size());\n    int rsIdx = 0;\n    org.apache.hadoop.hbase.regionserver.HRegionServer regionServer = org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.TEST_UTIL.getHBaseCluster().getRegionServer(rsIdx);\n    java.util.Collection<org.apache.hadoop.hbase.regionserver.HRegion> regions = regionServer.getOnlineRegions();\n    org.apache.hadoop.hbase.regionserver.HRegion region = regions.iterator().next();\n    org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.LOG.debug(\"Asking RS to close region \" + region.getRegionNameAsString());\n    java.util.concurrent.atomic.AtomicBoolean closeEventProcessed = new java.util.concurrent.atomic.AtomicBoolean(false);\n    java.util.concurrent.atomic.AtomicBoolean reopenEventProcessed = new java.util.concurrent.atomic.AtomicBoolean(false);\n    org.apache.hadoop.hbase.master.RegionServerOperationListener listener = new org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.ReopenRegionEventListener(region.getRegionNameAsString(), closeEventProcessed, reopenEventProcessed);\n    org.apache.hadoop.hbase.master.HMaster master = org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.TEST_UTIL.getHBaseCluster().getMaster();\n    master.getRegionServerOperationQueue().registerRegionServerOperationListener(listener);\n    org.apache.hadoop.hbase.HMsg closeRegionMsg = new org.apache.hadoop.hbase.HMsg(org.apache.hadoop.hbase.HMsg.Type.MSG_REGION_CLOSE, region.getRegionInfo(), org.apache.hadoop.hbase.util.Bytes.toBytes(\"Forcing close in test\"));\n    org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.TEST_UTIL.getHBaseCluster().addMessageToSendRegionServer(rsIdx, closeRegionMsg);\n    synchronized(closeEventProcessed) {\n        closeEventProcessed.wait((3 * 60) * 1000);\n    }\n    if (!closeEventProcessed.get()) {\n        throw new java.lang.Exception(\"Timed out, close event not called on master.\");\n    }\n    synchronized(reopenEventProcessed) {\n        reopenEventProcessed.wait((3 * 60) * 1000);\n    }\n    if (!reopenEventProcessed.get()) {\n        throw new java.lang.Exception(\"Timed out, open event not called on master after region close.\");\n    }\n    org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.LOG.info(\"Done with test, RS informed master successfully.\");\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.conf.Configuration c = org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.TEST_UTIL.getConfiguration();\n    c.setBoolean(\"dfs.support.append\", true);\n    c.setInt(\"hbase.regionserver.info.port\", 0);\n    c.setInt(\"hbase.master.meta.thread.rescanfrequency\", 5 * 1000);\n    org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.TEST_UTIL.startMiniCluster(2);\n    org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.TEST_UTIL.createTable(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.TABLENAME), org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.FAMILIES);\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.TABLENAME);\n    int countOfRegions = org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.TEST_UTIL.createMultiRegions(t, org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.getTestFamily());\n    org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.waitUntilAllRegionsAssigned(countOfRegions);\n    org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.addToEachStartKey(countOfRegions);\n}",
            "ClassName": "TestZKBasedReopenRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "beforeAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    if (org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.TEST_UTIL.getHBaseCluster().getLiveRegionServerThreads().size() < 2) {\n        org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.LOG.info(\"Started new server=\" + org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.TEST_UTIL.getHBaseCluster().startRegionServer());\n    }\n}",
            "ClassName": "TestZKBasedReopenRegion",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setup",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.beforeAllTests();\n    org.apache.hadoop.hbase.master.TestZKBasedReopenRegion test = new org.apache.hadoop.hbase.master.TestZKBasedReopenRegion();\n    test.setup();\n    test.testOpenRegion();\n    org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.afterAllTests();\n}",
            "ClassName": "TestZKBasedReopenRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "main",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.FAMILIES[0];\n}",
            "ClassName": "TestZKBasedReopenRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTestFamily",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestZKBasedReopenRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "afterAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestZKBasedReopenRegion",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 2,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 1,
        "Label": 0,
        "MethodName": "testOpenRegion",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 2,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 29,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 18,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    doIncrementalLoadTest(false);\n}",
        "CUT_1": {
            "Body": "{\n    return false;\n}",
            "ClassName": "ScanWildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "done",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return false;\n}",
            "ClassName": "FilterBase",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "filterAllRemaining",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return false;\n}",
            "ClassName": "FilterBase",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "filterRowKey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return false;\n}",
            "ClassName": "FilterBase",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "hasFilterRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return false;\n}",
            "ClassName": "DependentColumnFilter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "filterAllRemaining",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHFileOutputFormat",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testMRIncrementalLoad",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.conf.Configuration conf = util.getConfiguration();\n    org.apache.hadoop.fs.Path testDir = org.apache.hadoop.hbase.HBaseTestingUtility.getTestDir(\"testWritingPEData\");\n    org.apache.hadoop.fs.FileSystem fs = testDir.getFileSystem(conf);\n    conf.setInt(\"io.sort.mb\", 20);\n    conf.setLong(\"hbase.hregion.max.filesize\", 64 * 1024);\n    org.apache.hadoop.mapreduce.Job job = new org.apache.hadoop.mapreduce.Job(conf, \"testWritingPEData\");\n    setupRandomGeneratorMapper(job);\n    byte[] startKey = new byte[org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.RandomKVGeneratingMapper.KEYLEN_DEFAULT];\n    byte[] endKey = new byte[org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.RandomKVGeneratingMapper.KEYLEN_DEFAULT];\n    java.util.Arrays.fill(startKey, ((byte) (0)));\n    java.util.Arrays.fill(endKey, ((byte) (0xff)));\n    job.setPartitionerClass(org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner.class);\n    org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner.setStartKey(job.getConfiguration(), startKey);\n    org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner.setEndKey(job.getConfiguration(), endKey);\n    job.setReducerClass(org.apache.hadoop.hbase.mapreduce.KeyValueSortReducer.class);\n    job.setOutputFormatClass(org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.class);\n    job.setNumReduceTasks(4);\n    org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, testDir);\n    org.junit.Assert.assertTrue(job.waitForCompletion(false));\n    org.apache.hadoop.fs.FileStatus[] files = fs.listStatus(testDir);\n    org.junit.Assert.assertTrue(files.length > 0);\n}",
        "CUT_1": {
            "Body": "{\n    java.lang.String tableName = args[0];\n    org.apache.hadoop.fs.Path inputDir = new org.apache.hadoop.fs.Path(args[1]);\n    org.apache.hadoop.mapreduce.Job job = new org.apache.hadoop.mapreduce.Job(conf, (org.apache.hadoop.hbase.mapreduce.ImportTsv.NAME + \"_\") + tableName);\n    job.setJarByClass(org.apache.hadoop.hbase.mapreduce.ImportTsv.TsvImporter.class);\n    org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(job, inputDir);\n    job.setInputFormatClass(org.apache.hadoop.mapreduce.lib.input.TextInputFormat.class);\n    job.setMapperClass(org.apache.hadoop.hbase.mapreduce.ImportTsv.TsvImporter.class);\n    java.lang.String hfileOutPath = conf.get(org.apache.hadoop.hbase.mapreduce.ImportTsv.BULK_OUTPUT_CONF_KEY);\n    if (hfileOutPath != null) {\n        org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n        job.setReducerClass(org.apache.hadoop.hbase.mapreduce.PutSortReducer.class);\n        org.apache.hadoop.fs.Path outputDir = new org.apache.hadoop.fs.Path(hfileOutPath);\n        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, outputDir);\n        job.setMapOutputKeyClass(org.apache.hadoop.hbase.io.ImmutableBytesWritable.class);\n        job.setMapOutputValueClass(org.apache.hadoop.hbase.client.Put.class);\n        org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.configureIncrementalLoad(job, table);\n    } else {\n        org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob(tableName, null, job);\n        job.setNumReduceTasks(0);\n    }\n    org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.addDependencyJars(job);\n    return job;\n}",
            "ClassName": "ImportTsv",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createSubmittableJob",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 24,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.mapreduce.Job job = new org.apache.hadoop.mapreduce.Job(conf, \"testLocalMRIncrementalLoad\");\n    setupRandomGeneratorMapper(job);\n    org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.configureIncrementalLoad(job, table);\n    org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, outDir);\n    org.junit.Assert.assertEquals(table.getRegionsInfo().size(), job.getNumReduceTasks());\n    org.junit.Assert.assertTrue(job.waitForCompletion(true));\n}",
            "ClassName": "TestHFileOutputFormat",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "runIncrementalPELoad",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    java.lang.String tableName = args[0];\n    org.apache.hadoop.fs.Path inputDir = new org.apache.hadoop.fs.Path(args[1]);\n    org.apache.hadoop.mapreduce.Job job = new org.apache.hadoop.mapreduce.Job(conf, (org.apache.hadoop.hbase.mapreduce.Import.NAME + \"_\") + tableName);\n    job.setJarByClass(org.apache.hadoop.hbase.mapreduce.Import.Importer.class);\n    org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(job, inputDir);\n    job.setInputFormatClass(org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.class);\n    job.setMapperClass(org.apache.hadoop.hbase.mapreduce.Import.Importer.class);\n    org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob(tableName, null, job);\n    job.setNumReduceTasks(0);\n    return job;\n}",
            "ClassName": "Import",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createSubmittableJob",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.fs.Path inputPath = new org.apache.hadoop.fs.Path(args[0]);\n    java.lang.String tableName = args[1];\n    org.apache.hadoop.mapreduce.Job job = new org.apache.hadoop.mapreduce.Job(conf, (org.apache.hadoop.hbase.mapreduce.SampleUploader.NAME + \"_\") + tableName);\n    job.setJarByClass(org.apache.hadoop.hbase.mapreduce.SampleUploader.Uploader.class);\n    org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(job, inputPath);\n    job.setInputFormatClass(org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.class);\n    job.setMapperClass(org.apache.hadoop.hbase.mapreduce.SampleUploader.Uploader.class);\n    org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob(tableName, null, job);\n    job.setNumReduceTasks(0);\n    return job;\n}",
            "ClassName": "SampleUploader",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "configureJob",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.HBaseConfiguration.create();\n    org.apache.hadoop.mapreduce.Job job = org.apache.hadoop.hbase.mapreduce.CopyTable.createSubmittableJob(conf, args);\n    if (job != null) {\n        java.lang.System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n}",
            "ClassName": "CopyTable",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "main",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHFileOutputFormat",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testWritingPEData",
        "NumberOfAsserts": 2,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 23,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    generateHLogs(-1);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    for (java.lang.String region : org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions) {\n        org.apache.hadoop.fs.Path logfile = getLogForRegion(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, region);\n        org.junit.Assert.assertEquals(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.NUM_WRITERS * org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.ENTRIES, countHLog(logfile, fs, conf));\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.removeAll(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions);\n    for (int i = 0; i < 100; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.add(\"region__\" + i);\n    }\n    generateHLogs(1, 100, -1);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    fs.rename(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir);\n    org.apache.hadoop.fs.Path firstSplitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME) + \".first\");\n    org.apache.hadoop.fs.Path splitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME));\n    fs.rename(splitPath, firstSplitPath);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    org.junit.Assert.assertEquals(0, compareHLogSplitDirs(firstSplitPath, splitPath));\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSplittingLargeNumberOfRegionsConsistency",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    for (int i = 0; i < writers; i++) {\n        writer[i] = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + i), conf);\n        for (int j = 0; j < entries; j++) {\n            int prefix = 0;\n            for (java.lang.String region : org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions) {\n                java.lang.String row_key = ((region + (prefix++)) + i) + j;\n                appendEntry(writer[i], org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, region.getBytes(), row_key.getBytes(), org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.FAMILY, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.QUALIFIER, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.VALUE, seq);\n            }\n        }\n        if (i != leaveOpen) {\n            writer[i].close();\n            flushToConsole(\"Closing writer \" + i);\n        }\n    }\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "generateHLogs",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.flushlogentries\", 1);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setStrings(\"hbase.rootdir\", org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir.toString());\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setClass(\"hbase.regionserver.hlog.writer.impl\", org.apache.hadoop.hbase.regionserver.wal.InstrumentedSequenceFileLogWriter.class, org.apache.hadoop.hbase.regionserver.wal.HLog.Writer.class);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.startMiniDFSCluster(2);\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Writer writer = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + suffix), conf);\n    if (closeFile)\n        writer.close();\n\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "injectEmptyFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.shutdownMiniDFSCluster();\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHLogSplit",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 1,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testSplit",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 9,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testVersionLimits\");\n    byte[][] FAMILIES = makeNAscii(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, 3);\n    int[] LIMITS = new int[]{ 1, 3, 5 };\n    long[] STAMPS = makeStamps(10);\n    byte[][] VALUES = makeNAscii(org.apache.hadoop.hbase.client.TestFromClientSide.VALUE, 10);\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, FAMILIES, LIMITS);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0], VALUES[0]);\n    put.add(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    put.add(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0], VALUES[0]);\n    put.add(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    put.add(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    put.add(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3], VALUES[3]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0], VALUES[0]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3], VALUES[3]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    put.add(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6], VALUES[6]);\n    ht.put(put);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1] }, new byte[][]{ VALUES[1] }, 0, 0);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addFamily(FAMILIES[0]);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1] }, new byte[][]{ VALUES[1] }, 0, 0);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1] }, new byte[][]{ VALUES[1] }, 0, 0);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addFamily(FAMILIES[0]);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1] }, new byte[][]{ VALUES[1] }, 0, 0);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1], STAMPS[2], STAMPS[3] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3] }, 0, 2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addFamily(FAMILIES[1]);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1], STAMPS[2], STAMPS[3] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3] }, 0, 2);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1], STAMPS[2], STAMPS[3] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3] }, 0, 2);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addFamily(FAMILIES[1]);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[1], STAMPS[2], STAMPS[3] }, new byte[][]{ VALUES[1], VALUES[2], VALUES[3] }, 0, 2);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6] }, new byte[][]{ VALUES[2], VALUES[3], VALUES[4], VALUES[5], VALUES[6] }, 0, 4);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addFamily(FAMILIES[2]);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6] }, new byte[][]{ VALUES[2], VALUES[3], VALUES[4], VALUES[5], VALUES[6] }, 0, 4);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6] }, new byte[][]{ VALUES[2], VALUES[3], VALUES[4], VALUES[5], VALUES[6] }, 0, 4);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addFamily(FAMILIES[2]);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6] }, new byte[][]{ VALUES[2], VALUES[3], VALUES[4], VALUES[5], VALUES[6] }, 0, 4);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    org.junit.Assert.assertTrue(\"Expected 9 keys but received \" + result.size(), result.size() == 9);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addFamily(FAMILIES[0]);\n    get.addFamily(FAMILIES[1]);\n    get.addFamily(FAMILIES[2]);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    org.junit.Assert.assertTrue(\"Expected 9 keys but received \" + result.size(), result.size() == 9);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.addColumn(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.addColumn(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    org.junit.Assert.assertTrue(\"Expected 9 keys but received \" + result.size(), result.size() == 9);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    org.junit.Assert.assertTrue(\"Expected 9 keys but received \" + result.size(), result.size() == 9);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    scan.addFamily(FAMILIES[0]);\n    scan.addFamily(FAMILIES[1]);\n    scan.addFamily(FAMILIES[2]);\n    result = getSingleScanResult(ht, scan);\n    org.junit.Assert.assertTrue(\"Expected 9 keys but received \" + result.size(), result.size() == 9);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    scan.addColumn(FAMILIES[0], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.addColumn(FAMILIES[1], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.addColumn(FAMILIES[2], org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    result = getSingleScanResult(ht, scan);\n    org.junit.Assert.assertTrue(\"Expected 9 keys but received \" + result.size(), result.size() == 9);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row);\n    scan.addColumn(family, qualifier);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    scan.setTimeRange(stamps[start], stamps[end] + 1);\n    org.apache.hadoop.hbase.client.Result result = getSingleScanResult(ht, scan);\n    assertNResult(result, row, family, qualifier, stamps, values, start, end);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "scanVersionRangeAndVerify",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row);\n    scan.addColumn(family, qualifier);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    org.apache.hadoop.hbase.client.Result result = getSingleScanResult(ht, scan);\n    assertNResult(result, row, family, qualifier, stamps, values, start, end);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "scanAllVersionsAndVerify",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row);\n    scan.addColumn(family, qualifier);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    scan.setTimeRange(stamps[start + 1], java.lang.Long.MAX_VALUE);\n    org.apache.hadoop.hbase.client.Result result = getSingleScanResult(ht, scan);\n    assertNResult(result, row, family, qualifier, stamps, values, start + 1, end);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "scanVersionRangeAndVerifyGreaterThan",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(family, qualifier);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    get.setTimeRange(stamps[start], stamps[end] + 1);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertNResult(result, row, family, qualifier, stamps, values, start, end);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getVersionRangeAndVerify",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testVersionLimits",
        "NumberOfAsserts": 6,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 119,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper zkw = org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.createInstance(org.apache.hadoop.hbase.TestZooKeeper.conf, org.apache.hadoop.hbase.TestZooKeeper.class.getName());\n    zkw.registerListener(org.apache.hadoop.hbase.EmptyWatcher.instance);\n    zkw.ensureExists(\"/l1/l2/l3/l4\");\n    try {\n        zkw.deleteZNode(\"/l1/l2\");\n        org.junit.Assert.fail(\"We should not be able to delete if znode has childs\");\n    } catch (org.apache.zookeeper.KeeperException ex) {\n        org.junit.Assert.assertNotNull(zkw.getData(\"/l1/l2/l3\", \"l4\"));\n    }\n    zkw.deleteZNode(\"/l1/l2\", true);\n    org.junit.Assert.assertNull(zkw.getData(\"/l1/l2/l3\", \"l4\"));\n    zkw.deleteZNode(\"/l1\");\n    org.junit.Assert.assertNull(zkw.getData(\"/l1\", \"l2\"));\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper zkw = org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.createInstance(conf, org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.class.getName());\n    zkw.registerListener(org.apache.hadoop.hbase.EmptyWatcher.instance);\n    java.lang.String quorumServers = zkw.getQuorumServers();\n    int sessionTimeout = 5 * 1000;\n    byte[] password = nodeZK.getSessionPassword();\n    long sessionID = nodeZK.getSessionID();\n    org.apache.zookeeper.ZooKeeper zk = new org.apache.zookeeper.ZooKeeper(quorumServers, sessionTimeout, org.apache.hadoop.hbase.EmptyWatcher.instance, sessionID, password);\n    zk.close();\n    final long sleep = sessionTimeout * 5L;\n    LOG.info(\"ZK Closed; sleeping=\" + sleep);\n    java.lang.Thread.sleep(sleep);\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "expireSession",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 14,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    java.util.List<org.apache.hadoop.hbase.KeyValue> l1 = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    l1.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col5, data));\n    l1.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col1, data));\n    l1.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col2, data));\n    scanners.add(new org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.Scanner(l1));\n    java.util.List<org.apache.hadoop.hbase.KeyValue> l2 = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    l2.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col1, data));\n    l2.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col2, data));\n    scanners.add(new org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.Scanner(l2));\n    java.util.List<org.apache.hadoop.hbase.KeyValue> l3 = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col3, data));\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col4, data));\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row1, fam2, col1, data));\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row1, fam2, col2, data));\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col3, data));\n    scanners.add(new org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.Scanner(l3));\n    java.util.List<org.apache.hadoop.hbase.KeyValue> l4 = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    scanners.add(new org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.Scanner(l4));\n    org.apache.hadoop.hbase.regionserver.KeyValueHeap kvh = new org.apache.hadoop.hbase.regionserver.KeyValueHeap(scanners, org.apache.hadoop.hbase.KeyValue.COMPARATOR);\n    while (kvh.next() != null);\n    for (org.apache.hadoop.hbase.regionserver.KeyValueScanner scanner : scanners) {\n        junit.framework.Assert.assertTrue(((org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.Scanner) (scanner)).isClosed());\n    }\n}",
            "ClassName": "TestKeyValueHeap",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScannerLeak",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 25,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    java.util.List<org.apache.hadoop.hbase.KeyValue> l1 = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    l1.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col5, data));\n    l1.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col1, data));\n    l1.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col2, data));\n    scanners.add(new org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.Scanner(l1));\n    java.util.List<org.apache.hadoop.hbase.KeyValue> l2 = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    l2.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col1, data));\n    l2.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col2, data));\n    scanners.add(new org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.Scanner(l2));\n    java.util.List<org.apache.hadoop.hbase.KeyValue> l3 = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col3, data));\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col4, data));\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row1, fam2, col1, data));\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row1, fam2, col2, data));\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col3, data));\n    scanners.add(new org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.Scanner(l3));\n    java.util.List<org.apache.hadoop.hbase.KeyValue> expected = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    expected.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col1, data));\n    org.apache.hadoop.hbase.regionserver.KeyValueHeap kvh = new org.apache.hadoop.hbase.regionserver.KeyValueHeap(scanners, org.apache.hadoop.hbase.KeyValue.COMPARATOR);\n    org.apache.hadoop.hbase.KeyValue seekKv = new org.apache.hadoop.hbase.KeyValue(row2, fam1, null, null);\n    kvh.seek(seekKv);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> actual = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    actual.add(kvh.peek());\n    junit.framework.Assert.assertEquals(expected.size(), actual.size());\n    for (int i = 0; i < expected.size(); i++) {\n        junit.framework.Assert.assertEquals(expected.get(i), actual.get(i));\n        if (org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.PRINT) {\n            java.lang.System.out.println((((\"expected \" + expected.get(i)) + \"\\nactual   \") + actual.get(i)) + \"\\n\");\n        }\n    }\n}",
            "ClassName": "TestKeyValueHeap",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSeek",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 32,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    int end1 = s1 + l1;\n    int end2 = s2 + l2;\n    for (int i = s1, j = s2; (i < end1) && (j < end2); i++ , j++) {\n        int a = b1[i] & 0xff;\n        int b = b2[j] & 0xff;\n        if (a != b) {\n            return a - b;\n        }\n    }\n    return l1 - l2;\n}",
            "ClassName": "Bytes",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "compareTo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    java.util.List<org.apache.hadoop.hbase.KeyValue> l1 = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    l1.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col5, data));\n    l1.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col1, data));\n    l1.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col2, data));\n    scanners.add(new org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.Scanner(l1));\n    java.util.List<org.apache.hadoop.hbase.KeyValue> l2 = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    l2.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col1, data));\n    l2.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col2, data));\n    scanners.add(new org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.Scanner(l2));\n    java.util.List<org.apache.hadoop.hbase.KeyValue> l3 = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col3, data));\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col4, data));\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row1, fam2, col1, data));\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row1, fam2, col2, data));\n    l3.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col3, data));\n    scanners.add(new org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.Scanner(l3));\n    java.util.List<org.apache.hadoop.hbase.KeyValue> expected = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    expected.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col1, data));\n    expected.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col2, data));\n    expected.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col3, data));\n    expected.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col4, data));\n    expected.add(new org.apache.hadoop.hbase.KeyValue(row1, fam1, col5, data));\n    expected.add(new org.apache.hadoop.hbase.KeyValue(row1, fam2, col1, data));\n    expected.add(new org.apache.hadoop.hbase.KeyValue(row1, fam2, col2, data));\n    expected.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col1, data));\n    expected.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col2, data));\n    expected.add(new org.apache.hadoop.hbase.KeyValue(row2, fam1, col3, data));\n    org.apache.hadoop.hbase.regionserver.KeyValueHeap kvh = new org.apache.hadoop.hbase.regionserver.KeyValueHeap(scanners, org.apache.hadoop.hbase.KeyValue.COMPARATOR);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> actual = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    while (kvh.peek() != null) {\n        actual.add(kvh.next());\n    } \n    junit.framework.Assert.assertEquals(expected.size(), actual.size());\n    for (int i = 0; i < expected.size(); i++) {\n        junit.framework.Assert.assertEquals(expected.get(i), actual.get(i));\n        if (org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.PRINT) {\n            java.lang.System.out.println((((\"expected \" + expected.get(i)) + \"\\nactual   \") + actual.get(i)) + \"\\n\");\n        }\n    }\n    for (int i = 0; i < (actual.size() - 1); i++) {\n        int ret = org.apache.hadoop.hbase.KeyValue.COMPARATOR.compare(actual.get(i), actual.get(i + 1));\n        junit.framework.Assert.assertTrue(ret < 0);\n    }\n}",
            "ClassName": "TestKeyValueHeap",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSorted",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 45,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestZooKeeper",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testZNodeDeletes",
        "NumberOfAsserts": 4,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 15,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testJiraTest1182\");\n    byte[][] VALUES = makeNAscii(org.apache.hadoop.hbase.client.TestFromClientSide.VALUE, 7);\n    long[] STAMPS = makeStamps(7);\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, 10);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0], VALUES[0]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3], VALUES[3]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    ht.put(put);\n    getVersionRangeAndVerifyGreaterThan(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 5);\n    getVersionRangeAndVerifyGreaterThan(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 2, 5);\n    getVersionRangeAndVerifyGreaterThan(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 4, 5);\n    scanVersionRangeAndVerifyGreaterThan(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 5);\n    scanVersionRangeAndVerifyGreaterThan(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 2, 5);\n    scanVersionRangeAndVerifyGreaterThan(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 4, 5);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    getVersionRangeAndVerifyGreaterThan(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 5);\n    getVersionRangeAndVerifyGreaterThan(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 2, 5);\n    getVersionRangeAndVerifyGreaterThan(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 4, 5);\n    scanVersionRangeAndVerifyGreaterThan(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 5);\n    scanVersionRangeAndVerifyGreaterThan(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 2, 5);\n    scanVersionRangeAndVerifyGreaterThan(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 4, 5);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration());\n    admin.split(t.getTableName());\n    java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> regions = waitOnSplit(t);\n    org.junit.Assert.assertTrue(regions.size() > 1);\n    return regions;\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "splitTable",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(family, qualifier);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    get.setTimeRange(stamps[start], stamps[end] + 1);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertNResult(result, row, family, qualifier, stamps, values, start, end);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getVersionRangeAndVerify",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testJiraTest1182",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 27,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testDuplicateVersions\");\n    long[] STAMPS = makeStamps(20);\n    byte[][] VALUES = makeNAscii(org.apache.hadoop.hbase.client.TestFromClientSide.VALUE, 20);\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, 10);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    ht.put(put);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6]);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(2);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[4], STAMPS[5] }, new byte[][]{ VALUES[4], VALUES[5] }, 0, 1);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(2);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[4], STAMPS[5] }, new byte[][]{ VALUES[4], VALUES[5] }, 0, 1);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6]);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(2);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[4], STAMPS[5] }, new byte[][]{ VALUES[4], VALUES[5] }, 0, 1);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(2);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[4], STAMPS[5] }, new byte[][]{ VALUES[4], VALUES[5] }, 0, 1);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3], VALUES[3]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[14]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6], VALUES[6]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[7], VALUES[7]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[8], VALUES[8]);\n    ht.put(put);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(7);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[7], STAMPS[8] }, new byte[][]{ VALUES[2], VALUES[3], VALUES[14], VALUES[5], VALUES[6], VALUES[7], VALUES[8] }, 0, 6);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(7);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[7], STAMPS[8] }, new byte[][]{ VALUES[2], VALUES[3], VALUES[14], VALUES[5], VALUES[6], VALUES[7], VALUES[8] }, 0, 6);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.setMaxVersions(7);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[7], STAMPS[8] }, new byte[][]{ VALUES[2], VALUES[3], VALUES[14], VALUES[5], VALUES[6], VALUES[7], VALUES[8] }, 0, 6);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.setMaxVersions(7);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[2], STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[7], STAMPS[8] }, new byte[][]{ VALUES[2], VALUES[3], VALUES[14], VALUES[5], VALUES[6], VALUES[7], VALUES[8] }, 0, 6);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[14]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[7], VALUES[7]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[14]);\n    scanVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[7], VALUES[7]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[9]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0]);\n    scanVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[9]);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[9], VALUES[9]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[11], VALUES[11]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[13], VALUES[13]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[15], VALUES[15]);\n    ht.put(put);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[7], STAMPS[8], STAMPS[9], STAMPS[11], STAMPS[13], STAMPS[15] }, new byte[][]{ VALUES[3], VALUES[14], VALUES[5], VALUES[6], VALUES[7], VALUES[8], VALUES[9], VALUES[11], VALUES[13], VALUES[15] }, 0, 9);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[7], STAMPS[8], STAMPS[9], STAMPS[11], STAMPS[13], STAMPS[15] }, new byte[][]{ VALUES[3], VALUES[14], VALUES[5], VALUES[6], VALUES[7], VALUES[8], VALUES[9], VALUES[11], VALUES[13], VALUES[15] }, 0, 9);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    delete.deleteColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[11]);\n    delete.deleteColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[7]);\n    ht.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = ht.get(get);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[8], STAMPS[9], STAMPS[13], STAMPS[15] }, new byte[][]{ VALUES[3], VALUES[14], VALUES[5], VALUES[6], VALUES[8], VALUES[9], VALUES[13], VALUES[15] }, 0, 7);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, new long[]{ STAMPS[3], STAMPS[4], STAMPS[5], STAMPS[6], STAMPS[8], STAMPS[9], STAMPS[13], STAMPS[15] }, new byte[][]{ VALUES[3], VALUES[14], VALUES[5], VALUES[6], VALUES[8], VALUES[9], VALUES[13], VALUES[15] }, 0, 7);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(family, qualifier);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    get.setTimeRange(stamps[start], stamps[end] + 1);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertNResult(result, row, family, qualifier, stamps, values, start, end);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getVersionRangeAndVerify",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row);\n    scan.addColumn(family, qualifier);\n    scan.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    scan.setTimeRange(stamps[start], stamps[end] + 1);\n    org.apache.hadoop.hbase.client.Result result = getSingleScanResult(ht, scan);\n    assertNResult(result, row, family, qualifier, stamps, values, start, end);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "scanVersionRangeAndVerify",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testDuplicateVersions",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 129,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\");\n    org.apache.hadoop.hbase.HColumnDescriptor hcd = new org.apache.hadoop.hbase.HColumnDescriptor(family);\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.regionserver.TestStoreReconstruction.TABLE);\n    htd.addFamily(hcd);\n    org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(htd, null, null, false);\n    org.apache.hadoop.fs.Path oldLogDir = new org.apache.hadoop.fs.Path(this.dir, org.apache.hadoop.hbase.HConstants.HREGION_OLDLOGDIR_NAME);\n    org.apache.hadoop.fs.Path logDir = new org.apache.hadoop.fs.Path(this.dir, org.apache.hadoop.hbase.HConstants.HREGION_LOGDIR_NAME);\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(cluster.getFileSystem(), logDir, oldLogDir, conf, null);\n    org.apache.hadoop.hbase.regionserver.HRegion region = new org.apache.hadoop.hbase.regionserver.HRegion(dir, log, cluster.getFileSystem(), conf, info, null);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> result = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    java.util.NavigableSet<byte[]> qualifiers = new java.util.concurrent.ConcurrentSkipListSet<byte[]>(org.apache.hadoop.hbase.util.Bytes.BYTES_COMPARATOR);\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.regionserver.TestStoreReconstruction.TABLE);\n    final byte[] rowName = tableName;\n    final byte[] regionName = info.getRegionName();\n    for (int j = 0; j < org.apache.hadoop.hbase.regionserver.TestStoreReconstruction.TOTAL_EDITS; j++) {\n        byte[] qualifier = org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(j));\n        byte[] column = org.apache.hadoop.hbase.util.Bytes.toBytes(\"column:\" + java.lang.Integer.toString(j));\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit edit = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        edit.add(new org.apache.hadoop.hbase.KeyValue(rowName, family, qualifier, java.lang.System.currentTimeMillis(), column));\n        log.append(info, tableName, edit, java.lang.System.currentTimeMillis());\n    }\n    long logSeqId = log.startCacheFlush();\n    log.completeCacheFlush(regionName, tableName, logSeqId, info.isMetaRegion());\n    org.apache.hadoop.hbase.regionserver.wal.WALEdit edit = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n    edit.add(new org.apache.hadoop.hbase.KeyValue(rowName, org.apache.hadoop.hbase.util.Bytes.toBytes(\"another family\"), rowName, java.lang.System.currentTimeMillis(), rowName));\n    log.append(info, tableName, edit, java.lang.System.currentTimeMillis());\n    log.sync();\n    log.close();\n    java.util.List<org.apache.hadoop.fs.Path> splits = org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(new org.apache.hadoop.fs.Path(conf.get(org.apache.hadoop.hbase.HConstants.HBASE_DIR)), logDir, oldLogDir, cluster.getFileSystem(), conf);\n    org.junit.Assert.assertEquals(1, splits.size());\n    org.junit.Assert.assertTrue(cluster.getFileSystem().exists(splits.get(0)));\n    org.apache.hadoop.hbase.regionserver.Store store = new org.apache.hadoop.hbase.regionserver.Store(dir, region, hcd, cluster.getFileSystem(), splits.get(0), conf, null);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(rowName);\n    store.get(get, qualifiers, result);\n    org.junit.Assert.assertEquals(org.apache.hadoop.hbase.regionserver.TestStoreReconstruction.TOTAL_EDITS, result.size());\n}",
        "CUT_1": {
            "Body": "{\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(getName());\n    final byte[] rowName = tableName;\n    org.apache.hadoop.fs.Path logdir = new org.apache.hadoop.fs.Path(this.dir, org.apache.hadoop.hbase.HConstants.HREGION_LOGDIR_NAME);\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(this.fs, logdir, this.oldLogDir, this.conf, null);\n    final int howmany = 3;\n    org.apache.hadoop.hbase.HRegionInfo[] infos = new org.apache.hadoop.hbase.HRegionInfo[3];\n    for (int i = 0; i < howmany; i++) {\n        infos[i] = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(tableName), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\" + i), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\" + (i + 1)), false);\n    }\n    try {\n        for (int ii = 0; ii < howmany; ii++) {\n            for (int i = 0; i < howmany; i++) {\n                for (int j = 0; j < howmany; j++) {\n                    org.apache.hadoop.hbase.regionserver.wal.WALEdit edit = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n                    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\");\n                    byte[] qualifier = org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(j));\n                    byte[] column = org.apache.hadoop.hbase.util.Bytes.toBytes(\"column:\" + java.lang.Integer.toString(j));\n                    edit.add(new org.apache.hadoop.hbase.KeyValue(rowName, family, qualifier, java.lang.System.currentTimeMillis(), column));\n                    org.apache.hadoop.hbase.regionserver.wal.TestHLog.LOG.info(((\"Region \" + i) + \": \") + edit);\n                    log.append(infos[i], tableName, edit, java.lang.System.currentTimeMillis());\n                }\n            }\n            log.hflush();\n            log.rollWriter();\n        }\n        org.apache.hadoop.fs.Path splitsdir = new org.apache.hadoop.fs.Path(this.dir, \"splits\");\n        java.util.List<org.apache.hadoop.fs.Path> splits = org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(splitsdir, logdir, this.oldLogDir, this.fs, this.conf);\n        verifySplits(splits, howmany);\n        log = null;\n    } finally {\n        if (log != null) {\n            log.closeAndDelete();\n        }\n    }\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSplit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 36,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.fs.Path basedir = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.TestStore.DIR + methodName);\n    org.apache.hadoop.fs.Path logdir = new org.apache.hadoop.fs.Path((org.apache.hadoop.hbase.regionserver.TestStore.DIR + methodName) + \"/logs\");\n    org.apache.hadoop.fs.Path oldLogDir = new org.apache.hadoop.fs.Path(basedir, org.apache.hadoop.hbase.HConstants.HREGION_OLDLOGDIR_NAME);\n    org.apache.hadoop.hbase.HColumnDescriptor hcd = new org.apache.hadoop.hbase.HColumnDescriptor(family);\n    org.apache.hadoop.hbase.HBaseConfiguration conf = new org.apache.hadoop.hbase.HBaseConfiguration();\n    org.apache.hadoop.fs.FileSystem fs = org.apache.hadoop.fs.FileSystem.get(conf);\n    org.apache.hadoop.fs.Path reconstructionLog = null;\n    org.apache.hadoop.util.Progressable reporter = null;\n    fs.delete(logdir, true);\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(table);\n    htd.addFamily(hcd);\n    org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(htd, null, null, false);\n    org.apache.hadoop.hbase.regionserver.wal.HLog hlog = new org.apache.hadoop.hbase.regionserver.wal.HLog(fs, logdir, oldLogDir, conf, null);\n    org.apache.hadoop.hbase.regionserver.HRegion region = new org.apache.hadoop.hbase.regionserver.HRegion(basedir, hlog, fs, conf, info, null);\n    store = new org.apache.hadoop.hbase.regionserver.Store(basedir, region, hcd, fs, reconstructionLog, conf, reporter);\n}",
            "ClassName": "TestStore",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "init",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 17,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    final int COL_COUNT = 10;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"tablename\");\n    final byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\");\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = null;\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(fs, dir, this.oldLogDir, this.conf, null);\n    try {\n        long timestamp = java.lang.System.currentTimeMillis();\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit cols = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        for (int i = 0; i < COL_COUNT; i++) {\n            cols.add(new org.apache.hadoop.hbase.KeyValue(row, org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\"), org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(i)), timestamp, new byte[]{ ((byte) (i + '0')) }));\n        }\n        org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(tableName), row, org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.util.Bytes.toString(row) + \"1\"), false);\n        final byte[] regionName = info.getRegionName();\n        log.append(info, tableName, cols, java.lang.System.currentTimeMillis());\n        long logSeqId = log.startCacheFlush();\n        log.completeCacheFlush(regionName, tableName, logSeqId, info.isMetaRegion());\n        log.close();\n        org.apache.hadoop.fs.Path filename = log.computeFilename();\n        log = null;\n        reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, filename, conf);\n        for (int i = 0; i < 1; i++) {\n            org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = reader.next(null);\n            if (entry == null)\n                break;\n\n            org.apache.hadoop.hbase.regionserver.wal.HLogKey key = entry.getKey();\n            org.apache.hadoop.hbase.regionserver.wal.WALEdit val = entry.getEdit();\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(regionName, key.getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, key.getTablename()));\n            org.apache.hadoop.hbase.KeyValue kv = val.getKeyValues().get(0);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(row, kv.getRow()));\n            junit.framework.Assert.assertEquals(((byte) (i + '0')), kv.getValue()[0]);\n            java.lang.System.out.println((key + \" \") + val);\n        }\n        org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = null;\n        while ((entry = reader.next(null)) != null) {\n            org.apache.hadoop.hbase.regionserver.wal.HLogKey key = entry.getKey();\n            org.apache.hadoop.hbase.regionserver.wal.WALEdit val = entry.getEdit();\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(regionName, key.getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, key.getTablename()));\n            org.apache.hadoop.hbase.KeyValue kv = val.getKeyValues().get(0);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, kv.getRow()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, kv.getFamily()));\n            junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH, val.getKeyValues().get(0).getValue()));\n            java.lang.System.out.println((key + \" \") + val);\n        } \n    } finally {\n        if (log != null) {\n            log.closeAndDelete();\n        }\n        if (reader != null) {\n            reader.close();\n        }\n    }\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 6,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testEditAdd",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 56,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    for (byte[] family : families) {\n        htd.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(family));\n    }\n    org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(htd, null, null, false);\n    org.apache.hadoop.fs.Path path = new org.apache.hadoop.fs.Path(DIR + callingMethod);\n    region = org.apache.hadoop.hbase.regionserver.HRegion.createHRegion(info, path, conf);\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "initHRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    final int COL_COUNT = 10;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"tablename\");\n    final byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\");\n    this.conf.setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = null;\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(this.fs, dir, this.oldLogDir, this.conf, null);\n    try {\n        long timestamp = java.lang.System.currentTimeMillis();\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit cols = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        for (int i = 0; i < COL_COUNT; i++) {\n            cols.add(new org.apache.hadoop.hbase.KeyValue(row, org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\"), org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(i)), timestamp, new byte[]{ ((byte) (i + '0')) }));\n        }\n        org.apache.hadoop.hbase.HRegionInfo hri = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(tableName), org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.HConstants.EMPTY_END_ROW);\n        log.append(hri, tableName, cols, java.lang.System.currentTimeMillis());\n        long logSeqId = log.startCacheFlush();\n        log.completeCacheFlush(hri.getRegionName(), tableName, logSeqId, false);\n        log.close();\n        org.apache.hadoop.fs.Path filename = log.computeFilename();\n        log = null;\n        reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, filename, conf);\n        org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = reader.next();\n        junit.framework.Assert.assertEquals(COL_COUNT, entry.getEdit().size());\n        int idx = 0;\n        for (org.apache.hadoop.hbase.KeyValue val : entry.getEdit().getKeyValues()) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getRegionName(), entry.getKey().getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, entry.getKey().getTablename()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(row, val.getRow()));\n            junit.framework.Assert.assertEquals(((byte) (idx + '0')), val.getValue()[0]);\n            java.lang.System.out.println((entry.getKey() + \" \") + val);\n            idx++;\n        }\n        entry = reader.next();\n        junit.framework.Assert.assertEquals(1, entry.getEdit().size());\n        for (org.apache.hadoop.hbase.KeyValue val : entry.getEdit().getKeyValues()) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getRegionName(), entry.getKey().getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, entry.getKey().getTablename()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, val.getRow()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, val.getFamily()));\n            junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH, val.getValue()));\n            java.lang.System.out.println((entry.getKey() + \" \") + val);\n        }\n    } finally {\n        if (log != null) {\n            log.closeAndDelete();\n        }\n        if (reader != null) {\n            reader.close();\n        }\n    }\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testAppend",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 51,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestStoreReconstruction",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 1,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "runReconstructionLog",
        "NumberOfAsserts": 3,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 37,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 2,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.ipc.HBaseServer.Call call = org.apache.hadoop.hbase.ipc.HBaseServer.CurCall.get();\n    if (call != null) {\n        return call.connection.socket.getInetAddress();\n    }\n    return null;\n}",
            "ClassName": "HBaseServer",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getRemoteIp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    this.writer.append(entry.getKey(), entry.getEdit());\n}",
            "ClassName": "SequenceFileLogWriter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "append",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return cacheBlocks;\n}",
            "ClassName": "Scan",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getCacheBlocks",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.HRegion r = org.apache.hadoop.hbase.regionserver.HRegion.newHRegion(org.apache.hadoop.hbase.HTableDescriptor.getTableDir(rootDir, regionInfo.getTableDesc().getName()), this.hlog, this.fs, conf, regionInfo, this.cacheFlusher);\n    r.initialize(null, new org.apache.hadoop.util.Progressable() {\n        public void progress() {\n            addProcessingMessage(regionInfo);\n        }\n    });\n    return r;\n}",
            "ClassName": "HRegionServer",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "instantiateRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    final org.apache.hadoop.hbase.HRegionInfo oldRegionInfo = region.getRegionInfo();\n    final long startTime = java.lang.System.currentTimeMillis();\n    final org.apache.hadoop.hbase.regionserver.HRegion[] newRegions = region.splitRegion(midKey);\n    if (newRegions == null) {\n        return;\n    }\n    org.apache.hadoop.hbase.client.HTable t = null;\n    if (region.getRegionInfo().isMetaTable()) {\n        if (this.root == null) {\n            this.root = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.ROOT_TABLE_NAME);\n        }\n        t = root;\n    } else {\n        if (meta == null) {\n            meta = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n        }\n        t = meta;\n    }\n    oldRegionInfo.setOffline(true);\n    oldRegionInfo.setSplit(true);\n    this.server.removeFromOnlineRegions(oldRegionInfo);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(oldRegionInfo.getRegionName());\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(oldRegionInfo));\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITA_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(newRegions[0].getRegionInfo()));\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITB_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(newRegions[1].getRegionInfo()));\n    t.put(put);\n    for (int i = 0; i < newRegions.length; i++) {\n        put = new org.apache.hadoop.hbase.client.Put(newRegions[i].getRegionName());\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(newRegions[i].getRegionInfo()));\n        t.put(put);\n    }\n    server.reportSplit(oldRegionInfo, newRegions[0].getRegionInfo(), newRegions[1].getRegionInfo());\n    org.apache.hadoop.hbase.regionserver.CompactSplitThread.LOG.info((((((((\"region split, META updated, and report to master all\" + \" successful. Old region=\") + oldRegionInfo.toString()) + \", new regions: \") + newRegions[0].toString()) + \", \") + newRegions[1].toString()) + \". Split took \") + org.apache.hadoop.util.StringUtils.formatTimeDiff(java.lang.System.currentTimeMillis(), startTime));\n}",
            "ClassName": "CompactSplitThread",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "split",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 37,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testMillions",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 2,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.LOG.info(\"Running testCloseRegion\");\n    org.apache.hadoop.hbase.MiniHBaseCluster cluster = org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.TEST_UTIL.getHBaseCluster();\n    org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.LOG.info(\"Number of region servers = \" + cluster.getLiveRegionServerThreads().size());\n    int rsIdx = 0;\n    org.apache.hadoop.hbase.regionserver.HRegionServer regionServer = org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.TEST_UTIL.getHBaseCluster().getRegionServer(rsIdx);\n    java.util.Collection<org.apache.hadoop.hbase.regionserver.HRegion> regions = regionServer.getOnlineRegions();\n    org.apache.hadoop.hbase.regionserver.HRegion region = regions.iterator().next();\n    org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.LOG.debug(\"Asking RS to close region \" + region.getRegionNameAsString());\n    java.util.concurrent.atomic.AtomicBoolean closeEventProcessed = new java.util.concurrent.atomic.AtomicBoolean(false);\n    org.apache.hadoop.hbase.master.RegionServerOperationListener listener = new org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.CloseRegionEventListener(region.getRegionNameAsString(), closeEventProcessed);\n    org.apache.hadoop.hbase.master.HMaster master = org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.TEST_UTIL.getHBaseCluster().getMaster();\n    master.getRegionServerOperationQueue().registerRegionServerOperationListener(listener);\n    org.apache.hadoop.hbase.HMsg closeRegionMsg = new org.apache.hadoop.hbase.HMsg(org.apache.hadoop.hbase.HMsg.Type.MSG_REGION_CLOSE, region.getRegionInfo(), org.apache.hadoop.hbase.util.Bytes.toBytes(\"Forcing close in test\"));\n    org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.TEST_UTIL.getHBaseCluster().addMessageToSendRegionServer(rsIdx, closeRegionMsg);\n    synchronized(closeEventProcessed) {\n        closeEventProcessed.wait((3 * 60) * 1000);\n    }\n    if (!closeEventProcessed.get()) {\n        throw new java.lang.Exception(\"Timed out, close event not called on master.\");\n    } else {\n        org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.LOG.info(\"Done with test, RS informed master successfully.\");\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.conf.Configuration c = org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.TEST_UTIL.getConfiguration();\n    c.setBoolean(\"dfs.support.append\", true);\n    c.setInt(\"hbase.regionserver.info.port\", 0);\n    c.setInt(\"hbase.master.meta.thread.rescanfrequency\", 5 * 1000);\n    org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.TEST_UTIL.startMiniCluster(2);\n    org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.TEST_UTIL.createTable(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.TABLENAME), org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.FAMILIES);\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.TABLENAME);\n    int countOfRegions = org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.TEST_UTIL.createMultiRegions(t, org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.getTestFamily());\n    org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.waitUntilAllRegionsAssigned(countOfRegions);\n    org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.addToEachStartKey(countOfRegions);\n}",
            "ClassName": "TestZKBasedCloseRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "beforeAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    if (org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.TEST_UTIL.getHBaseCluster().getLiveRegionServerThreads().size() < 2) {\n        org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.LOG.info(\"Started new server=\" + org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.TEST_UTIL.getHBaseCluster().startRegionServer());\n    }\n}",
            "ClassName": "TestZKBasedCloseRegion",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setup",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.beforeAllTests();\n    org.apache.hadoop.hbase.master.TestZKBasedCloseRegion test = new org.apache.hadoop.hbase.master.TestZKBasedCloseRegion();\n    test.setup();\n    test.testCloseRegion();\n    org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.afterAllTests();\n}",
            "ClassName": "TestZKBasedCloseRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "main",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.FAMILIES[0];\n}",
            "ClassName": "TestZKBasedCloseRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTestFamily",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestZKBasedCloseRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "afterAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestZKBasedCloseRegion",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 1,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 1,
        "Label": 0,
        "MethodName": "testCloseRegion",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 1,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 24,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 9,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    final byte[] CONTENTS_FAMILY = org.apache.hadoop.hbase.util.Bytes.toBytes(\"contents\");\n    final byte[] SMALL_FAMILY = org.apache.hadoop.hbase.util.Bytes.toBytes(\"smallfam\");\n    final int NB_BATCH_ROWS = 10;\n    final byte[] value = org.apache.hadoop.hbase.util.Bytes.toBytes(\"abcd\");\n    org.apache.hadoop.hbase.client.HTable table = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowsPut\"), new byte[][]{ CONTENTS_FAMILY, SMALL_FAMILY });\n    java.util.ArrayList<org.apache.hadoop.hbase.client.Put> rowsUpdate = new java.util.ArrayList<org.apache.hadoop.hbase.client.Put>();\n    for (int i = 0; i < NB_BATCH_ROWS; i++) {\n        byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + i);\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n        put.add(CONTENTS_FAMILY, null, value);\n        rowsUpdate.add(put);\n    }\n    table.put(rowsUpdate);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(CONTENTS_FAMILY);\n    org.apache.hadoop.hbase.client.ResultScanner scanner = table.getScanner(scan);\n    int nbRows = 0;\n    for (@java.lang.SuppressWarnings(\"unused\")\n    org.apache.hadoop.hbase.client.Result row : scanner)\n        nbRows++;\n\n    org.junit.Assert.assertEquals(NB_BATCH_ROWS, nbRows);\n}",
        "CUT_1": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"test_table\");\n    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testFamily\");\n    initHRegion(tableName, getName(), family);\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row111\");\n    byte[] row2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row222\");\n    byte[] row3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row333\");\n    byte[] row4 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row444\");\n    byte[] row5 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row555\");\n    byte[] col1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub111\");\n    byte[] col2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub222\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(10L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row2);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(15L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row3);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(20L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row4);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(30L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row5);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(40L));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row3, row4);\n    scan.setMaxVersions();\n    scan.addColumn(family, col1);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    junit.framework.Assert.assertEquals(false, s.next(results));\n    junit.framework.Assert.assertEquals(0, results.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_StopRow1542",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 34,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.TestZooKeeper.conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"test_table\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"columnA\");\n    byte[] fam2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"columnB\");\n    initHRegion(tableName, getName(), fam1, fam2);\n    byte[] rowA = org.apache.hadoop.hbase.util.Bytes.toBytes(\"rowA\");\n    byte[] rowB = org.apache.hadoop.hbase.util.Bytes.toBytes(\"rowB\");\n    byte[] value = org.apache.hadoop.hbase.util.Bytes.toBytes(\"value\");\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(rowA);\n    delete.deleteFamily(fam1);\n    region.delete(delete, null, true);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(rowA);\n    put.add(fam2, null, value);\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(rowB);\n    put.add(fam1, null, value);\n    put.add(fam2, null, value);\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(fam1).addFamily(fam2);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    s.next(results);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(rowA, results.get(0).getRow()));\n    results.clear();\n    s.next(results);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(rowB, results.get(0).getRow()));\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_DeleteOneFamilyNotAnother",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 28,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    for (int i = 0; i < numRows; i++) {\n        java.lang.String row = (key + \"_\") + i;\n        java.lang.System.out.println(java.lang.String.format(\"Saving row: %s, with value %s\", row, value));\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(row));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-blob\"), null, org.apache.hadoop.hbase.util.Bytes.toBytes(\"value for blob\"));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-type\"), null, org.apache.hadoop.hbase.util.Bytes.toBytes(\"statement\"));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-date\"), null, org.apache.hadoop.hbase.util.Bytes.toBytes(\"20090921010101999\"));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-tags\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"qual2\"), org.apache.hadoop.hbase.util.Bytes.toBytes(value));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-group\"), null, org.apache.hadoop.hbase.util.Bytes.toBytes(\"adhocTransactionGroupId\"));\n        r.put(put);\n    }\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "putRows",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 13,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 2,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testRowsPut",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 24,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testWeirdCacheBehaviour\");\n    byte[][] FAMILIES = new byte[][]{ org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-blob\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-type\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-date\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-tags\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-group\") };\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, FAMILIES);\n    java.lang.String value = \"this is the value\";\n    java.lang.String value2 = \"this is some other value\";\n    java.lang.String keyPrefix1 = java.util.UUID.randomUUID().toString();\n    java.lang.String keyPrefix2 = java.util.UUID.randomUUID().toString();\n    java.lang.String keyPrefix3 = java.util.UUID.randomUUID().toString();\n    putRows(ht, 3, value, keyPrefix1);\n    putRows(ht, 3, value, keyPrefix2);\n    putRows(ht, 3, value, keyPrefix3);\n    ht.flushCommits();\n    putRows(ht, 3, value2, keyPrefix1);\n    putRows(ht, 3, value2, keyPrefix2);\n    putRows(ht, 3, value2, keyPrefix3);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testWeirdCacheBehaviour\"));\n    java.lang.System.out.println(\"Checking values for key: \" + keyPrefix1);\n    org.junit.Assert.assertEquals(\"Got back incorrect number of rows from scan\", 3, getNumberOfRows(keyPrefix1, value2, table));\n    java.lang.System.out.println(\"Checking values for key: \" + keyPrefix2);\n    org.junit.Assert.assertEquals(\"Got back incorrect number of rows from scan\", 3, getNumberOfRows(keyPrefix2, value2, table));\n    java.lang.System.out.println(\"Checking values for key: \" + keyPrefix3);\n    org.junit.Assert.assertEquals(\"Got back incorrect number of rows from scan\", 3, getNumberOfRows(keyPrefix3, value2, table));\n    deleteColumns(ht, value2, keyPrefix1);\n    deleteColumns(ht, value2, keyPrefix2);\n    deleteColumns(ht, value2, keyPrefix3);\n    java.lang.System.out.println(\"Starting important checks.....\");\n    org.junit.Assert.assertEquals(\"Got back incorrect number of rows from scan: \" + keyPrefix1, 0, getNumberOfRows(keyPrefix1, value2, table));\n    org.junit.Assert.assertEquals(\"Got back incorrect number of rows from scan: \" + keyPrefix2, 0, getNumberOfRows(keyPrefix2, value2, table));\n    org.junit.Assert.assertEquals(\"Got back incorrect number of rows from scan: \" + keyPrefix3, 0, getNumberOfRows(keyPrefix3, value2, table));\n    ht.setScannerCaching(0);\n    org.junit.Assert.assertEquals(\"Got back incorrect number of rows from scan\", 0, getNumberOfRows(keyPrefix1, value2, table));\n    ht.setScannerCaching(100);\n    org.junit.Assert.assertEquals(\"Got back incorrect number of rows from scan\", 0, getNumberOfRows(keyPrefix2, value2, table));\n}",
        "CUT_1": {
            "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testWeirdCacheBehaviour\");\n    byte[][] FAMILIES = new byte[][]{ org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-blob\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-type\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-date\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-tags\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-group\") };\n    initHRegion(TABLE, getName(), FAMILIES);\n    java.lang.String value = \"this is the value\";\n    java.lang.String value2 = \"this is some other value\";\n    java.lang.String keyPrefix1 = \"prefix1\";\n    java.lang.String keyPrefix2 = \"prefix2\";\n    java.lang.String keyPrefix3 = \"prefix3\";\n    putRows(this.region, 3, value, keyPrefix1);\n    putRows(this.region, 3, value, keyPrefix2);\n    putRows(this.region, 3, value, keyPrefix3);\n    putRows(this.region, 3, value2, keyPrefix1);\n    putRows(this.region, 3, value2, keyPrefix2);\n    putRows(this.region, 3, value2, keyPrefix3);\n    java.lang.System.out.println(\"Checking values for key: \" + keyPrefix1);\n    junit.framework.Assert.assertEquals(\"Got back incorrect number of rows from scan\", 3, getNumberOfRows(keyPrefix1, value2, this.region));\n    java.lang.System.out.println(\"Checking values for key: \" + keyPrefix2);\n    junit.framework.Assert.assertEquals(\"Got back incorrect number of rows from scan\", 3, getNumberOfRows(keyPrefix2, value2, this.region));\n    java.lang.System.out.println(\"Checking values for key: \" + keyPrefix3);\n    junit.framework.Assert.assertEquals(\"Got back incorrect number of rows from scan\", 3, getNumberOfRows(keyPrefix3, value2, this.region));\n    deleteColumns(this.region, value2, keyPrefix1);\n    deleteColumns(this.region, value2, keyPrefix2);\n    deleteColumns(this.region, value2, keyPrefix3);\n    java.lang.System.out.println(\"Starting important checks.....\");\n    junit.framework.Assert.assertEquals(\"Got back incorrect number of rows from scan: \" + keyPrefix1, 0, getNumberOfRows(keyPrefix1, value2, this.region));\n    junit.framework.Assert.assertEquals(\"Got back incorrect number of rows from scan: \" + keyPrefix2, 0, getNumberOfRows(keyPrefix2, value2, this.region));\n    junit.framework.Assert.assertEquals(\"Got back incorrect number of rows from scan: \" + keyPrefix3, 0, getNumberOfRows(keyPrefix3, value2, this.region));\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testWeirdCacheBehaviour",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 29,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    for (int i = 0; i < numRows; i++) {\n        java.lang.String row = (key + \"_\") + java.util.UUID.randomUUID().toString();\n        java.lang.System.out.println(java.lang.String.format(\"Saving row: %s, with value %s\", row, value));\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(row));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-blob\"), null, org.apache.hadoop.hbase.util.Bytes.toBytes(\"value for blob\"));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-type\"), null, org.apache.hadoop.hbase.util.Bytes.toBytes(\"statement\"));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-date\"), null, org.apache.hadoop.hbase.util.Bytes.toBytes(\"20090921010101999\"));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-tags\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"qual2\"), org.apache.hadoop.hbase.util.Bytes.toBytes(value));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-group\"), null, org.apache.hadoop.hbase.util.Bytes.toBytes(\"adhocTransactionGroupId\"));\n        ht.put(put);\n    }\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "putRows",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 13,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.filter.FilterList allFilters = new org.apache.hadoop.hbase.filter.FilterList();\n    allFilters.addFilter(new org.apache.hadoop.hbase.filter.PrefixFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(keyPrefix)));\n    org.apache.hadoop.hbase.filter.SingleColumnValueFilter filter = new org.apache.hadoop.hbase.filter.SingleColumnValueFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-tags\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"qual2\"), org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, org.apache.hadoop.hbase.util.Bytes.toBytes(value));\n    filter.setFilterIfMissing(true);\n    allFilters.addFilter(filter);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-blob\"));\n    scan.addFamily(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-type\"));\n    scan.addFamily(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-date\"));\n    scan.addFamily(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-tags\"));\n    scan.addFamily(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-group\"));\n    scan.setFilter(allFilters);\n    return ht.getScanner(scan);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "buildScanner",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 15,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    for (int i = 0; i < numRows; i++) {\n        java.lang.String row = (key + \"_\") + i;\n        java.lang.System.out.println(java.lang.String.format(\"Saving row: %s, with value %s\", row, value));\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(row));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-blob\"), null, org.apache.hadoop.hbase.util.Bytes.toBytes(\"value for blob\"));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-type\"), null, org.apache.hadoop.hbase.util.Bytes.toBytes(\"statement\"));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-date\"), null, org.apache.hadoop.hbase.util.Bytes.toBytes(\"20090921010101999\"));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-tags\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"qual2\"), org.apache.hadoop.hbase.util.Bytes.toBytes(value));\n        put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"trans-group\"), null, org.apache.hadoop.hbase.util.Bytes.toBytes(\"adhocTransactionGroupId\"));\n        r.put(put);\n    }\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "putRows",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 13,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(conf, TABLE_NAME);\n    for (int i = 0; i < ROWS.length; i++) {\n        for (int j = 0; j < TIMESTAMPS.length; j++) {\n            org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(ROWS[i]);\n            get.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n            get.setTimeStamp(TIMESTAMPS[j]);\n            org.apache.hadoop.hbase.client.Result result = t.get(get);\n            int cellCount = 0;\n            for (@java.lang.SuppressWarnings(\"unused\")\n            org.apache.hadoop.hbase.KeyValue kv : result.sorted()) {\n                cellCount++;\n            }\n            junit.framework.Assert.assertTrue(cellCount == 1);\n        }\n    }\n    int count = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.apache.hadoop.hbase.client.ResultScanner s = t.getScanner(scan);\n    try {\n        for (org.apache.hadoop.hbase.client.Result rr = null; (rr = s.next()) != null;) {\n            java.lang.System.out.println(rr.toString());\n            count += 1;\n        }\n        junit.framework.Assert.assertEquals(\"Number of rows should be 2\", 2, count);\n    } finally {\n        s.close();\n    }\n    count = 0;\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.setTimeRange(1000L, java.lang.Long.MAX_VALUE);\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    s = t.getScanner(scan);\n    try {\n        while (s.next() != null) {\n            count += 1;\n        } \n        junit.framework.Assert.assertEquals(\"Number of rows should be 2\", 2, count);\n    } finally {\n        s.close();\n    }\n    count = 0;\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.setTimeStamp(1000L);\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    s = t.getScanner(scan);\n    try {\n        while (s.next() != null) {\n            count += 1;\n        } \n        junit.framework.Assert.assertEquals(\"Number of rows should be 2\", 2, count);\n    } finally {\n        s.close();\n    }\n    count = 0;\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.setTimeRange(100L, 1000L);\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    s = t.getScanner(scan);\n    try {\n        while (s.next() != null) {\n            count += 1;\n        } \n        junit.framework.Assert.assertEquals(\"Number of rows should be 2\", 2, count);\n    } finally {\n        s.close();\n    }\n    count = 0;\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.setTimeStamp(100L);\n    scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    s = t.getScanner(scan);\n    try {\n        while (s.next() != null) {\n            count += 1;\n        } \n        junit.framework.Assert.assertEquals(\"Number of rows should be 2\", 2, count);\n    } finally {\n        s.close();\n    }\n}",
            "ClassName": "TestScanMultipleVersions",
            "CyclomaticComplexity": 8,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanMultipleVersions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 82,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testWeirdCacheBehaviour",
        "NumberOfAsserts": 8,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 35,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testSuperSimple\");\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n    ht.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, TABLE);\n    org.apache.hadoop.hbase.client.ResultScanner scanner = ht.getScanner(scan);\n    org.apache.hadoop.hbase.client.Result result = scanner.next();\n    org.junit.Assert.assertTrue(\"Expected null result\", result == null);\n    scanner.close();\n    java.lang.System.out.println(\"Done.\");\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(getConfiguration(), tableName);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.ResultScanner resScan = table.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result res : resScan) {\n        org.apache.hadoop.hbase.client.Delete del = new org.apache.hadoop.hbase.client.Delete(res.getRow());\n        table.delete(del);\n    }\n    return table;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "truncateTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 10,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        org.apache.hadoop.hbase.HRegionInfo hri = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(b);\n        byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(row);\n        p.add(org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily(), org.apache.hadoop.hbase.master.TestMasterTransitions.getTestQualifier(), row);\n        t.put(p);\n        rows++;\n    }\n    s.close();\n    org.junit.Assert.assertEquals(expected, rows);\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addToEachStartKey",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testSuperSimple",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 14,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.TestShell.jruby.runScriptlet(PathType.ABSOLUTE, \"src/test/ruby/tests_runner.rb\");\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestShell.TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.msginterval\", 100);\n    org.apache.hadoop.hbase.client.TestShell.TEST_UTIL.getConfiguration().setInt(\"hbase.client.pause\", 250);\n    org.apache.hadoop.hbase.client.TestShell.TEST_UTIL.getConfiguration().setInt(\"hbase.client.retries.number\", 6);\n    org.apache.hadoop.hbase.client.TestShell.TEST_UTIL.startMiniCluster();\n    java.util.List<java.lang.String> loadPaths = new java.util.ArrayList();\n    loadPaths.add(\"src/main/ruby\");\n    loadPaths.add(\"src/test/ruby\");\n    org.apache.hadoop.hbase.client.TestShell.jruby.getProvider().setLoadPaths(loadPaths);\n    org.apache.hadoop.hbase.client.TestShell.jruby.put(\"$TEST_CLUSTER\", org.apache.hadoop.hbase.client.TestShell.TEST_UTIL);\n}",
            "ClassName": "TestShell",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 11,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestShell.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestShell",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    if (!fs.exists(src)) {\n        throw new java.io.FileNotFoundException(src.toString());\n    }\n    if (!fs.rename(src, tgt)) {\n        throw new java.io.IOException(((\"Failed rename of \" + src) + \" to \") + tgt);\n    }\n    return tgt;\n}",
            "ClassName": "StoreFile",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "rename",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 2,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    java.util.List<org.apache.hadoop.hbase.KeyValue> keys = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    org.apache.hadoop.hbase.client.Result r = new org.apache.hadoop.hbase.client.Result(keys);\n    junit.framework.Assert.assertTrue(r.isEmpty());\n    byte[] rb = org.apache.hadoop.hbase.util.Writables.getBytes(r);\n    org.apache.hadoop.hbase.client.Result deserializedR = ((org.apache.hadoop.hbase.client.Result) (org.apache.hadoop.hbase.util.Writables.getWritable(rb, new org.apache.hadoop.hbase.client.Result())));\n    junit.framework.Assert.assertTrue(deserializedR.isEmpty());\n}",
            "ClassName": "TestSerialization",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testResultEmpty",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    for (org.apache.hadoop.hbase.regionserver.StoreFile src : storeFiles) {\n        byte[] family = src.getFamily();\n        java.util.List<org.apache.hadoop.hbase.regionserver.StoreFile> v = byFamily.get(family);\n        if (v == null) {\n            v = new java.util.ArrayList<org.apache.hadoop.hbase.regionserver.StoreFile>();\n            byFamily.put(family, v);\n        }\n        v.add(src);\n    }\n    return byFamily;\n}",
            "ClassName": "HRegion",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "filesByFamily",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestShell",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testRunShellTests",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    testScan(\"yyy\", null, \"zzz\");\n}",
        "CUT_1": {
            "Body": "{\n    return ((x == null) && (y == null)) || ((x != null) && x.equals(y));\n}",
            "ClassName": "Pair",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "equals",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HalfStoreFileReader",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "midkey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return null;\n}",
            "ClassName": "WildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return null;\n}",
            "ClassName": "ScanWildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "internalPreFlushcacheCommit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableInputFormatScan",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testScanYYYToEmpty",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    conf.setBoolean(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HBASE_SKIP_ERRORS, true);\n    generateHLogs(java.lang.Integer.MAX_VALUE);\n    corruptHLog(new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + \"5\"), org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.Corruptions.INSERT_GARBAGE_IN_THE_MIDDLE, false, fs);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    for (java.lang.String region : org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions) {\n        org.apache.hadoop.fs.Path logfile = getLogForRegion(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, region);\n        int goodEntries = (org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.NUM_WRITERS - 1) * org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.ENTRIES;\n        int firstHalfEntries = ((int) (java.lang.Math.ceil(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.ENTRIES / 2))) - 1;\n        org.junit.Assert.assertTrue(\"The file up to the corrupted area hasn't been parsed\", (goodEntries + firstHalfEntries) <= countHLog(logfile, fs, conf));\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.removeAll(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions);\n    for (int i = 0; i < 100; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.add(\"region__\" + i);\n    }\n    generateHLogs(1, 100, -1);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    fs.rename(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir);\n    org.apache.hadoop.fs.Path firstSplitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME) + \".first\");\n    org.apache.hadoop.fs.Path splitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME));\n    fs.rename(splitPath, firstSplitPath);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    org.junit.Assert.assertEquals(0, compareHLogSplitDirs(firstSplitPath, splitPath));\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSplittingLargeNumberOfRegionsConsistency",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    for (int i = 0; i < writers; i++) {\n        writer[i] = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + i), conf);\n        for (int j = 0; j < entries; j++) {\n            int prefix = 0;\n            for (java.lang.String region : org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions) {\n                java.lang.String row_key = ((region + (prefix++)) + i) + j;\n                appendEntry(writer[i], org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, region.getBytes(), row_key.getBytes(), org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.FAMILY, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.QUALIFIER, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.VALUE, seq);\n            }\n        }\n        if (i != leaveOpen) {\n            writer[i].close();\n            flushToConsole(\"Closing writer \" + i);\n        }\n    }\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "generateHLogs",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.flushlogentries\", 1);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setStrings(\"hbase.rootdir\", org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir.toString());\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setClass(\"hbase.regionserver.hlog.writer.impl\", org.apache.hadoop.hbase.regionserver.wal.InstrumentedSequenceFileLogWriter.class, org.apache.hadoop.hbase.regionserver.wal.HLog.Writer.class);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.startMiniDFSCluster(2);\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Writer writer = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + suffix), conf);\n    if (closeFile)\n        writer.close();\n\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "injectEmptyFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.shutdownMiniDFSCluster();\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHLogSplit",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 1,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testMiddleGarbageCorruptionSkipErrorsReadsHalfOfFile",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 13,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    testScan(null, \"opp\", \"opo\");\n}",
        "CUT_1": {
            "Body": "{\n    return ((x == null) && (y == null)) || ((x != null) && x.equals(y));\n}",
            "ClassName": "Pair",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "equals",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return null;\n}",
            "ClassName": "ScanWildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HalfStoreFileReader",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "midkey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "internalPreFlushcacheCommit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return null;\n}",
            "ClassName": "WildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableInputFormatScan",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testScanEmptyToOPP",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.master.TestKillingServersFromMaster.cluster.addExceptionToSendRegionServer(0, new org.apache.hadoop.hbase.YouAreDeadException(\"bam!\"));\n    org.apache.hadoop.hbase.master.TestKillingServersFromMaster.cluster.waitOnRegionServer(0);\n    org.junit.Assert.assertEquals(1, org.apache.hadoop.hbase.master.TestKillingServersFromMaster.cluster.getLiveRegionServerThreads().size());\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestKillingServersFromMaster.TEST_UTIL.startMiniCluster(2);\n    org.apache.hadoop.hbase.master.TestKillingServersFromMaster.cluster = org.apache.hadoop.hbase.master.TestKillingServersFromMaster.TEST_UTIL.getHBaseCluster();\n}",
            "ClassName": "TestKillingServersFromMaster",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "beforeAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestKillingServersFromMaster.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestKillingServersFromMaster",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "afterAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestKillingServersFromMaster.TEST_UTIL.ensureSomeRegionServersAvailable(2);\n}",
            "ClassName": "TestKillingServersFromMaster",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setup",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    this.cluster = cluster;\n}",
            "ClassName": "Client",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setCluster",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return cluster;\n}",
            "ClassName": "Client",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getCluster",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestKillingServersFromMaster",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 1,
        "Label": 0,
        "MethodName": "testSendYouAreDead",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 5,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testGetConfiguration\");\n    byte[][] FAMILIES = new byte[][]{ org.apache.hadoop.hbase.util.Bytes.toBytes(\"foo\") };\n    org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration();\n    org.apache.hadoop.hbase.client.HTable table = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, FAMILIES);\n    org.junit.Assert.assertSame(conf, table.getConfiguration());\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.TestZooKeeper.conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(getConfiguration(), tableName);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.ResultScanner resScan = table.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result res : resScan) {\n        org.apache.hadoop.hbase.client.Delete del = new org.apache.hadoop.hbase.client.Delete(res.getRow());\n        table.delete(del);\n    }\n    return table;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "truncateTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 10,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testGetConfiguration",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 7,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    injectEmptyFile(\".empty\", true);\n    generateHLogs(java.lang.Integer.MAX_VALUE);\n    injectEmptyFile(\"empty\", true);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    for (java.lang.String region : org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions) {\n        org.apache.hadoop.fs.Path logfile = getLogForRegion(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, region);\n        org.junit.Assert.assertEquals(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.NUM_WRITERS * org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.ENTRIES, countHLog(logfile, fs, conf));\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.removeAll(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions);\n    for (int i = 0; i < 100; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.add(\"region__\" + i);\n    }\n    generateHLogs(1, 100, -1);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    fs.rename(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir);\n    org.apache.hadoop.fs.Path firstSplitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME) + \".first\");\n    org.apache.hadoop.fs.Path splitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME));\n    fs.rename(splitPath, firstSplitPath);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    org.junit.Assert.assertEquals(0, compareHLogSplitDirs(firstSplitPath, splitPath));\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSplittingLargeNumberOfRegionsConsistency",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    for (int i = 0; i < writers; i++) {\n        writer[i] = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + i), conf);\n        for (int j = 0; j < entries; j++) {\n            int prefix = 0;\n            for (java.lang.String region : org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions) {\n                java.lang.String row_key = ((region + (prefix++)) + i) + j;\n                appendEntry(writer[i], org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, region.getBytes(), row_key.getBytes(), org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.FAMILY, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.QUALIFIER, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.VALUE, seq);\n            }\n        }\n        if (i != leaveOpen) {\n            writer[i].close();\n            flushToConsole(\"Closing writer \" + i);\n        }\n    }\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "generateHLogs",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.flushlogentries\", 1);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setStrings(\"hbase.rootdir\", org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir.toString());\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setClass(\"hbase.regionserver.hlog.writer.impl\", org.apache.hadoop.hbase.regionserver.wal.InstrumentedSequenceFileLogWriter.class, org.apache.hadoop.hbase.regionserver.wal.HLog.Writer.class);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.startMiniDFSCluster(2);\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Writer writer = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + suffix), conf);\n    if (closeFile)\n        writer.close();\n\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "injectEmptyFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.shutdownMiniDFSCluster();\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHLogSplit",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 1,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testEmptyLogFiles",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 11,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.ipc.HBaseServer.Call call = org.apache.hadoop.hbase.ipc.HBaseServer.CurCall.get();\n    if (call != null) {\n        return call.connection.socket.getInetAddress();\n    }\n    return null;\n}",
            "ClassName": "HBaseServer",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getRemoteIp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    this.writer.append(entry.getKey(), entry.getEdit());\n}",
            "ClassName": "SequenceFileLogWriter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "append",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return cacheBlocks;\n}",
            "ClassName": "Scan",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getCacheBlocks",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.HRegion r = org.apache.hadoop.hbase.regionserver.HRegion.newHRegion(org.apache.hadoop.hbase.HTableDescriptor.getTableDir(rootDir, regionInfo.getTableDesc().getName()), this.hlog, this.fs, conf, regionInfo, this.cacheFlusher);\n    r.initialize(null, new org.apache.hadoop.util.Progressable() {\n        public void progress() {\n            addProcessingMessage(regionInfo);\n        }\n    });\n    return r;\n}",
            "ClassName": "HRegionServer",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "instantiateRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    final org.apache.hadoop.hbase.HRegionInfo oldRegionInfo = region.getRegionInfo();\n    final long startTime = java.lang.System.currentTimeMillis();\n    final org.apache.hadoop.hbase.regionserver.HRegion[] newRegions = region.splitRegion(midKey);\n    if (newRegions == null) {\n        return;\n    }\n    org.apache.hadoop.hbase.client.HTable t = null;\n    if (region.getRegionInfo().isMetaTable()) {\n        if (this.root == null) {\n            this.root = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.ROOT_TABLE_NAME);\n        }\n        t = root;\n    } else {\n        if (meta == null) {\n            meta = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n        }\n        t = meta;\n    }\n    oldRegionInfo.setOffline(true);\n    oldRegionInfo.setSplit(true);\n    this.server.removeFromOnlineRegions(oldRegionInfo);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(oldRegionInfo.getRegionName());\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(oldRegionInfo));\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITA_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(newRegions[0].getRegionInfo()));\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITB_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(newRegions[1].getRegionInfo()));\n    t.put(put);\n    for (int i = 0; i < newRegions.length; i++) {\n        put = new org.apache.hadoop.hbase.client.Put(newRegions[i].getRegionName());\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(newRegions[i].getRegionInfo()));\n        t.put(put);\n    }\n    server.reportSplit(oldRegionInfo, newRegions[0].getRegionInfo(), newRegions[1].getRegionInfo());\n    org.apache.hadoop.hbase.regionserver.CompactSplitThread.LOG.info((((((((\"region split, META updated, and report to master all\" + \" successful. Old region=\") + oldRegionInfo.toString()) + \", new regions: \") + newRegions[0].toString()) + \", \") + newRegions[1].toString()) + \". Split took \") + org.apache.hadoop.util.StringUtils.formatTimeDiff(java.lang.System.currentTimeMillis(), startTime));\n}",
            "ClassName": "CompactSplitThread",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "split",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 37,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testMultipleRegionsAndBatchPuts",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 2,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    injectEmptyFile(\".empty\", false);\n    generateHLogs(java.lang.Integer.MAX_VALUE);\n    injectEmptyFile(\"empty\", false);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    for (java.lang.String region : org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions) {\n        org.apache.hadoop.fs.Path logfile = getLogForRegion(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, region);\n        org.junit.Assert.assertEquals(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.NUM_WRITERS * org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.ENTRIES, countHLog(logfile, fs, conf));\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.removeAll(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions);\n    for (int i = 0; i < 100; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.add(\"region__\" + i);\n    }\n    generateHLogs(1, 100, -1);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    fs.rename(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir);\n    org.apache.hadoop.fs.Path firstSplitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME) + \".first\");\n    org.apache.hadoop.fs.Path splitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME));\n    fs.rename(splitPath, firstSplitPath);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    org.junit.Assert.assertEquals(0, compareHLogSplitDirs(firstSplitPath, splitPath));\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSplittingLargeNumberOfRegionsConsistency",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    for (int i = 0; i < writers; i++) {\n        writer[i] = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + i), conf);\n        for (int j = 0; j < entries; j++) {\n            int prefix = 0;\n            for (java.lang.String region : org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions) {\n                java.lang.String row_key = ((region + (prefix++)) + i) + j;\n                appendEntry(writer[i], org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, region.getBytes(), row_key.getBytes(), org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.FAMILY, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.QUALIFIER, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.VALUE, seq);\n            }\n        }\n        if (i != leaveOpen) {\n            writer[i].close();\n            flushToConsole(\"Closing writer \" + i);\n        }\n    }\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "generateHLogs",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.flushlogentries\", 1);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setStrings(\"hbase.rootdir\", org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir.toString());\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setClass(\"hbase.regionserver.hlog.writer.impl\", org.apache.hadoop.hbase.regionserver.wal.InstrumentedSequenceFileLogWriter.class, org.apache.hadoop.hbase.regionserver.wal.HLog.Writer.class);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.startMiniDFSCluster(2);\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Writer writer = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + suffix), conf);\n    if (closeFile)\n        writer.close();\n\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "injectEmptyFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.shutdownMiniDFSCluster();\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHLogSplit",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 1,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testEmptyOpenLogFiles",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 11,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testJiraTest861\");\n    byte[][] VALUES = makeNAscii(org.apache.hadoop.hbase.client.TestFromClientSide.VALUE, 7);\n    long[] STAMPS = makeStamps(7);\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, 10);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3], VALUES[3]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    ht.put(put);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5]);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5]);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0], VALUES[0]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6], VALUES[6]);\n    ht.put(put);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0], VALUES[0]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3], VALUES[3]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6], VALUES[6]);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0], VALUES[0]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3], VALUES[3]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    getVersionAndVerifyMissing(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5]);\n    getVersionAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[6], VALUES[6]);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration());\n    admin.split(t.getTableName());\n    java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> regions = waitOnSplit(t);\n    org.junit.Assert.assertTrue(regions.size() > 1);\n    return regions;\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "splitTable",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(family, qualifier);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertNResult(result, row, family, qualifier, stamps, values, start, end);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getAllVersionsAndVerify",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testJiraTest861",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 37,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor tableDesc = new org.apache.hadoop.hbase.HTableDescriptor(\"testtable\");\n    org.apache.hadoop.hbase.HRegionInfo hri = new org.apache.hadoop.hbase.HRegionInfo(tableDesc, org.apache.hadoop.hbase.util.Bytes.toBytes(\"a\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"g\"));\n    org.junit.Assert.assertTrue(hri.containsRange(org.apache.hadoop.hbase.util.Bytes.toBytes(\"a\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"a\")));\n    org.junit.Assert.assertTrue(hri.containsRange(org.apache.hadoop.hbase.util.Bytes.toBytes(\"b\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"c\")));\n    org.junit.Assert.assertTrue(hri.containsRange(org.apache.hadoop.hbase.util.Bytes.toBytes(\"a\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"c\")));\n    org.junit.Assert.assertTrue(hri.containsRange(org.apache.hadoop.hbase.util.Bytes.toBytes(\"c\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"c\")));\n    org.junit.Assert.assertFalse(hri.containsRange(org.apache.hadoop.hbase.util.Bytes.toBytes(\"a\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"g\")));\n    org.junit.Assert.assertFalse(hri.containsRange(org.apache.hadoop.hbase.util.Bytes.toBytes(\"g\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"g\")));\n    org.junit.Assert.assertFalse(hri.containsRange(org.apache.hadoop.hbase.util.Bytes.toBytes(\"z\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"z\")));\n    try {\n        hri.containsRange(org.apache.hadoop.hbase.util.Bytes.toBytes(\"z\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"a\"));\n        org.junit.Assert.fail(\"Invalid range did not throw IAE\");\n    } catch (java.lang.IllegalArgumentException iae) {\n    }\n}",
        "CUT_1": {
            "Body": "{\n    return new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(row), org.apache.hadoop.hbase.util.Bytes.toBytes(family), org.apache.hadoop.hbase.util.Bytes.toBytes(qualifier), timestamp, type, org.apache.hadoop.hbase.util.Bytes.toBytes(value));\n}",
            "ClassName": "KeyValueTestUtil",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "create",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.HRegionInfo a = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(\"a\"), null, null);\n    org.apache.hadoop.hbase.HRegionInfo b = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(\"b\"), null, null);\n    junit.framework.Assert.assertTrue(a.compareTo(b) != 0);\n    org.apache.hadoop.hbase.HTableDescriptor t = new org.apache.hadoop.hbase.HTableDescriptor(\"t\");\n    byte[] midway = org.apache.hadoop.hbase.util.Bytes.toBytes(\"midway\");\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, null, midway);\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, midway, null);\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n    junit.framework.Assert.assertTrue(b.compareTo(a) > 0);\n    junit.framework.Assert.assertEquals(a, a);\n    junit.framework.Assert.assertTrue(a.compareTo(a) == 0);\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"a\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"d\"));\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"e\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"g\"));\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"dddd\"));\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"e\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"g\"));\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"dddd\"));\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"eeee\"));\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n}",
            "ClassName": "TestCompare",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testHRegionInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.startsWith(org.apache.hadoop.hbase.util.Bytes.toBytes(\"hello\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"h\")));\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.startsWith(org.apache.hadoop.hbase.util.Bytes.toBytes(\"hello\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\")));\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.startsWith(org.apache.hadoop.hbase.util.Bytes.toBytes(\"hello\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"hello\")));\n    junit.framework.Assert.assertFalse(org.apache.hadoop.hbase.util.Bytes.startsWith(org.apache.hadoop.hbase.util.Bytes.toBytes(\"hello\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"helloworld\")));\n    junit.framework.Assert.assertFalse(org.apache.hadoop.hbase.util.Bytes.startsWith(org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"hello\")));\n}",
            "ClassName": "TestBytes",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testStartsWith",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    super.setUp();\n    data = org.apache.hadoop.hbase.util.Bytes.toBytes(\"data\");\n    row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam1\");\n    col1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"col1\");\n    row2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row2\");\n    fam2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam2\");\n    col2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"col2\");\n    col3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"col3\");\n    col4 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"col4\");\n    col5 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"col5\");\n}",
            "ClassName": "TestKeyValueHeap",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 13,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    setValue(org.apache.hadoop.hbase.util.Bytes.toBytes(key), org.apache.hadoop.hbase.util.Bytes.toBytes(value));\n}",
            "ClassName": "HColumnDescriptor",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setValue",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHRegionInfo",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testContainsRange",
        "NumberOfAsserts": 8,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 16,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    testScan(null, \"app\", \"apo\");\n}",
        "CUT_1": {
            "Body": "{\n    return ((x == null) && (y == null)) || ((x != null) && x.equals(y));\n}",
            "ClassName": "Pair",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "equals",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return null;\n}",
            "ClassName": "ScanWildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HalfStoreFileReader",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "midkey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "internalPreFlushcacheCommit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return null;\n}",
            "ClassName": "WildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableInputFormatScan",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testScanEmptyToAPP",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    try {\n        util.startMiniCluster(1);\n        byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"table\");\n        byte[] fam = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\");\n        org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(util.getConfiguration());\n        org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n        desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(fam, 1, org.apache.hadoop.hbase.HColumnDescriptor.DEFAULT_COMPRESSION, false, false, org.apache.hadoop.hbase.HConstants.FOREVER, \"NONE\"));\n        admin.createTable(desc);\n        org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n        util.loadTable(table, fam);\n        table.flushCommits();\n        util.flush();\n        util.countRows(table);\n        util.getDFSCluster().shutdownDataNodes();\n        try {\n            util.countRows(table);\n            org.junit.Assert.fail(\"Did not fail to count after removing data\");\n        } catch (java.lang.Exception e) {\n            org.apache.hadoop.hbase.regionserver.TestFSErrorsExposed.LOG.info(\"Got expected error\", e);\n            org.junit.Assert.assertTrue(e.getMessage().contains(\"Could not seek\"));\n        }\n    } finally {\n        util.shutdownMiniCluster();\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.TestZooKeeper.conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    this.server = cluster.getRegionServerThreads().get(0).getRegionServer();\n    this.log = server.getLog();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    for (int i = 1; i <= 256; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + java.lang.String.format(\"%1$04d\", i)));\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, value);\n        table.put(put);\n        if ((i % 32) == 0) {\n            try {\n                java.lang.Thread.sleep(2000);\n            } catch (java.lang.InterruptedException e) {\n            }\n        }\n    }\n}",
            "ClassName": "TestLogRolling",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startAndWriteData",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(getName());\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.client.TestTimestamp.COLUMN_NAME));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    return new org.apache.hadoop.hbase.client.HTable(conf, getName());\n}",
            "ClassName": "TestTimestamp",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(this.conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    java.util.List<byte[]> rows = new java.util.ArrayList<byte[]>();\n    org.apache.hadoop.hbase.client.ResultScanner s = t.getScanner(new org.apache.hadoop.hbase.client.Scan());\n    for (org.apache.hadoop.hbase.client.Result result : s) {\n        org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(result.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER));\n        org.apache.hadoop.hbase.HTableDescriptor desc = info.getTableDesc();\n        if (org.apache.hadoop.hbase.util.Bytes.compareTo(desc.getName(), tableName) == 0) {\n            LOG.info(\"getMetaTableRows: row -> \" + org.apache.hadoop.hbase.util.Bytes.toStringBinary(result.getRow()));\n            rows.add(result.getRow());\n        }\n    }\n    s.close();\n    return rows;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getMetaTableRows",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 15,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFSErrorsExposed",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testFullSystemBubblesFSErrors",
        "NumberOfAsserts": 2,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 26,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.fs.Path hfilePath = new org.apache.hadoop.fs.Path(new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.HBaseTestingUtility.getTestDir(\"internalScannerExposesErrors\"), \"regionname\"), \"familyname\");\n    org.apache.hadoop.hbase.regionserver.TestFSErrorsExposed.FaultyFileSystem fs = new org.apache.hadoop.hbase.regionserver.TestFSErrorsExposed.FaultyFileSystem(util.getTestFileSystem());\n    org.apache.hadoop.hbase.io.hfile.HFile.Writer writer = org.apache.hadoop.hbase.regionserver.StoreFile.createWriter(fs, hfilePath, 2 * 1024);\n    org.apache.hadoop.hbase.regionserver.TestStoreFile.writeStoreFile(writer, org.apache.hadoop.hbase.util.Bytes.toBytes(\"cf\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"qual\"));\n    org.apache.hadoop.hbase.regionserver.StoreFile sf = new org.apache.hadoop.hbase.regionserver.StoreFile(fs, writer.getPath(), false, util.getConfiguration(), org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE, false);\n    java.util.List<org.apache.hadoop.hbase.regionserver.StoreFileScanner> scanners = org.apache.hadoop.hbase.regionserver.StoreFileScanner.getScannersForStoreFiles(java.util.Collections.singletonList(sf), false, true);\n    org.apache.hadoop.hbase.regionserver.KeyValueScanner scanner = scanners.get(0);\n    org.apache.hadoop.hbase.regionserver.TestFSErrorsExposed.FaultyInputStream inStream = fs.inStreams.get(0).get();\n    org.junit.Assert.assertNotNull(inStream);\n    scanner.seek(org.apache.hadoop.hbase.KeyValue.LOWESTKEY);\n    org.junit.Assert.assertNotNull(scanner.next());\n    inStream.startFaults();\n    try {\n        int scanned = 0;\n        while (scanner.next() != null) {\n            scanned++;\n        } \n        org.junit.Assert.fail(\"Scanner didn't throw after faults injected\");\n    } catch (java.io.IOException ioe) {\n        org.apache.hadoop.hbase.regionserver.TestFSErrorsExposed.LOG.info(\"Got expected exception\", ioe);\n        org.junit.Assert.assertTrue(ioe.getMessage().contains(\"Could not iterate\"));\n    }\n    scanner.close();\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.io.hfile.HFile.Writer writer = org.apache.hadoop.hbase.regionserver.StoreFile.createWriter(this.fs, new org.apache.hadoop.fs.Path(new org.apache.hadoop.fs.Path(this.testDir, \"regionname\"), \"familyname\"), 2 * 1024);\n    writeStoreFile(writer);\n    checkHalfHFile(new org.apache.hadoop.hbase.regionserver.StoreFile(this.fs, writer.getPath(), true, conf, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE, false));\n}",
            "ClassName": "TestStoreFile",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testBasicHalfMapFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 5,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.fs.Path storedir = new org.apache.hadoop.fs.Path(new org.apache.hadoop.fs.Path(this.testDir, \"regionname\"), \"familyname\");\n    org.apache.hadoop.fs.Path dir = new org.apache.hadoop.fs.Path(storedir, \"1234567890\");\n    org.apache.hadoop.hbase.io.hfile.HFile.Writer writer = org.apache.hadoop.hbase.regionserver.StoreFile.createWriter(this.fs, dir, 8 * 1024);\n    writeStoreFile(writer);\n    org.apache.hadoop.hbase.regionserver.StoreFile hsf = new org.apache.hadoop.hbase.regionserver.StoreFile(this.fs, writer.getPath(), true, conf, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE, false);\n    org.apache.hadoop.hbase.io.hfile.HFile.Reader reader = hsf.createReader();\n    org.apache.hadoop.hbase.KeyValue kv = org.apache.hadoop.hbase.KeyValue.createKeyValueFromKey(reader.midkey());\n    byte[] midRow = kv.getRow();\n    kv = org.apache.hadoop.hbase.KeyValue.createKeyValueFromKey(reader.getLastKey());\n    byte[] finalRow = kv.getRow();\n    org.apache.hadoop.fs.Path refPath = org.apache.hadoop.hbase.regionserver.StoreFile.split(fs, dir, hsf, midRow, org.apache.hadoop.hbase.io.Reference.Range.top);\n    org.apache.hadoop.hbase.regionserver.StoreFile refHsf = new org.apache.hadoop.hbase.regionserver.StoreFile(this.fs, refPath, true, conf, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.NONE, false);\n    org.apache.hadoop.hbase.io.hfile.HFileScanner s = refHsf.createReader().getScanner(false, false);\n    for (boolean first = true; ((!s.isSeeked()) && s.seekTo()) || s.next();) {\n        java.nio.ByteBuffer bb = s.getKey();\n        kv = org.apache.hadoop.hbase.KeyValue.createKeyValueFromKey(bb);\n        if (first) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(kv.getRow(), midRow));\n            first = false;\n        }\n    }\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(kv.getRow(), finalRow));\n}",
            "ClassName": "TestStoreFile",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testReference",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 24,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    int maxKeyCount = 0;\n    for (org.apache.hadoop.hbase.regionserver.StoreFile file : filesToCompact) {\n        org.apache.hadoop.hbase.regionserver.StoreFile.Reader r = file.getReader();\n        if (r != null) {\n            maxKeyCount += (r.getBloomFilterType() == family.getBloomFilterType()) ? r.getFilterEntries() : r.getEntries();\n        }\n    }\n    java.util.List<org.apache.hadoop.hbase.regionserver.StoreFileScanner> scanners = org.apache.hadoop.hbase.regionserver.StoreFileScanner.getScannersForStoreFiles(filesToCompact, false, false);\n    org.apache.hadoop.hbase.regionserver.StoreFile.Writer writer = null;\n    try {\n        if (majorCompaction) {\n            org.apache.hadoop.hbase.regionserver.InternalScanner scanner = null;\n            try {\n                org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n                scan.setMaxVersions(family.getMaxVersions());\n                scanner = new org.apache.hadoop.hbase.regionserver.StoreScanner(this, scan, scanners);\n                java.util.ArrayList<org.apache.hadoop.hbase.KeyValue> kvs = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n                while (scanner.next(kvs)) {\n                    for (org.apache.hadoop.hbase.KeyValue kv : kvs) {\n                        if (writer == null) {\n                            writer = createWriter(this.regionCompactionDir, maxKeyCount);\n                        }\n                        writer.append(kv);\n                    }\n                    kvs.clear();\n                } \n            } finally {\n                if (scanner != null) {\n                    scanner.close();\n                }\n            }\n        } else {\n            org.apache.hadoop.hbase.regionserver.MinorCompactingStoreScanner scanner = null;\n            try {\n                scanner = new org.apache.hadoop.hbase.regionserver.MinorCompactingStoreScanner(this, scanners);\n                writer = createWriter(this.regionCompactionDir, maxKeyCount);\n                while (scanner.next(writer)) {\n                } \n            } finally {\n                if (scanner != null)\n                    scanner.close();\n\n            }\n        }\n    } finally {\n        if (writer != null) {\n            writer.appendMetadata(maxId, majorCompaction);\n            writer.close();\n        }\n    }\n    return writer;\n}",
            "ClassName": "Store",
            "CyclomaticComplexity": 10,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "compact",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 53,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.fs.Path basedir = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.TestStore.DIR + methodName);\n    org.apache.hadoop.fs.Path logdir = new org.apache.hadoop.fs.Path((org.apache.hadoop.hbase.regionserver.TestStore.DIR + methodName) + \"/logs\");\n    org.apache.hadoop.fs.Path oldLogDir = new org.apache.hadoop.fs.Path(basedir, org.apache.hadoop.hbase.HConstants.HREGION_OLDLOGDIR_NAME);\n    org.apache.hadoop.hbase.HColumnDescriptor hcd = new org.apache.hadoop.hbase.HColumnDescriptor(family);\n    org.apache.hadoop.hbase.HBaseConfiguration conf = new org.apache.hadoop.hbase.HBaseConfiguration();\n    org.apache.hadoop.fs.FileSystem fs = org.apache.hadoop.fs.FileSystem.get(conf);\n    org.apache.hadoop.fs.Path reconstructionLog = null;\n    org.apache.hadoop.util.Progressable reporter = null;\n    fs.delete(logdir, true);\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(table);\n    htd.addFamily(hcd);\n    org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(htd, null, null, false);\n    org.apache.hadoop.hbase.regionserver.wal.HLog hlog = new org.apache.hadoop.hbase.regionserver.wal.HLog(fs, logdir, oldLogDir, conf, null);\n    org.apache.hadoop.hbase.regionserver.HRegion region = new org.apache.hadoop.hbase.regionserver.HRegion(basedir, hlog, fs, conf, info, null);\n    store = new org.apache.hadoop.hbase.regionserver.Store(basedir, region, hcd, fs, reconstructionLog, conf, reporter);\n}",
            "ClassName": "TestStore",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "init",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 17,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.fs.FileSystem fs = org.apache.hadoop.fs.FileSystem.getLocal(conf);\n    conf.setFloat(\"io.hfile.bloom.error.rate\", ((float) (0.01)));\n    conf.setBoolean(\"io.hfile.bloom.enabled\", true);\n    org.apache.hadoop.fs.Path f = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.TestStoreFile.ROOT_DIR, getName());\n    org.apache.hadoop.hbase.regionserver.StoreFile.Writer writer = new org.apache.hadoop.hbase.regionserver.StoreFile.Writer(fs, f, org.apache.hadoop.hbase.regionserver.StoreFile.DEFAULT_BLOCKSIZE_SMALL, org.apache.hadoop.hbase.io.hfile.HFile.DEFAULT_COMPRESSION_ALGORITHM, conf, org.apache.hadoop.hbase.KeyValue.COMPARATOR, org.apache.hadoop.hbase.regionserver.StoreFile.BloomType.ROW, 2000);\n    long now = java.lang.System.currentTimeMillis();\n    for (int i = 0; i < 2000; i += 2) {\n        java.lang.String row = java.lang.String.format(org.apache.hadoop.hbase.regionserver.TestStoreFile.localFormatter, java.lang.Integer.valueOf(i));\n        org.apache.hadoop.hbase.KeyValue kv = new org.apache.hadoop.hbase.KeyValue(row.getBytes(), \"family\".getBytes(), \"col\".getBytes(), now, \"value\".getBytes());\n        writer.append(kv);\n    }\n    writer.close();\n    org.apache.hadoop.hbase.regionserver.StoreFile.Reader reader = new org.apache.hadoop.hbase.regionserver.StoreFile.Reader(fs, f, null, false);\n    reader.loadFileInfo();\n    reader.loadBloomfilter();\n    org.apache.hadoop.hbase.io.hfile.HFileScanner scanner = reader.getScanner(false, false);\n    int falsePos = 0;\n    int falseNeg = 0;\n    for (int i = 0; i < 2000; i++) {\n        java.lang.String row = java.lang.String.format(org.apache.hadoop.hbase.regionserver.TestStoreFile.localFormatter, java.lang.Integer.valueOf(i));\n        java.util.TreeSet<byte[]> columns = new java.util.TreeSet<byte[]>();\n        columns.add(\"family:col\".getBytes());\n        boolean exists = scanner.shouldSeek(row.getBytes(), columns);\n        if ((i % 2) == 0) {\n            if (!exists)\n                falseNeg++;\n\n        } else {\n            if (exists)\n                falsePos++;\n\n        }\n    }\n    reader.close();\n    fs.delete(f, true);\n    java.lang.System.out.println(\"False negatives: \" + falseNeg);\n    junit.framework.Assert.assertEquals(0, falseNeg);\n    java.lang.System.out.println(\"False positives: \" + falsePos);\n    junit.framework.Assert.assertTrue(falsePos < 2);\n}",
            "ClassName": "TestStoreFile",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testBloomFilter",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 41,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFSErrorsExposed",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 1,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testStoreFileScannerThrowsErrors",
        "NumberOfAsserts": 4,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 25,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.util.IncrementingEnvironmentEdge edge = new org.apache.hadoop.hbase.util.IncrementingEnvironmentEdge();\n    junit.framework.Assert.assertEquals(1, edge.currentTimeMillis());\n    junit.framework.Assert.assertEquals(2, edge.currentTimeMillis());\n    junit.framework.Assert.assertEquals(3, edge.currentTimeMillis());\n    junit.framework.Assert.assertEquals(4, edge.currentTimeMillis());\n}",
        "CUT_1": {
            "Body": "{\n    if (edge == null) {\n        org.apache.hadoop.hbase.util.EnvironmentEdgeManager.reset();\n    } else {\n        org.apache.hadoop.hbase.util.EnvironmentEdgeManager.delegate = edge;\n    }\n}",
            "ClassName": "EnvironmentEdgeManager",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "injectEdge",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.util.EnvironmentEdgeManager.injectEdge(edge);\n}",
            "ClassName": "EnvironmentEdgeManagerTestHelper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "injectEdge",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    junit.framework.Assert.assertEquals(model.getRegions(), 2);\n    junit.framework.Assert.assertEquals(model.getRequests(), 0);\n    junit.framework.Assert.assertEquals(model.getAverageLoad(), 1.0);\n    java.util.Iterator<org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel.Node> nodes = model.getLiveNodes().iterator();\n    org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel.Node node = nodes.next();\n    junit.framework.Assert.assertEquals(node.getName(), \"test1\");\n    junit.framework.Assert.assertEquals(node.getStartCode(), 1245219839331L);\n    junit.framework.Assert.assertEquals(node.getHeapSizeMB(), 128);\n    junit.framework.Assert.assertEquals(node.getMaxHeapSizeMB(), 1024);\n    java.util.Iterator<org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel.Node.Region> regions = node.getRegions().iterator();\n    org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel.Node.Region region = regions.next();\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.toString(region.getName()).equals(\"-ROOT-,,0\"));\n    junit.framework.Assert.assertEquals(region.getStores(), 1);\n    junit.framework.Assert.assertEquals(region.getStorefiles(), 1);\n    junit.framework.Assert.assertEquals(region.getStorefileSizeMB(), 0);\n    junit.framework.Assert.assertEquals(region.getMemstoreSizeMB(), 0);\n    junit.framework.Assert.assertEquals(region.getStorefileIndexSizeMB(), 0);\n    junit.framework.Assert.assertFalse(regions.hasNext());\n    node = nodes.next();\n    junit.framework.Assert.assertEquals(node.getName(), \"test2\");\n    junit.framework.Assert.assertEquals(node.getStartCode(), 1245239331198L);\n    junit.framework.Assert.assertEquals(node.getHeapSizeMB(), 512);\n    junit.framework.Assert.assertEquals(node.getMaxHeapSizeMB(), 1024);\n    regions = node.getRegions().iterator();\n    region = regions.next();\n    junit.framework.Assert.assertEquals(org.apache.hadoop.hbase.util.Bytes.toString(region.getName()), \".META.,,1246000043724\");\n    junit.framework.Assert.assertEquals(region.getStores(), 1);\n    junit.framework.Assert.assertEquals(region.getStorefiles(), 1);\n    junit.framework.Assert.assertEquals(region.getStorefileSizeMB(), 0);\n    junit.framework.Assert.assertEquals(region.getMemstoreSizeMB(), 0);\n    junit.framework.Assert.assertEquals(region.getStorefileIndexSizeMB(), 0);\n    junit.framework.Assert.assertFalse(regions.hasNext());\n    junit.framework.Assert.assertFalse(nodes.hasNext());\n}",
            "ClassName": "TestStorageClusterStatusModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "checkModel",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 35,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.fs.Path p = makeNewFile();\n    org.apache.hadoop.hbase.io.hfile.HFile.Reader reader = new org.apache.hadoop.hbase.io.hfile.HFile.Reader(fs, p, null, false);\n    reader.loadFileInfo();\n    org.apache.hadoop.hbase.io.hfile.HFileScanner scanner = reader.getScanner(false, true);\n    junit.framework.Assert.assertEquals(false, scanner.seekBefore(org.apache.hadoop.hbase.util.Bytes.toBytes(\"a\")));\n    junit.framework.Assert.assertEquals(false, scanner.seekBefore(org.apache.hadoop.hbase.util.Bytes.toBytes(\"c\")));\n    junit.framework.Assert.assertEquals(true, scanner.seekBefore(org.apache.hadoop.hbase.util.Bytes.toBytes(\"d\")));\n    junit.framework.Assert.assertEquals(\"c\", scanner.getKeyString());\n    junit.framework.Assert.assertEquals(true, scanner.seekBefore(org.apache.hadoop.hbase.util.Bytes.toBytes(\"e\")));\n    junit.framework.Assert.assertEquals(\"c\", scanner.getKeyString());\n    junit.framework.Assert.assertEquals(true, scanner.seekBefore(org.apache.hadoop.hbase.util.Bytes.toBytes(\"f\")));\n    junit.framework.Assert.assertEquals(\"e\", scanner.getKeyString());\n    junit.framework.Assert.assertEquals(true, scanner.seekBefore(org.apache.hadoop.hbase.util.Bytes.toBytes(\"g\")));\n    junit.framework.Assert.assertEquals(\"e\", scanner.getKeyString());\n    junit.framework.Assert.assertEquals(true, scanner.seekBefore(org.apache.hadoop.hbase.util.Bytes.toBytes(\"h\")));\n    junit.framework.Assert.assertEquals(\"g\", scanner.getKeyString());\n    junit.framework.Assert.assertEquals(true, scanner.seekBefore(org.apache.hadoop.hbase.util.Bytes.toBytes(\"i\")));\n    junit.framework.Assert.assertEquals(\"g\", scanner.getKeyString());\n    junit.framework.Assert.assertEquals(true, scanner.seekBefore(org.apache.hadoop.hbase.util.Bytes.toBytes(\"j\")));\n    junit.framework.Assert.assertEquals(\"i\", scanner.getKeyString());\n    junit.framework.Assert.assertEquals(true, scanner.seekBefore(org.apache.hadoop.hbase.util.Bytes.toBytes(\"k\")));\n    junit.framework.Assert.assertEquals(\"i\", scanner.getKeyString());\n    junit.framework.Assert.assertEquals(true, scanner.seekBefore(org.apache.hadoop.hbase.util.Bytes.toBytes(\"l\")));\n    junit.framework.Assert.assertEquals(\"k\", scanner.getKeyString());\n}",
            "ClassName": "TestSeekTo",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSeekBefore",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 26,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.fs.Path p = makeNewFile();\n    org.apache.hadoop.hbase.io.hfile.HFile.Reader reader = new org.apache.hadoop.hbase.io.hfile.HFile.Reader(fs, p, null, false);\n    reader.loadFileInfo();\n    junit.framework.Assert.assertEquals(2, reader.blockIndex.count);\n    org.apache.hadoop.hbase.io.hfile.HFileScanner scanner = reader.getScanner(false, true);\n    junit.framework.Assert.assertEquals(-1, scanner.seekTo(org.apache.hadoop.hbase.util.Bytes.toBytes(\"a\")));\n    junit.framework.Assert.assertEquals(1, scanner.seekTo(org.apache.hadoop.hbase.util.Bytes.toBytes(\"d\")));\n    junit.framework.Assert.assertEquals(\"c\", scanner.getKeyString());\n    junit.framework.Assert.assertEquals(1, scanner.seekTo(org.apache.hadoop.hbase.util.Bytes.toBytes(\"h\")));\n    junit.framework.Assert.assertEquals(\"g\", scanner.getKeyString());\n    junit.framework.Assert.assertEquals(1, scanner.seekTo(org.apache.hadoop.hbase.util.Bytes.toBytes(\"l\")));\n    junit.framework.Assert.assertEquals(\"k\", scanner.getKeyString());\n}",
            "ClassName": "TestSeekTo",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSeekTo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 14,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestIncrementingEnvironmentEdge",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testGetCurrentTimeUsesSystemClock",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 7,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    java.util.concurrent.atomic.AtomicBoolean stop = new java.util.concurrent.atomic.AtomicBoolean(false);\n    generateHLogs(-1);\n    fs.initialize(fs.getUri(), conf);\n    try {\n        new org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.ZombieNewLogWriterRegionServer(stop).start();\n        org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    } finally {\n        stop.set(true);\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.removeAll(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions);\n    for (int i = 0; i < 100; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.add(\"region__\" + i);\n    }\n    generateHLogs(1, 100, -1);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    fs.rename(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir);\n    org.apache.hadoop.fs.Path firstSplitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME) + \".first\");\n    org.apache.hadoop.fs.Path splitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME));\n    fs.rename(splitPath, firstSplitPath);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    org.junit.Assert.assertEquals(0, compareHLogSplitDirs(firstSplitPath, splitPath));\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSplittingLargeNumberOfRegionsConsistency",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    for (int i = 0; i < writers; i++) {\n        writer[i] = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + i), conf);\n        for (int j = 0; j < entries; j++) {\n            int prefix = 0;\n            for (java.lang.String region : org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions) {\n                java.lang.String row_key = ((region + (prefix++)) + i) + j;\n                appendEntry(writer[i], org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, region.getBytes(), row_key.getBytes(), org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.FAMILY, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.QUALIFIER, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.VALUE, seq);\n            }\n        }\n        if (i != leaveOpen) {\n            writer[i].close();\n            flushToConsole(\"Closing writer \" + i);\n        }\n    }\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "generateHLogs",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.flushlogentries\", 1);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setStrings(\"hbase.rootdir\", org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir.toString());\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setClass(\"hbase.regionserver.hlog.writer.impl\", org.apache.hadoop.hbase.regionserver.wal.InstrumentedSequenceFileLogWriter.class, org.apache.hadoop.hbase.regionserver.wal.HLog.Writer.class);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.startMiniDFSCluster(2);\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Writer writer = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + suffix), conf);\n    if (closeFile)\n        writer.close();\n\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "injectEmptyFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.shutdownMiniDFSCluster();\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHLogSplit",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testSplitFailsIfNewHLogGetsCreatedAfterSplitStarted",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 11,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 8,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    final byte[] table = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testTableExist\");\n    boolean exist = false;\n    exist = this.admin.tableExists(table);\n    org.junit.Assert.assertEquals(false, exist);\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.createTable(table, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    exist = this.admin.tableExists(table);\n    org.junit.Assert.assertEquals(true, exist);\n}",
        "CUT_1": {
            "Body": "{\n    this.table = table;\n}",
            "ClassName": "TableInputFormatBase",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setHTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return this.table;\n}",
            "ClassName": "TableInputFormatBase",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getHTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.conf.Configuration conf = util.getConfiguration();\n    org.apache.hadoop.fs.Path testDir = org.apache.hadoop.hbase.HBaseTestingUtility.getTestDir(\"testLocalMRIncrementalLoad\");\n    byte[][] startKeys = generateRandomStartKeys(5);\n    try {\n        util.startMiniCluster();\n        org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n        org.apache.hadoop.hbase.client.HTable table = util.createTable(org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.TABLE_NAME, org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.FAMILY_NAME);\n        int numRegions = util.createMultiRegions(util.getConfiguration(), table, org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.FAMILY_NAME, startKeys);\n        org.junit.Assert.assertEquals(\"Should make 5 regions\", numRegions, 5);\n        org.junit.Assert.assertEquals(\"Should start with empty table\", 0, util.countRows(table));\n        util.startMiniMapReduceCluster();\n        runIncrementalPELoad(conf, table, testDir);\n        org.junit.Assert.assertEquals(\"HFOF should not touch actual table\", 0, util.countRows(table));\n        if (shouldChangeRegions) {\n            org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.LOG.info(\"Changing regions in table\");\n            admin.disableTable(table.getTableName());\n            byte[][] newStartKeys = generateRandomStartKeys(15);\n            util.createMultiRegions(util.getConfiguration(), table, org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.FAMILY_NAME, newStartKeys);\n            admin.enableTable(table.getTableName());\n            while ((table.getRegionsInfo().size() != 15) || (!admin.isTableAvailable(table.getTableName()))) {\n                java.lang.Thread.sleep(1000);\n                org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.LOG.info(\"Waiting for new region assignment to happen\");\n            } \n        }\n        new org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles(conf).doBulkLoad(testDir, table);\n        int expectedRows = conf.getInt(\"mapred.map.tasks\", 1) * org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.ROWSPERSPLIT;\n        org.junit.Assert.assertEquals(\"LoadIncrementalHFiles should put expected data in table\", expectedRows, util.countRows(table));\n        java.lang.String tableDigestBefore = util.checksumRows(table);\n        admin.disableTable(org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.TABLE_NAME);\n        while (table.getRegionsInfo().size() != 0) {\n            java.lang.Thread.sleep(1000);\n            org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.LOG.info(\"Waiting for table to disable\");\n        } \n        admin.enableTable(org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.TABLE_NAME);\n        util.waitTableAvailable(org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.TABLE_NAME, 30000);\n        org.junit.Assert.assertEquals(\"Data should remain after reopening of regions\", tableDigestBefore, util.checksumRows(table));\n    } finally {\n        util.shutdownMiniMapReduceCluster();\n        util.shutdownMiniCluster();\n    }\n}",
            "ClassName": "TestHFileOutputFormat",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "doIncrementalLoadTest",
            "NumberOfAsserts": 5,
            "NumberOfAsynchronousWaits": 2,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 42,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 4
        },
        "CUT_4": {
            "Body": "{\n    this.admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration());\n}",
            "ClassName": "TestAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return tableExists(org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n}",
            "ClassName": "HBaseAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tableExists",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testTableExist",
        "NumberOfAsserts": 2,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 9,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    util.startMiniCluster(1);\n    try {\n        runTestAtomicity(20000, 5, 0, 5, 3);\n    } finally {\n        util.shutdownMiniCluster();\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.TestZooKeeper.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "afterAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestZKBasedCloseRegion.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestZKBasedCloseRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "afterAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestZKBasedReopenRegion.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestZKBasedReopenRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "afterAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAcidGuarantees",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testScanAtomicity",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 8,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    testScan(\"obb\", \"opp\", \"opo\");\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.ipc.HBaseServer.Call call = org.apache.hadoop.hbase.ipc.HBaseServer.CurCall.get();\n    if (call != null) {\n        return call.connection.socket.getInetAddress();\n    }\n    return null;\n}",
            "ClassName": "HBaseServer",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getRemoteIp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    this.writer.append(entry.getKey(), entry.getEdit());\n}",
            "ClassName": "SequenceFileLogWriter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "append",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return cacheBlocks;\n}",
            "ClassName": "Scan",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getCacheBlocks",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.HRegion r = org.apache.hadoop.hbase.regionserver.HRegion.newHRegion(org.apache.hadoop.hbase.HTableDescriptor.getTableDir(rootDir, regionInfo.getTableDesc().getName()), this.hlog, this.fs, conf, regionInfo, this.cacheFlusher);\n    r.initialize(null, new org.apache.hadoop.util.Progressable() {\n        public void progress() {\n            addProcessingMessage(regionInfo);\n        }\n    });\n    return r;\n}",
            "ClassName": "HRegionServer",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "instantiateRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    final org.apache.hadoop.hbase.HRegionInfo oldRegionInfo = region.getRegionInfo();\n    final long startTime = java.lang.System.currentTimeMillis();\n    final org.apache.hadoop.hbase.regionserver.HRegion[] newRegions = region.splitRegion(midKey);\n    if (newRegions == null) {\n        return;\n    }\n    org.apache.hadoop.hbase.client.HTable t = null;\n    if (region.getRegionInfo().isMetaTable()) {\n        if (this.root == null) {\n            this.root = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.ROOT_TABLE_NAME);\n        }\n        t = root;\n    } else {\n        if (meta == null) {\n            meta = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n        }\n        t = meta;\n    }\n    oldRegionInfo.setOffline(true);\n    oldRegionInfo.setSplit(true);\n    this.server.removeFromOnlineRegions(oldRegionInfo);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(oldRegionInfo.getRegionName());\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(oldRegionInfo));\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITA_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(newRegions[0].getRegionInfo()));\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITB_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(newRegions[1].getRegionInfo()));\n    t.put(put);\n    for (int i = 0; i < newRegions.length; i++) {\n        put = new org.apache.hadoop.hbase.client.Put(newRegions[i].getRegionName());\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(newRegions[i].getRegionInfo()));\n        t.put(put);\n    }\n    server.reportSplit(oldRegionInfo, newRegions[0].getRegionInfo(), newRegions[1].getRegionInfo());\n    org.apache.hadoop.hbase.regionserver.CompactSplitThread.LOG.info((((((((\"region split, META updated, and report to master all\" + \" successful. Old region=\") + oldRegionInfo.toString()) + \", new regions: \") + newRegions[0].toString()) + \", \") + newRegions[1].toString()) + \". Split took \") + org.apache.hadoop.util.StringUtils.formatTimeDiff(java.lang.System.currentTimeMillis(), startTime));\n}",
            "ClassName": "CompactSplitThread",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "split",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 37,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableInputFormatScan",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testScanOBBToOPP",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.ipc.HBaseServer.Call call = org.apache.hadoop.hbase.ipc.HBaseServer.CurCall.get();\n    if (call != null) {\n        return call.connection.socket.getInetAddress();\n    }\n    return null;\n}",
            "ClassName": "HBaseServer",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getRemoteIp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    this.writer.append(entry.getKey(), entry.getEdit());\n}",
            "ClassName": "SequenceFileLogWriter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "append",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return cacheBlocks;\n}",
            "ClassName": "Scan",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getCacheBlocks",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.HRegion r = org.apache.hadoop.hbase.regionserver.HRegion.newHRegion(org.apache.hadoop.hbase.HTableDescriptor.getTableDir(rootDir, regionInfo.getTableDesc().getName()), this.hlog, this.fs, conf, regionInfo, this.cacheFlusher);\n    r.initialize(null, new org.apache.hadoop.util.Progressable() {\n        public void progress() {\n            addProcessingMessage(regionInfo);\n        }\n    });\n    return r;\n}",
            "ClassName": "HRegionServer",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "instantiateRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    final org.apache.hadoop.hbase.HRegionInfo oldRegionInfo = region.getRegionInfo();\n    final long startTime = java.lang.System.currentTimeMillis();\n    final org.apache.hadoop.hbase.regionserver.HRegion[] newRegions = region.splitRegion(midKey);\n    if (newRegions == null) {\n        return;\n    }\n    org.apache.hadoop.hbase.client.HTable t = null;\n    if (region.getRegionInfo().isMetaTable()) {\n        if (this.root == null) {\n            this.root = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.ROOT_TABLE_NAME);\n        }\n        t = root;\n    } else {\n        if (meta == null) {\n            meta = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n        }\n        t = meta;\n    }\n    oldRegionInfo.setOffline(true);\n    oldRegionInfo.setSplit(true);\n    this.server.removeFromOnlineRegions(oldRegionInfo);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(oldRegionInfo.getRegionName());\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(oldRegionInfo));\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITA_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(newRegions[0].getRegionInfo()));\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITB_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(newRegions[1].getRegionInfo()));\n    t.put(put);\n    for (int i = 0; i < newRegions.length; i++) {\n        put = new org.apache.hadoop.hbase.client.Put(newRegions[i].getRegionName());\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(newRegions[i].getRegionInfo()));\n        t.put(put);\n    }\n    server.reportSplit(oldRegionInfo, newRegions[0].getRegionInfo(), newRegions[1].getRegionInfo());\n    org.apache.hadoop.hbase.regionserver.CompactSplitThread.LOG.info((((((((\"region split, META updated, and report to master all\" + \" successful. Old region=\") + oldRegionInfo.toString()) + \", new regions: \") + newRegions[0].toString()) + \", \") + newRegions[1].toString()) + \". Split took \") + org.apache.hadoop.util.StringUtils.formatTimeDiff(java.lang.System.currentTimeMillis(), startTime));\n}",
            "ClassName": "CompactSplitThread",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "split",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 37,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testMultipleRowMultipleFamily",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 2,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    final byte[] CONTENTS_FAMILY = org.apache.hadoop.hbase.util.Bytes.toBytes(\"contents\");\n    final byte[] value = org.apache.hadoop.hbase.util.Bytes.toBytes(\"abcd\");\n    final byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    final byte[] row2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row2\");\n    byte[] qualifier = org.apache.hadoop.hbase.util.Bytes.toBytes(\"qf1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    org.apache.hadoop.hbase.KeyValue kv = new org.apache.hadoop.hbase.KeyValue(row1, CONTENTS_FAMILY, qualifier, value);\n    boolean ok = true;\n    try {\n        put.add(kv);\n    } catch (java.io.IOException e) {\n        ok = false;\n    }\n    org.junit.Assert.assertEquals(true, ok);\n    kv = new org.apache.hadoop.hbase.KeyValue(row2, CONTENTS_FAMILY, qualifier, value);\n    ok = false;\n    try {\n        put.add(kv);\n    } catch (java.io.IOException e) {\n        ok = true;\n    }\n    org.junit.Assert.assertEquals(true, ok);\n}",
        "CUT_1": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam1\");\n    byte[] fam2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam2\");\n    byte[] qf1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"qualifier\");\n    byte[] val1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"value1\");\n    byte[] val2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"value2\");\n    java.lang.Integer lockId = null;\n    byte[][] families = new byte[][]{ fam1, fam2 };\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, families);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(fam1, qf1, val1);\n    region.put(put);\n    long ts = java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.KeyValue kv = new org.apache.hadoop.hbase.KeyValue(row1, fam2, qf1, ts, org.apache.hadoop.hbase.KeyValue.Type.Put, val2);\n    put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(kv);\n    org.apache.hadoop.hbase.regionserver.Store store = region.getStore(fam1);\n    store.memstore.kvset.size();\n    boolean res = region.checkAndMutate(row1, fam1, qf1, val1, put, lockId, true);\n    junit.framework.Assert.assertEquals(true, res);\n    store.memstore.kvset.size();\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row1);\n    get.addColumn(fam2, qf1);\n    org.apache.hadoop.hbase.KeyValue[] actual = region.get(get, null).raw();\n    org.apache.hadoop.hbase.KeyValue[] expected = new org.apache.hadoop.hbase.KeyValue[]{ kv };\n    junit.framework.Assert.assertEquals(expected.length, actual.length);\n    for (int i = 0; i < actual.length; i++) {\n        junit.framework.Assert.assertEquals(expected[i], actual[i]);\n    }\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testCheckAndPut_ThatPutWasWritten",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 33,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"test_table\");\n    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testFamily\");\n    initHRegion(tableName, getName(), family);\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row111\");\n    byte[] row2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row222\");\n    byte[] row3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row333\");\n    byte[] row4 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row444\");\n    byte[] row5 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row555\");\n    byte[] col1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub111\");\n    byte[] col2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub222\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(10L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row2);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(15L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row3);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(20L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row4);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(30L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row5);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(40L));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row3, row4);\n    scan.setMaxVersions();\n    scan.addColumn(family, col1);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    junit.framework.Assert.assertEquals(false, s.next(results));\n    junit.framework.Assert.assertEquals(0, results.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_StopRow1542",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 34,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    byte[] qf1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"qualifier1\");\n    byte[] qf2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"qualifier2\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam1\");\n    byte[][] families = new byte[][]{ fam1 };\n    long ts1 = java.lang.System.currentTimeMillis();\n    long ts2 = ts1 + 1;\n    long ts3 = ts1 + 2;\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, families);\n    org.apache.hadoop.hbase.client.Put put = null;\n    org.apache.hadoop.hbase.KeyValue kv13 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf1, ts3, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv12 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf1, ts2, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv11 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf1, ts1, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv23 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf2, ts3, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv22 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf2, ts2, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv21 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf2, ts1, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(kv13);\n    put.add(kv12);\n    put.add(kv11);\n    put.add(kv23);\n    put.add(kv22);\n    put.add(kv21);\n    region.put(put);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> expected = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    expected.add(kv13);\n    expected.add(kv12);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row1);\n    scan.addColumn(fam1, qf1);\n    scan.setMaxVersions(MAX_VERSIONS);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> actual = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    org.apache.hadoop.hbase.regionserver.InternalScanner scanner = region.getScanner(scan);\n    boolean hasNext = scanner.next(actual);\n    junit.framework.Assert.assertEquals(false, hasNext);\n    for (int i = 0; i < expected.size(); i++) {\n        junit.framework.Assert.assertEquals(expected.get(i), actual.get(i));\n    }\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_ExplicitColumns_FromMemStore_EnforceVersions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 41,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam1\");\n    byte[] qf1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"qualifier1\");\n    byte[] qf2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"quateslifier2\");\n    long ts1 = 1;\n    long ts2 = ts1 + 1;\n    long ts3 = ts1 + 2;\n    long ts4 = ts1 + 3;\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, fam1);\n    org.apache.hadoop.hbase.KeyValue kv14 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf1, ts4, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv13 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf1, ts3, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv12 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf1, ts2, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv11 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf1, ts1, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv24 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf2, ts4, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv23 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf2, ts3, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv22 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf2, ts2, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv21 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf2, ts1, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.client.Put put = null;\n    put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(kv14);\n    put.add(kv24);\n    region.put(put);\n    region.flushcache();\n    put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(kv23);\n    put.add(kv13);\n    region.put(put);\n    region.flushcache();\n    put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(kv22);\n    put.add(kv12);\n    region.put(put);\n    region.flushcache();\n    put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(kv21);\n    put.add(kv11);\n    region.put(put);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> expected = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    expected.add(kv14);\n    expected.add(kv13);\n    expected.add(kv12);\n    expected.add(kv24);\n    expected.add(kv23);\n    expected.add(kv22);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row1);\n    int versions = 3;\n    scan.setMaxVersions(versions);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> actual = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    org.apache.hadoop.hbase.regionserver.InternalScanner scanner = region.getScanner(scan);\n    boolean hasNext = scanner.next(actual);\n    junit.framework.Assert.assertEquals(false, hasNext);\n    for (int i = 0; i < expected.size(); i++) {\n        junit.framework.Assert.assertEquals(expected.get(i), actual.get(i));\n    }\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_Wildcard_FromMemStoreAndFiles_EnforceVersions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 58,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable\");\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    byte[] qf1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"qualifier1\");\n    byte[] qf2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"qualifier2\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam1\");\n    byte[][] families = new byte[][]{ fam1 };\n    long ts1 = java.lang.System.currentTimeMillis();\n    long ts2 = ts1 + 1;\n    long ts3 = ts1 + 2;\n    java.lang.String method = this.getName();\n    initHRegion(tableName, method, families);\n    org.apache.hadoop.hbase.client.Put put = null;\n    org.apache.hadoop.hbase.KeyValue kv13 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf1, ts3, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv12 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf1, ts2, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv11 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf1, ts1, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv23 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf2, ts3, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv22 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf2, ts2, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    org.apache.hadoop.hbase.KeyValue kv21 = new org.apache.hadoop.hbase.KeyValue(row1, fam1, qf2, ts1, org.apache.hadoop.hbase.KeyValue.Type.Put, null);\n    put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(kv13);\n    put.add(kv12);\n    put.add(kv11);\n    put.add(kv23);\n    put.add(kv22);\n    put.add(kv21);\n    region.put(put);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> expected = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    expected.add(kv13);\n    expected.add(kv12);\n    expected.add(kv23);\n    expected.add(kv22);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row1);\n    scan.addFamily(fam1);\n    scan.setMaxVersions(MAX_VERSIONS);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> actual = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    org.apache.hadoop.hbase.regionserver.InternalScanner scanner = region.getScanner(scan);\n    boolean hasNext = scanner.next(actual);\n    junit.framework.Assert.assertEquals(false, hasNext);\n    for (int i = 0; i < expected.size(); i++) {\n        junit.framework.Assert.assertEquals(expected.get(i), actual.get(i));\n    }\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_Wildcard_FromMemStore_EnforceVersions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 43,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testAddKeyValue",
        "NumberOfAsserts": 2,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 24,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testSingleRowMultipleFamily\");\n    byte[][] ROWS = makeN(org.apache.hadoop.hbase.client.TestFromClientSide.ROW, 3);\n    byte[][] FAMILIES = makeNAscii(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, 10);\n    byte[][] QUALIFIERS = makeN(org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, 10);\n    byte[][] VALUES = makeN(org.apache.hadoop.hbase.client.TestFromClientSide.VALUE, 10);\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, FAMILIES);\n    org.apache.hadoop.hbase.client.Get get;\n    org.apache.hadoop.hbase.client.Scan scan;\n    org.apache.hadoop.hbase.client.Delete delete;\n    org.apache.hadoop.hbase.client.Put put;\n    org.apache.hadoop.hbase.client.Result result;\n    put = new org.apache.hadoop.hbase.client.Put(ROWS[0]);\n    put.add(FAMILIES[4], QUALIFIERS[0], VALUES[0]);\n    ht.put(put);\n    getVerifySingleColumn(ht, ROWS, 0, FAMILIES, 4, QUALIFIERS, 0, VALUES, 0);\n    scanVerifySingleColumn(ht, ROWS, 0, FAMILIES, 4, QUALIFIERS, 0, VALUES, 0);\n    getVerifySingleEmpty(ht, ROWS, 0, FAMILIES, 4, QUALIFIERS, 0);\n    scanVerifySingleEmpty(ht, ROWS, 0, FAMILIES, 4, QUALIFIERS, 0);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    getVerifySingleColumn(ht, ROWS, 0, FAMILIES, 4, QUALIFIERS, 0, VALUES, 0);\n    scanVerifySingleColumn(ht, ROWS, 0, FAMILIES, 4, QUALIFIERS, 0, VALUES, 0);\n    getVerifySingleEmpty(ht, ROWS, 0, FAMILIES, 4, QUALIFIERS, 0);\n    scanVerifySingleEmpty(ht, ROWS, 0, FAMILIES, 4, QUALIFIERS, 0);\n    put = new org.apache.hadoop.hbase.client.Put(ROWS[0]);\n    put.add(FAMILIES[2], QUALIFIERS[2], VALUES[2]);\n    put.add(FAMILIES[2], QUALIFIERS[4], VALUES[4]);\n    put.add(FAMILIES[4], QUALIFIERS[4], VALUES[4]);\n    put.add(FAMILIES[6], QUALIFIERS[6], VALUES[6]);\n    put.add(FAMILIES[6], QUALIFIERS[7], VALUES[7]);\n    put.add(FAMILIES[7], QUALIFIERS[7], VALUES[7]);\n    put.add(FAMILIES[9], QUALIFIERS[0], VALUES[0]);\n    ht.put(put);\n    singleRowGetTest(ht, ROWS, FAMILIES, QUALIFIERS, VALUES);\n    singleRowScanTest(ht, ROWS, FAMILIES, QUALIFIERS, VALUES);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    singleRowGetTest(ht, ROWS, FAMILIES, QUALIFIERS, VALUES);\n    singleRowScanTest(ht, ROWS, FAMILIES, QUALIFIERS, VALUES);\n    put = new org.apache.hadoop.hbase.client.Put(ROWS[0]);\n    put.add(FAMILIES[6], QUALIFIERS[5], VALUES[5]);\n    put.add(FAMILIES[6], QUALIFIERS[8], VALUES[8]);\n    put.add(FAMILIES[6], QUALIFIERS[9], VALUES[9]);\n    put.add(FAMILIES[4], QUALIFIERS[3], VALUES[3]);\n    ht.put(put);\n    delete = new org.apache.hadoop.hbase.client.Delete(ROWS[0]);\n    delete.deleteColumns(FAMILIES[6], QUALIFIERS[7]);\n    ht.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[6], QUALIFIERS[7]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[6], QUALIFIERS[7]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[6], QUALIFIERS[6]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[6], VALUES[6]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[6], QUALIFIERS[8]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[8], VALUES[8]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[6], QUALIFIERS[6]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[6], VALUES[6]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[6], QUALIFIERS[8]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[8], VALUES[8]);\n    delete = new org.apache.hadoop.hbase.client.Delete(ROWS[0]);\n    delete.deleteColumns(FAMILIES[6], QUALIFIERS[8]);\n    ht.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[6], QUALIFIERS[8]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[6], QUALIFIERS[8]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[6], QUALIFIERS[6]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[6], VALUES[6]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[6], QUALIFIERS[9]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[9], VALUES[9]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[6], QUALIFIERS[6]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[6], VALUES[6]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[6], QUALIFIERS[9]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[9], VALUES[9]);\n    delete = new org.apache.hadoop.hbase.client.Delete(ROWS[0]);\n    delete.deleteFamily(FAMILIES[4]);\n    ht.delete(delete);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[4]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[3]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(FAMILIES[4]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[4], QUALIFIERS[4]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[4], QUALIFIERS[3]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(FAMILIES[4]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[2], QUALIFIERS[2]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[2], QUALIFIERS[2], VALUES[2]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[6], QUALIFIERS[9]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[9], VALUES[9]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[6], QUALIFIERS[6]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[6], VALUES[6]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[6], QUALIFIERS[9]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[9], VALUES[9]);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[4]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[3]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(FAMILIES[4]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[4], QUALIFIERS[4]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[4], QUALIFIERS[3]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(FAMILIES[4]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[2], QUALIFIERS[2]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[2], QUALIFIERS[2], VALUES[2]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[6], QUALIFIERS[9]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[9], VALUES[9]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[6], QUALIFIERS[6]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[6], VALUES[6]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[6], QUALIFIERS[9]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[0], FAMILIES[6], QUALIFIERS[9], VALUES[9]);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[0]);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[4], QUALIFIERS[0], VALUES[0]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[2], QUALIFIERS[2]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[2], QUALIFIERS[2], VALUES[2]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(FAMILIES[7]);\n    result = ht.get(get);\n    assertSingleResult(result, ROWS[0], FAMILIES[7], QUALIFIERS[7], VALUES[7]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(FAMILIES[4]);\n    result = ht.get(get);\n    assertDoubleResult(result, ROWS[0], FAMILIES[4], QUALIFIERS[0], VALUES[0], FAMILIES[4], QUALIFIERS[4], VALUES[4]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[4]);\n    result = ht.get(get);\n    assertDoubleResult(result, ROWS[0], FAMILIES[4], QUALIFIERS[0], VALUES[0], FAMILIES[4], QUALIFIERS[4], VALUES[4]);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(FAMILIES[4]);\n    get.addFamily(FAMILIES[7]);\n    result = ht.get(get);\n    assertNResult(result, ROWS[0], FAMILIES, QUALIFIERS, VALUES, new int[][]{ new int[]{ 4, 0, 0 }, new int[]{ 4, 4, 4 }, new int[]{ 7, 7, 7 } });\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addFamily(FAMILIES[2]);\n    get.addFamily(FAMILIES[4]);\n    get.addFamily(FAMILIES[6]);\n    get.addFamily(FAMILIES[7]);\n    result = ht.get(get);\n    assertNResult(result, ROWS[0], FAMILIES, QUALIFIERS, VALUES, new int[][]{ new int[]{ 2, 2, 2 }, new int[]{ 2, 4, 4 }, new int[]{ 4, 0, 0 }, new int[]{ 4, 4, 4 }, new int[]{ 6, 6, 6 }, new int[]{ 6, 7, 7 }, new int[]{ 7, 7, 7 } });\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[2], QUALIFIERS[2]);\n    get.addColumn(FAMILIES[2], QUALIFIERS[4]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[4]);\n    get.addColumn(FAMILIES[6], QUALIFIERS[6]);\n    get.addColumn(FAMILIES[6], QUALIFIERS[7]);\n    get.addColumn(FAMILIES[7], QUALIFIERS[7]);\n    get.addColumn(FAMILIES[7], QUALIFIERS[8]);\n    result = ht.get(get);\n    assertNResult(result, ROWS[0], FAMILIES, QUALIFIERS, VALUES, new int[][]{ new int[]{ 2, 2, 2 }, new int[]{ 2, 4, 4 }, new int[]{ 4, 0, 0 }, new int[]{ 4, 4, 4 }, new int[]{ 6, 6, 6 }, new int[]{ 6, 7, 7 }, new int[]{ 7, 7, 7 } });\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    result = ht.get(get);\n    assertNResult(result, ROWS[0], FAMILIES, QUALIFIERS, VALUES, new int[][]{ new int[]{ 2, 2, 2 }, new int[]{ 2, 4, 4 }, new int[]{ 4, 0, 0 }, new int[]{ 4, 4, 4 }, new int[]{ 6, 6, 6 }, new int[]{ 6, 7, 7 }, new int[]{ 7, 7, 7 }, new int[]{ 9, 0, 0 } });\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[1]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[0]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[3]);\n    get.addColumn(FAMILIES[2], QUALIFIERS[3]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "singleRowGetTest",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 57,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[4], QUALIFIERS[0]);\n    org.apache.hadoop.hbase.client.Result result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[0], FAMILIES[4], QUALIFIERS[0], VALUES[0]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[2], QUALIFIERS[2]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[0], FAMILIES[2], QUALIFIERS[2], VALUES[2]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(FAMILIES[7]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[0], FAMILIES[7], QUALIFIERS[7], VALUES[7]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(FAMILIES[4]);\n    result = getSingleScanResult(ht, scan);\n    assertDoubleResult(result, ROWS[0], FAMILIES[4], QUALIFIERS[0], VALUES[0], FAMILIES[4], QUALIFIERS[4], VALUES[4]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[4], QUALIFIERS[0]);\n    scan.addColumn(FAMILIES[4], QUALIFIERS[4]);\n    result = getSingleScanResult(ht, scan);\n    assertDoubleResult(result, ROWS[0], FAMILIES[4], QUALIFIERS[0], VALUES[0], FAMILIES[4], QUALIFIERS[4], VALUES[4]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(FAMILIES[4]);\n    scan.addFamily(FAMILIES[7]);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, ROWS[0], FAMILIES, QUALIFIERS, VALUES, new int[][]{ new int[]{ 4, 0, 0 }, new int[]{ 4, 4, 4 }, new int[]{ 7, 7, 7 } });\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(FAMILIES[2]);\n    scan.addFamily(FAMILIES[4]);\n    scan.addFamily(FAMILIES[6]);\n    scan.addFamily(FAMILIES[7]);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, ROWS[0], FAMILIES, QUALIFIERS, VALUES, new int[][]{ new int[]{ 2, 2, 2 }, new int[]{ 2, 4, 4 }, new int[]{ 4, 0, 0 }, new int[]{ 4, 4, 4 }, new int[]{ 6, 6, 6 }, new int[]{ 6, 7, 7 }, new int[]{ 7, 7, 7 } });\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[2], QUALIFIERS[2]);\n    scan.addColumn(FAMILIES[2], QUALIFIERS[4]);\n    scan.addColumn(FAMILIES[4], QUALIFIERS[0]);\n    scan.addColumn(FAMILIES[4], QUALIFIERS[4]);\n    scan.addColumn(FAMILIES[6], QUALIFIERS[6]);\n    scan.addColumn(FAMILIES[6], QUALIFIERS[7]);\n    scan.addColumn(FAMILIES[7], QUALIFIERS[7]);\n    scan.addColumn(FAMILIES[7], QUALIFIERS[8]);\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, ROWS[0], FAMILIES, QUALIFIERS, VALUES, new int[][]{ new int[]{ 2, 2, 2 }, new int[]{ 2, 4, 4 }, new int[]{ 4, 0, 0 }, new int[]{ 4, 4, 4 }, new int[]{ 6, 6, 6 }, new int[]{ 6, 7, 7 }, new int[]{ 7, 7, 7 } });\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    result = getSingleScanResult(ht, scan);\n    assertNResult(result, ROWS[0], FAMILIES, QUALIFIERS, VALUES, new int[][]{ new int[]{ 2, 2, 2 }, new int[]{ 2, 4, 4 }, new int[]{ 4, 0, 0 }, new int[]{ 4, 4, 4 }, new int[]{ 6, 6, 6 }, new int[]{ 6, 7, 7 }, new int[]{ 7, 7, 7 }, new int[]{ 9, 0, 0 } });\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[1]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[4], QUALIFIERS[3]);\n    scan.addColumn(FAMILIES[2], QUALIFIERS[3]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "singleRowScanTest",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 57,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(ROWS[ROWIDX]);\n    get.addFamily(FAMILIES[4]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[1]);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[ROWIDX]);\n    get.addFamily(FAMILIES[4]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[2]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[ROWIDX]);\n    get.addFamily(FAMILIES[3]);\n    get.addColumn(FAMILIES[4], QUALIFIERS[2]);\n    get.addFamily(FAMILIES[5]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[ROWIDX + 1]);\n    result = ht.get(get);\n    assertEmptyResult(result);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getVerifySingleEmpty",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.Result result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[ROWIDX], FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX], VALUES[VALUEIDX]);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[ROWIDX]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[ROWIDX], FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX], VALUES[VALUEIDX]);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[ROWIDX], ROWS[ROWIDX + 1]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[ROWIDX], FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX], VALUES[VALUEIDX]);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, ROWS[ROWIDX + 1]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[ROWIDX], FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX], VALUES[VALUEIDX]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(FAMILIES[FAMILYIDX]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[ROWIDX], FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX], VALUES[VALUEIDX]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[ROWIDX], FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX], VALUES[VALUEIDX]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX + 1]);\n    scan.addFamily(FAMILIES[FAMILYIDX]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[ROWIDX], FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX], VALUES[VALUEIDX]);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[FAMILYIDX - 1], QUALIFIERS[QUALIFIERIDX + 1]);\n    scan.addColumn(FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX]);\n    scan.addFamily(FAMILIES[FAMILYIDX + 1]);\n    result = getSingleScanResult(ht, scan);\n    assertSingleResult(result, ROWS[ROWIDX], FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX], VALUES[VALUEIDX]);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "scanVerifySingleColumn",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 33,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(ROWS[ROWIDX + 1]);\n    org.apache.hadoop.hbase.client.Result result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan(ROWS[ROWIDX + 1], ROWS[ROWIDX + 2]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, ROWS[ROWIDX]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(FAMILIES[FAMILYIDX], QUALIFIERS[QUALIFIERIDX + 1]);\n    scan.addFamily(FAMILIES[FAMILYIDX - 1]);\n    result = getSingleScanResult(ht, scan);\n    assertNullResult(result);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "scanVerifySingleEmpty",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testSingleRowMultipleFamily",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 183,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testFilters\");\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    byte[][] ROWS = makeN(org.apache.hadoop.hbase.client.TestFromClientSide.ROW, 10);\n    byte[][] QUALIFIERS = new byte[][]{ org.apache.hadoop.hbase.util.Bytes.toBytes(\"col0-<d2v1>-<d3v2>\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col1-<d2v1>-<d3v2>\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col2-<d2v1>-<d3v2>\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col3-<d2v1>-<d3v2>\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col4-<d2v1>-<d3v2>\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col5-<d2v1>-<d3v2>\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col6-<d2v1>-<d3v2>\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col7-<d2v1>-<d3v2>\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col8-<d2v1>-<d3v2>\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col9-<d2v1>-<d3v2>\") };\n    for (int i = 0; i < 10; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(ROWS[i]);\n        put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, QUALIFIERS[i], org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n        ht.put(put);\n    }\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.apache.hadoop.hbase.filter.Filter filter = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL, new org.apache.hadoop.hbase.filter.RegexStringComparator(\"col[1-5]\"));\n    scan.setFilter(filter);\n    org.apache.hadoop.hbase.client.ResultScanner scanner = ht.getScanner(scan);\n    int expectedIndex = 1;\n    for (org.apache.hadoop.hbase.client.Result result : ht.getScanner(scan)) {\n        org.junit.Assert.assertEquals(result.size(), 1);\n        org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(result.raw()[0].getRow(), ROWS[expectedIndex]));\n        org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(result.raw()[0].getQualifier(), QUALIFIERS[expectedIndex]));\n        expectedIndex++;\n    }\n    org.junit.Assert.assertEquals(expectedIndex, 6);\n    scanner.close();\n}",
        "CUT_1": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testIndexesScanWithOneDeletedRow\");\n    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"family\");\n    java.lang.String method = \"testIndexesScanWithOneDeletedRow\";\n    initHRegion(tableName, method, new org.apache.hadoop.hbase.HBaseConfiguration(), family);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(1L));\n    put.add(family, qual1, 1L, org.apache.hadoop.hbase.util.Bytes.toBytes(1L));\n    region.put(put);\n    region.flushcache();\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(org.apache.hadoop.hbase.util.Bytes.toBytes(1L), 1L, null);\n    region.delete(delete, null, true);\n    put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(2L));\n    put.add(family, qual1, 2L, org.apache.hadoop.hbase.util.Bytes.toBytes(2L));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan idxScan = new org.apache.hadoop.hbase.client.Scan();\n    idxScan.addFamily(family);\n    idxScan.setFilter(new org.apache.hadoop.hbase.filter.FilterList(org.apache.hadoop.hbase.filter.FilterList.Operator.MUST_PASS_ALL, java.util.Arrays.<org.apache.hadoop.hbase.filter.Filter>asList(new org.apache.hadoop.hbase.filter.SingleColumnValueFilter(family, qual1, org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(0L))), new org.apache.hadoop.hbase.filter.SingleColumnValueFilter(family, qual1, org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.LESS_OR_EQUAL, new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(3L))))));\n    org.apache.hadoop.hbase.regionserver.InternalScanner scanner = region.getScanner(idxScan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> res = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    while (scanner.next(res));\n    junit.framework.Assert.assertEquals(1L, res.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testIndexesScanWithOneDeletedRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.TestZooKeeper.conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(row), org.apache.hadoop.hbase.util.Bytes.toBytes(family), org.apache.hadoop.hbase.util.Bytes.toBytes(qualifier), timestamp, type, org.apache.hadoop.hbase.util.Bytes.toBytes(value));\n}",
            "ClassName": "KeyValueTestUtil",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "create",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"test_table\");\n    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testFamily\");\n    initHRegion(tableName, getName(), family);\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row111\");\n    byte[] row2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row222\");\n    byte[] row3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row333\");\n    byte[] row4 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row444\");\n    byte[] row5 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row555\");\n    byte[] col1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub111\");\n    byte[] col2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub222\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(10L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row2);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(15L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row3);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(20L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row4);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(30L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row5);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(40L));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row3, row4);\n    scan.setMaxVersions();\n    scan.addColumn(family, col1);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    junit.framework.Assert.assertEquals(false, s.next(results));\n    junit.framework.Assert.assertEquals(0, results.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_StopRow1542",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 34,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 2,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testFilters",
        "NumberOfAsserts": 4,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 25,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    conf.setBoolean(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HBASE_SKIP_ERRORS, false);\n    generateHLogs(-1);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    org.apache.hadoop.fs.FileStatus[] archivedLogs = fs.listStatus(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir);\n    org.junit.Assert.assertEquals(\"wrong number of files in the archive log\", org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.NUM_WRITERS, archivedLogs.length);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.removeAll(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions);\n    for (int i = 0; i < 100; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.add(\"region__\" + i);\n    }\n    generateHLogs(1, 100, -1);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    fs.rename(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir);\n    org.apache.hadoop.fs.Path firstSplitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME) + \".first\");\n    org.apache.hadoop.fs.Path splitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME));\n    fs.rename(splitPath, firstSplitPath);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    org.junit.Assert.assertEquals(0, compareHLogSplitDirs(firstSplitPath, splitPath));\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSplittingLargeNumberOfRegionsConsistency",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    for (int i = 0; i < writers; i++) {\n        writer[i] = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + i), conf);\n        for (int j = 0; j < entries; j++) {\n            int prefix = 0;\n            for (java.lang.String region : org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions) {\n                java.lang.String row_key = ((region + (prefix++)) + i) + j;\n                appendEntry(writer[i], org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, region.getBytes(), row_key.getBytes(), org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.FAMILY, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.QUALIFIER, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.VALUE, seq);\n            }\n        }\n        if (i != leaveOpen) {\n            writer[i].close();\n            flushToConsole(\"Closing writer \" + i);\n        }\n    }\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "generateHLogs",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.flushlogentries\", 1);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setStrings(\"hbase.rootdir\", org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir.toString());\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setClass(\"hbase.regionserver.hlog.writer.impl\", org.apache.hadoop.hbase.regionserver.wal.InstrumentedSequenceFileLogWriter.class, org.apache.hadoop.hbase.regionserver.wal.HLog.Writer.class);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.startMiniDFSCluster(2);\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Writer writer = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + suffix), conf);\n    if (closeFile)\n        writer.close();\n\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "injectEmptyFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.shutdownMiniDFSCluster();\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHLogSplit",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testLogsGetArchivedAfterSplit",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 8,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    final byte[] CONTENTS_FAMILY = org.apache.hadoop.hbase.util.Bytes.toBytes(\"contents\");\n    final byte[] SMALL_FAMILY = org.apache.hadoop.hbase.util.Bytes.toBytes(\"smallfam\");\n    final byte[] value = org.apache.hadoop.hbase.util.Bytes.toBytes(\"abcd\");\n    final int NB_BATCH_ROWS = 10;\n    org.apache.hadoop.hbase.client.HTable table = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowsPutBufferedOneFlush\"), new byte[][]{ CONTENTS_FAMILY, SMALL_FAMILY });\n    table.setAutoFlush(false);\n    java.util.ArrayList<org.apache.hadoop.hbase.client.Put> rowsUpdate = new java.util.ArrayList<org.apache.hadoop.hbase.client.Put>();\n    for (int i = 0; i < (NB_BATCH_ROWS * 10); i++) {\n        byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + i);\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n        put.add(CONTENTS_FAMILY, null, value);\n        rowsUpdate.add(put);\n    }\n    table.put(rowsUpdate);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(CONTENTS_FAMILY);\n    org.apache.hadoop.hbase.client.ResultScanner scanner = table.getScanner(scan);\n    int nbRows = 0;\n    for (@java.lang.SuppressWarnings(\"unused\")\n    org.apache.hadoop.hbase.client.Result row : scanner)\n        nbRows++;\n\n    org.junit.Assert.assertEquals(0, nbRows);\n    scanner.close();\n    table.flushCommits();\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(CONTENTS_FAMILY);\n    scanner = table.getScanner(scan);\n    nbRows = 0;\n    for (@java.lang.SuppressWarnings(\"unused\")\n    org.apache.hadoop.hbase.client.Result row : scanner)\n        nbRows++;\n\n    org.junit.Assert.assertEquals(NB_BATCH_ROWS * 10, nbRows);\n}",
        "CUT_1": {
            "Body": "{\n    java.lang.Runnable runnable = new java.lang.Runnable() {\n        public void run() {\n            try {\n                org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n                org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n                scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n                org.apache.hadoop.hbase.client.ResultScanner s = t.getScanner(scan);\n                s.close();\n            } catch (java.io.IOException e) {\n                LOG.fatal(\"could not re-open meta table because\", e);\n                junit.framework.Assert.fail();\n            }\n            org.apache.hadoop.hbase.client.ResultScanner scanner = null;\n            try {\n                org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n                scan.addFamily(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n                scanner = table.getScanner(scan);\n                LOG.info(\"Obtained scanner \" + scanner);\n                for (org.apache.hadoop.hbase.client.Result r : scanner) {\n                    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(r.getRow(), row));\n                    junit.framework.Assert.assertEquals(1, r.size());\n                    byte[] bytes = r.value();\n                    junit.framework.Assert.assertNotNull(bytes);\n                    junit.framework.Assert.assertTrue(tableName.equals(org.apache.hadoop.hbase.util.Bytes.toString(bytes)));\n                }\n                LOG.info(\"Success!\");\n            } catch (java.lang.Exception e) {\n                e.printStackTrace();\n                junit.framework.Assert.fail();\n            } finally {\n                if (scanner != null) {\n                    LOG.info(\"Closing scanner \" + scanner);\n                    scanner.close();\n                }\n            }\n        }\n    };\n    return new java.lang.Thread(runnable);\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startVerificationThread",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 40,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"test_table\");\n    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testFamily\");\n    initHRegion(tableName, getName(), family);\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row111\");\n    byte[] row2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row222\");\n    byte[] row3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row333\");\n    byte[] row4 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row444\");\n    byte[] row5 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row555\");\n    byte[] col1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub111\");\n    byte[] col2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub222\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(10L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row2);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(15L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row3);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(20L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row4);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(30L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row5);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(40L));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row3, row4);\n    scan.setMaxVersions();\n    scan.addColumn(family, col1);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    junit.framework.Assert.assertEquals(false, s.next(results));\n    junit.framework.Assert.assertEquals(0, results.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_StopRow1542",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 34,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.TestZooKeeper.conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(getConfiguration(), tableName);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.ResultScanner resScan = table.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result res : resScan) {\n        org.apache.hadoop.hbase.client.Delete del = new org.apache.hadoop.hbase.client.Delete(res.getRow());\n        table.delete(del);\n    }\n    return table;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "truncateTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 10,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 3,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testRowsPutBufferedOneFlush",
        "NumberOfAsserts": 2,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 36,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    int numRows = 10;\n    int numColsPerRow = 2000;\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testJiraTest867\");\n    byte[][] ROWS = makeN(org.apache.hadoop.hbase.client.TestFromClientSide.ROW, numRows);\n    byte[][] QUALIFIERS = makeN(org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, numColsPerRow);\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    for (int i = 0; i < numRows; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(ROWS[i]);\n        for (int j = 0; j < numColsPerRow; j++) {\n            put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, QUALIFIERS[j], QUALIFIERS[j]);\n        }\n        org.junit.Assert.assertTrue((((\"Put expected to contain \" + numColsPerRow) + \" columns but \") + \"only contains \") + put.size(), put.size() == numColsPerRow);\n        ht.put(put);\n    }\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(ROWS[numRows - 1]);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertNumKeys(result, numColsPerRow);\n    org.apache.hadoop.hbase.KeyValue[] keys = result.sorted();\n    for (int i = 0; i < result.size(); i++) {\n        assertKey(keys[i], ROWS[numRows - 1], org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, QUALIFIERS[i], QUALIFIERS[i]);\n    }\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.ResultScanner scanner = ht.getScanner(scan);\n    int rowCount = 0;\n    while ((result = scanner.next()) != null) {\n        assertNumKeys(result, numColsPerRow);\n        org.apache.hadoop.hbase.KeyValue[] kvs = result.sorted();\n        for (int i = 0; i < numColsPerRow; i++) {\n            assertKey(kvs[i], ROWS[rowCount], org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, QUALIFIERS[i], QUALIFIERS[i]);\n        }\n        rowCount++;\n    } \n    scanner.close();\n    org.junit.Assert.assertTrue((((\"Expected to scan \" + numRows) + \" rows but actually scanned \") + rowCount) + \" rows\", rowCount == numRows);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    get = new org.apache.hadoop.hbase.client.Get(ROWS[numRows - 1]);\n    result = ht.get(get);\n    assertNumKeys(result, numColsPerRow);\n    keys = result.sorted();\n    for (int i = 0; i < result.size(); i++) {\n        assertKey(keys[i], ROWS[numRows - 1], org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, QUALIFIERS[i], QUALIFIERS[i]);\n    }\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scanner = ht.getScanner(scan);\n    rowCount = 0;\n    while ((result = scanner.next()) != null) {\n        assertNumKeys(result, numColsPerRow);\n        org.apache.hadoop.hbase.KeyValue[] kvs = result.sorted();\n        for (int i = 0; i < numColsPerRow; i++) {\n            assertKey(kvs[i], ROWS[rowCount], org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, QUALIFIERS[i], QUALIFIERS[i]);\n        }\n        rowCount++;\n    } \n    scanner.close();\n    org.junit.Assert.assertTrue((((\"Expected to scan \" + numRows) + \" rows but actually scanned \") + rowCount) + \" rows\", rowCount == numRows);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.util.TestMergeTool.FAMILY);\n    org.apache.hadoop.hbase.regionserver.InternalScanner scanner = merged.getScanner(scan);\n    try {\n        java.util.List<org.apache.hadoop.hbase.KeyValue> testRes = null;\n        while (true) {\n            testRes = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n            boolean hasNext = scanner.next(testRes);\n            if (!hasNext) {\n                break;\n            }\n        } \n    } finally {\n        scanner.close();\n    }\n    for (int i = 0; i < upperbound; i++) {\n        for (int j = 0; j < rows[i].length; j++) {\n            org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(rows[i][j]);\n            get.addFamily(org.apache.hadoop.hbase.util.TestMergeTool.FAMILY);\n            org.apache.hadoop.hbase.client.Result result = merged.get(get, null);\n            junit.framework.Assert.assertEquals(1, result.size());\n            byte[] bytes = result.sorted()[0].getValue();\n            junit.framework.Assert.assertNotNull(org.apache.hadoop.hbase.util.Bytes.toStringBinary(rows[i][j]), bytes);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(bytes, rows[i][j]));\n        }\n    }\n}",
            "ClassName": "TestMergeTool",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "verifyMerge",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 28,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.ResultScanner s = t.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        rows++;\n    }\n    s.close();\n    org.apache.hadoop.hbase.master.TestMasterTransitions.LOG.info(\"Counted=\" + rows);\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "count",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(this.conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    java.util.List<byte[]> rows = new java.util.ArrayList<byte[]>();\n    org.apache.hadoop.hbase.client.ResultScanner s = t.getScanner(new org.apache.hadoop.hbase.client.Scan());\n    for (org.apache.hadoop.hbase.client.Result result : s) {\n        org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(result.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER));\n        org.apache.hadoop.hbase.HTableDescriptor desc = info.getTableDesc();\n        if (org.apache.hadoop.hbase.util.Bytes.compareTo(desc.getName(), tableName) == 0) {\n            LOG.info(\"getMetaTableRows: row -> \" + org.apache.hadoop.hbase.util.Bytes.toStringBinary(result.getRow()));\n            rows.add(result.getRow());\n        }\n    }\n    s.close();\n    return rows;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getMetaTableRows",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 15,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        rows++;\n    }\n    s.close();\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "countOfMetaRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 8,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testJiraTest867",
        "NumberOfAsserts": 3,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 57,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    int initialCount = 10;\n    org.apache.hadoop.hbase.client.HTable t = org.apache.hadoop.hbase.client.TestScannerTimeout.TEST_UTIL.createTable(org.apache.hadoop.hbase.util.Bytes.toBytes(\"t\"), someBytes);\n    for (int i = 0; i < initialCount; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(i));\n        put.add(someBytes, someBytes, someBytes);\n        t.put(put);\n    }\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.ResultScanner r = t.getScanner(scan);\n    int count = 0;\n    try {\n        org.apache.hadoop.hbase.client.Result res = r.next();\n        while (res != null) {\n            count++;\n            if (count == 5) {\n                java.lang.Thread.sleep(1500);\n            }\n            res = r.next();\n        } \n    } catch (org.apache.hadoop.hbase.client.ScannerTimeoutException e) {\n        LOG.info(\"Got the timeout \" + e.getMessage(), e);\n        return;\n    }\n    org.junit.Assert.fail(\"We should be timing out\");\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    final int COUNT = 5;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(getName());\n    for (int i = 0; i < COUNT; i++) {\n        byte[] regionName = org.apache.hadoop.hbase.HRegionInfo.createRegionName(tableName, org.apache.hadoop.hbase.util.Bytes.toBytes(i == 0 ? \"\" : java.lang.Integer.toString(i)), java.lang.Long.toString(java.lang.System.currentTimeMillis()), true);\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(regionName);\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER, org.apache.hadoop.hbase.util.Bytes.toBytes(\"localhost:1234\"));\n        t.put(put);\n    }\n    long sleepTime = conf.getLong(\"hbase.master.meta.thread.rescanfrequency\", 10000);\n    int tries = conf.getInt(\"hbase.client.retries.number\", 5);\n    int count = 0;\n    do {\n        tries -= 1;\n        try {\n            java.lang.Thread.sleep(sleepTime);\n        } catch (java.lang.InterruptedException e) {\n        }\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITA_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITB_QUALIFIER);\n        org.apache.hadoop.hbase.client.ResultScanner scanner = t.getScanner(scan);\n        try {\n            count = 0;\n            org.apache.hadoop.hbase.client.Result r;\n            while ((r = scanner.next()) != null) {\n                if (!r.isEmpty()) {\n                    count += 1;\n                }\n            } \n        } finally {\n            scanner.close();\n        }\n    } while ((count != 0) && (tries >= 0) );\n    junit.framework.Assert.assertTrue(tries >= 0);\n    junit.framework.Assert.assertEquals(0, count);\n}",
            "ClassName": "TestEmptyMetaInfo",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testEmptyMetaInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 41,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.ResultScanner results = table.getScanner(scan);\n    int count = 0;\n    for (@java.lang.SuppressWarnings(\"unused\")\n    org.apache.hadoop.hbase.client.Result res : results) {\n        count++;\n    }\n    results.close();\n    return count;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "countRows",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 11,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.TestZooKeeper.conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(getConfiguration(), tableName);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.ResultScanner resScan = table.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result res : resScan) {\n        org.apache.hadoop.hbase.client.Delete del = new org.apache.hadoop.hbase.client.Delete(res.getRow());\n        table.delete(del);\n    }\n    return table;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "truncateTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 10,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    this.server = cluster.getRegionServerThreads().get(0).getRegionServer();\n    this.log = server.getLog();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    for (int i = 1; i <= 256; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + java.lang.String.format(\"%1$04d\", i)));\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, value);\n        table.put(put);\n        if ((i % 32) == 0) {\n            try {\n                java.lang.Thread.sleep(2000);\n            } catch (java.lang.InterruptedException e) {\n            }\n        }\n    }\n}",
            "ClassName": "TestLogRolling",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startAndWriteData",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "ClassName": "TestScannerTimeout",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 3,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "test2481",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 1,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 26,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 2,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    runTest(\"testSimpleLoad\", new byte[][][]{ new byte[][]{ org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"cccc\") }, new byte[][]{ org.apache.hadoop.hbase.util.Bytes.toBytes(\"ddd\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"ooo\") } });\n}",
        "CUT_1": {
            "Body": "{\n    return new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(row), org.apache.hadoop.hbase.util.Bytes.toBytes(family), org.apache.hadoop.hbase.util.Bytes.toBytes(qualifier), timestamp, type, org.apache.hadoop.hbase.util.Bytes.toBytes(value));\n}",
            "ClassName": "KeyValueTestUtil",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "create",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    super.setUp();\n    data = org.apache.hadoop.hbase.util.Bytes.toBytes(\"data\");\n    row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam1\");\n    col1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"col1\");\n    row2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row2\");\n    fam2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam2\");\n    col2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"col2\");\n    col3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"col3\");\n    col4 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"col4\");\n    col5 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"col5\");\n}",
            "ClassName": "TestKeyValueHeap",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 13,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    setValue(org.apache.hadoop.hbase.util.Bytes.toBytes(key), org.apache.hadoop.hbase.util.Bytes.toBytes(value));\n}",
            "ClassName": "HColumnDescriptor",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setValue",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.HRegionInfo a = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(\"a\"), null, null);\n    org.apache.hadoop.hbase.HRegionInfo b = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(\"b\"), null, null);\n    junit.framework.Assert.assertTrue(a.compareTo(b) != 0);\n    org.apache.hadoop.hbase.HTableDescriptor t = new org.apache.hadoop.hbase.HTableDescriptor(\"t\");\n    byte[] midway = org.apache.hadoop.hbase.util.Bytes.toBytes(\"midway\");\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, null, midway);\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, midway, null);\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n    junit.framework.Assert.assertTrue(b.compareTo(a) > 0);\n    junit.framework.Assert.assertEquals(a, a);\n    junit.framework.Assert.assertTrue(a.compareTo(a) == 0);\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"a\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"d\"));\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"e\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"g\"));\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"dddd\"));\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"e\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"g\"));\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"dddd\"));\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"eeee\"));\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n}",
            "ClassName": "TestCompare",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testHRegionInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.KeyValue rowA = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable,www.hbase.org/,1234\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), java.lang.Long.MAX_VALUE, ((byte[]) (null)));\n    org.apache.hadoop.hbase.KeyValue rowB = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable,www.hbase.org/%20,99999\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), java.lang.Long.MAX_VALUE, ((byte[]) (null)));\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.META_COMPARATOR.compare(rowA, rowB) < 0);\n    rowA = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable,,1234\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), java.lang.Long.MAX_VALUE, ((byte[]) (null)));\n    rowB = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable,$www.hbase.org/,99999\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), java.lang.Long.MAX_VALUE, ((byte[]) (null)));\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.META_COMPARATOR.compare(rowA, rowB) < 0);\n    rowA = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\".META.,testtable,www.hbase.org/,1234,4321\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), java.lang.Long.MAX_VALUE, ((byte[]) (null)));\n    rowB = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\".META.,testtable,www.hbase.org/%20,99999,99999\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), java.lang.Long.MAX_VALUE, ((byte[]) (null)));\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.ROOT_COMPARATOR.compare(rowA, rowB) < 0);\n}",
            "ClassName": "TestKeyValue",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testKeyValueBorderCases",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 11,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestLoadIncrementalHFiles",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testSimpleLoad",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    conf.setBoolean(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HBASE_SKIP_ERRORS, true);\n    generateHLogs(java.lang.Integer.MAX_VALUE);\n    corruptHLog(new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + \"5\"), org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.Corruptions.APPEND_GARBAGE, true, fs);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    for (java.lang.String region : org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions) {\n        org.apache.hadoop.fs.Path logfile = getLogForRegion(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, region);\n        org.junit.Assert.assertEquals(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.NUM_WRITERS * org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.ENTRIES, countHLog(logfile, fs, conf));\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.removeAll(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions);\n    for (int i = 0; i < 100; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.add(\"region__\" + i);\n    }\n    generateHLogs(1, 100, -1);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    fs.rename(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir);\n    org.apache.hadoop.fs.Path firstSplitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME) + \".first\");\n    org.apache.hadoop.fs.Path splitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME));\n    fs.rename(splitPath, firstSplitPath);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    org.junit.Assert.assertEquals(0, compareHLogSplitDirs(firstSplitPath, splitPath));\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSplittingLargeNumberOfRegionsConsistency",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    for (int i = 0; i < writers; i++) {\n        writer[i] = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + i), conf);\n        for (int j = 0; j < entries; j++) {\n            int prefix = 0;\n            for (java.lang.String region : org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions) {\n                java.lang.String row_key = ((region + (prefix++)) + i) + j;\n                appendEntry(writer[i], org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, region.getBytes(), row_key.getBytes(), org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.FAMILY, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.QUALIFIER, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.VALUE, seq);\n            }\n        }\n        if (i != leaveOpen) {\n            writer[i].close();\n            flushToConsole(\"Closing writer \" + i);\n        }\n    }\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "generateHLogs",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.flushlogentries\", 1);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setStrings(\"hbase.rootdir\", org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir.toString());\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setClass(\"hbase.regionserver.hlog.writer.impl\", org.apache.hadoop.hbase.regionserver.wal.InstrumentedSequenceFileLogWriter.class, org.apache.hadoop.hbase.regionserver.wal.HLog.Writer.class);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.startMiniDFSCluster(2);\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Writer writer = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + suffix), conf);\n    if (closeFile)\n        writer.close();\n\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "injectEmptyFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.shutdownMiniDFSCluster();\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHLogSplit",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 1,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testTralingGarbageCorruptionFileSkipErrorsPasses",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 11,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    testScan(\"obb\", \"qpp\", \"qpo\");\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.ipc.HBaseServer.Call call = org.apache.hadoop.hbase.ipc.HBaseServer.CurCall.get();\n    if (call != null) {\n        return call.connection.socket.getInetAddress();\n    }\n    return null;\n}",
            "ClassName": "HBaseServer",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getRemoteIp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_2": {
            "Body": "{\n    this.writer.append(entry.getKey(), entry.getEdit());\n}",
            "ClassName": "SequenceFileLogWriter",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "append",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return cacheBlocks;\n}",
            "ClassName": "Scan",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getCacheBlocks",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.HRegion r = org.apache.hadoop.hbase.regionserver.HRegion.newHRegion(org.apache.hadoop.hbase.HTableDescriptor.getTableDir(rootDir, regionInfo.getTableDesc().getName()), this.hlog, this.fs, conf, regionInfo, this.cacheFlusher);\n    r.initialize(null, new org.apache.hadoop.util.Progressable() {\n        public void progress() {\n            addProcessingMessage(regionInfo);\n        }\n    });\n    return r;\n}",
            "ClassName": "HRegionServer",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "instantiateRegion",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    final org.apache.hadoop.hbase.HRegionInfo oldRegionInfo = region.getRegionInfo();\n    final long startTime = java.lang.System.currentTimeMillis();\n    final org.apache.hadoop.hbase.regionserver.HRegion[] newRegions = region.splitRegion(midKey);\n    if (newRegions == null) {\n        return;\n    }\n    org.apache.hadoop.hbase.client.HTable t = null;\n    if (region.getRegionInfo().isMetaTable()) {\n        if (this.root == null) {\n            this.root = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.ROOT_TABLE_NAME);\n        }\n        t = root;\n    } else {\n        if (meta == null) {\n            meta = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n        }\n        t = meta;\n    }\n    oldRegionInfo.setOffline(true);\n    oldRegionInfo.setSplit(true);\n    this.server.removeFromOnlineRegions(oldRegionInfo);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(oldRegionInfo.getRegionName());\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(oldRegionInfo));\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER, org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITA_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(newRegions[0].getRegionInfo()));\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITB_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(newRegions[1].getRegionInfo()));\n    t.put(put);\n    for (int i = 0; i < newRegions.length; i++) {\n        put = new org.apache.hadoop.hbase.client.Put(newRegions[i].getRegionName());\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(newRegions[i].getRegionInfo()));\n        t.put(put);\n    }\n    server.reportSplit(oldRegionInfo, newRegions[0].getRegionInfo(), newRegions[1].getRegionInfo());\n    org.apache.hadoop.hbase.regionserver.CompactSplitThread.LOG.info((((((((\"region split, META updated, and report to master all\" + \" successful. Old region=\") + oldRegionInfo.toString()) + \", new regions: \") + newRegions[0].toString()) + \", \") + newRegions[1].toString()) + \". Split took \") + org.apache.hadoop.util.StringUtils.formatTimeDiff(java.lang.System.currentTimeMillis(), startTime));\n}",
            "ClassName": "CompactSplitThread",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "split",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 37,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableInputFormatScan",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testScanOBBToQPP",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    final byte[] name = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testTableNotFoundExceptionWithATable\");\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.createTable(name, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.createTable(name, org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.msginterval\", 100);\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration().setInt(\"hbase.client.pause\", 250);\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration().setInt(\"hbase.client.retries.number\", 6);\n    org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    this.admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration());\n}",
            "ClassName": "TestAdmin",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        rows++;\n    }\n    s.close();\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "countOfMetaRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    final int COUNT = 5;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(getName());\n    for (int i = 0; i < COUNT; i++) {\n        byte[] regionName = org.apache.hadoop.hbase.HRegionInfo.createRegionName(tableName, org.apache.hadoop.hbase.util.Bytes.toBytes(i == 0 ? \"\" : java.lang.Integer.toString(i)), java.lang.Long.toString(java.lang.System.currentTimeMillis()), true);\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(regionName);\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER, org.apache.hadoop.hbase.util.Bytes.toBytes(\"localhost:1234\"));\n        t.put(put);\n    }\n    long sleepTime = conf.getLong(\"hbase.master.meta.thread.rescanfrequency\", 10000);\n    int tries = conf.getInt(\"hbase.client.retries.number\", 5);\n    int count = 0;\n    do {\n        tries -= 1;\n        try {\n            java.lang.Thread.sleep(sleepTime);\n        } catch (java.lang.InterruptedException e) {\n        }\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITA_QUALIFIER);\n        scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SPLITB_QUALIFIER);\n        org.apache.hadoop.hbase.client.ResultScanner scanner = t.getScanner(scan);\n        try {\n            count = 0;\n            org.apache.hadoop.hbase.client.Result r;\n            while ((r = scanner.next()) != null) {\n                if (!r.isEmpty()) {\n                    count += 1;\n                }\n            } \n        } finally {\n            scanner.close();\n        }\n    } while ((count != 0) && (tries >= 0) );\n    junit.framework.Assert.assertTrue(tries >= 0);\n    junit.framework.Assert.assertEquals(0, count);\n}",
            "ClassName": "TestEmptyMetaInfo",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testEmptyMetaInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 41,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "ClassName": "TestAdmin",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testTableNotFoundExceptionWithATable",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 5,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    LOG.info(\"Starting testRegionCacheDeSerialization\");\n    final byte[] TABLENAME = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testCachePrewarm2\");\n    final byte[] FAMILY = org.apache.hadoop.hbase.util.Bytes.toBytes(\"family\");\n    org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration();\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLENAME, FAMILY);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, TABLENAME);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createMultiRegions(table, FAMILY);\n    org.apache.hadoop.fs.Path tempPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.HBaseTestingUtility.getTestDir(), \"regions.dat\");\n    final java.lang.String tempFileName = tempPath.toString();\n    java.io.FileOutputStream fos = new java.io.FileOutputStream(tempFileName);\n    java.io.DataOutputStream dos = new java.io.DataOutputStream(fos);\n    table.serializeRegionInfo(dos);\n    dos.flush();\n    dos.close();\n    java.io.FileInputStream fis = new java.io.FileInputStream(tempFileName);\n    java.io.DataInputStream dis = new java.io.DataInputStream(fis);\n    java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> deserRegions = table.deserializeRegionInfo(dis);\n    dis.close();\n    java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> loadedRegions = table.getRegionsInfo();\n    table.getConnection().clearRegionCache();\n    table.getConnection().prewarmRegionCache(table.getTableName(), deserRegions);\n    org.junit.Assert.assertEquals(\"Number of cached region is incorrect\", org.apache.hadoop.hbase.client.HConnectionManager.getCachedRegionCount(conf, TABLENAME), loadedRegions.size());\n    for (java.util.Map.Entry<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> e : loadedRegions.entrySet()) {\n        org.apache.hadoop.hbase.HRegionInfo hri = e.getKey();\n        org.junit.Assert.assertTrue(org.apache.hadoop.hbase.client.HConnectionManager.isRegionCached(conf, hri.getTableDesc().getName(), hri.getStartKey()));\n    }\n    java.io.File f = new java.io.File(tempFileName);\n    f.delete();\n    LOG.info(\"Finishing testRegionCacheDeSerialization\");\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.TestZooKeeper.conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    final java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> regionMap = new java.util.TreeMap<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress>();\n    org.apache.hadoop.hbase.client.MetaScanner.MetaScannerVisitor visitor = new org.apache.hadoop.hbase.client.MetaScanner.MetaScannerVisitor() {\n        public boolean processRow(org.apache.hadoop.hbase.client.Result rowResult) throws java.io.IOException {\n            org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(rowResult.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER));\n            if (!org.apache.hadoop.hbase.util.Bytes.equals(info.getTableDesc().getName(), getTableName())) {\n                return false;\n            }\n            org.apache.hadoop.hbase.HServerAddress server = new org.apache.hadoop.hbase.HServerAddress();\n            byte[] value = rowResult.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n            if ((value != null) && (value.length > 0)) {\n                java.lang.String address = org.apache.hadoop.hbase.util.Bytes.toString(value);\n                server = new org.apache.hadoop.hbase.HServerAddress(address);\n            }\n            if (!(info.isOffline() || info.isSplit())) {\n                regionMap.put(new org.apache.hadoop.hbase.client.UnmodifyableHRegionInfo(info), server);\n            }\n            return true;\n        }\n    };\n    org.apache.hadoop.hbase.client.MetaScanner.metaScan(configuration, visitor, tableName);\n    return regionMap;\n}",
            "ClassName": "HTable",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getRegionsInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    this.server = cluster.getRegionServerThreads().get(0).getRegionServer();\n    this.log = server.getLog();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    for (int i = 1; i <= 256; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + java.lang.String.format(\"%1$04d\", i)));\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, value);\n        table.put(put);\n        if ((i % 32) == 0) {\n            try {\n                java.lang.Thread.sleep(2000);\n            } catch (java.lang.InterruptedException e) {\n            }\n        }\n    }\n}",
            "ClassName": "TestLogRolling",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startAndWriteData",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(this.conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    java.util.List<byte[]> rows = new java.util.ArrayList<byte[]>();\n    org.apache.hadoop.hbase.client.ResultScanner s = t.getScanner(new org.apache.hadoop.hbase.client.Scan());\n    for (org.apache.hadoop.hbase.client.Result result : s) {\n        org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(result.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER));\n        org.apache.hadoop.hbase.HTableDescriptor desc = info.getTableDesc();\n        if (org.apache.hadoop.hbase.util.Bytes.compareTo(desc.getName(), tableName) == 0) {\n            LOG.info(\"getMetaTableRows: row -> \" + org.apache.hadoop.hbase.util.Bytes.toStringBinary(result.getRow()));\n            rows.add(result.getRow());\n        }\n    }\n    s.close();\n    return rows;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getMetaTableRows",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 15,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 1,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testRegionCacheDeSerialization",
        "NumberOfAsserts": 2,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 13,
        "NumberOfLines": 31,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testJiraTest33\");\n    byte[][] VALUES = makeNAscii(org.apache.hadoop.hbase.client.TestFromClientSide.VALUE, 7);\n    long[] STAMPS = makeStamps(7);\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, 10);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0], VALUES[0]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3], VALUES[3]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    ht.put(put);\n    getVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 5);\n    getVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 2);\n    getVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 4, 5);\n    getVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 2, 3);\n    scanVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 5);\n    scanVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 2);\n    scanVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 4, 5);\n    scanVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 2, 3);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    getVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 5);\n    getVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 2);\n    getVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 4, 5);\n    getVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 2, 3);\n    scanVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 5);\n    scanVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 2);\n    scanVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 4, 5);\n    scanVersionRangeAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 2, 3);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration());\n    admin.split(t.getTableName());\n    java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> regions = waitOnSplit(t);\n    org.junit.Assert.assertTrue(regions.size() > 1);\n    return regions;\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "splitTable",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(family, qualifier);\n    get.setMaxVersions(java.lang.Integer.MAX_VALUE);\n    get.setTimeRange(stamps[start], stamps[end] + 1);\n    org.apache.hadoop.hbase.client.Result result = ht.get(get);\n    assertNResult(result, row, family, qualifier, stamps, values, start, end);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getVersionRangeAndVerify",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testJiraTest33",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 31,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = new org.apache.hadoop.hbase.client.HTablePool(((org.apache.hadoop.hbase.HBaseConfiguration) (null)), java.lang.Integer.MAX_VALUE);\n    java.lang.String tableName = \"testTable\";\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    junit.framework.Assert.assertNotNull(table);\n    pool.putTable(table);\n    org.apache.hadoop.hbase.client.HTableInterface sameTable = pool.getTable(tableName);\n    junit.framework.Assert.assertSame(table, sameTable);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    try {\n        return ((org.apache.hadoop.hbase.client.HTable) (table)).getRegionsInfo();\n    } finally {\n        pool.putTable(table);\n    }\n}",
            "ClassName": "RegionsResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableRegions",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = pool.getTable(tableName);\n    try {\n        return table.getTableDescriptor();\n    } finally {\n        pool.putTable(table);\n    }\n}",
            "ClassName": "SchemaResource",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTableSchema",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return pool;\n}",
            "ClassName": "RESTServlet",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTablePool",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    servlet.getMetrics().incrementRequests(1);\n    org.apache.hadoop.hbase.client.HTablePool pool = servlet.getTablePool();\n    org.apache.hadoop.hbase.client.HTableInterface table = null;\n    try {\n        java.util.List<org.apache.hadoop.hbase.rest.model.RowModel> rows = model.getRows();\n        table = pool.getTable(tableName);\n        ((org.apache.hadoop.hbase.client.HTable) (table)).setAutoFlush(false);\n        for (org.apache.hadoop.hbase.rest.model.RowModel row : rows) {\n            byte[] key = row.getKey();\n            org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(key);\n            for (org.apache.hadoop.hbase.rest.model.CellModel cell : row.getCells()) {\n                byte[][] parts = org.apache.hadoop.hbase.KeyValue.parseColumn(cell.getColumn());\n                if ((parts.length == 2) && (parts[1].length > 0)) {\n                    put.add(parts[0], parts[1], cell.getTimestamp(), cell.getValue());\n                } else {\n                    put.add(parts[0], null, cell.getTimestamp(), cell.getValue());\n                }\n            }\n            table.put(put);\n            if (org.apache.hadoop.hbase.rest.RowResource.LOG.isDebugEnabled()) {\n                org.apache.hadoop.hbase.rest.RowResource.LOG.debug(\"PUT \" + put.toString());\n            }\n        }\n        ((org.apache.hadoop.hbase.client.HTable) (table)).setAutoFlush(true);\n        table.flushCommits();\n        javax.ws.rs.core.Response.ResponseBuilder response = javax.ws.rs.core.Response.ok();\n        return response.build();\n    } catch (java.io.IOException e) {\n        throw new javax.ws.rs.WebApplicationException(e, Response.Status.SERVICE_UNAVAILABLE);\n    } finally {\n        if (table != null) {\n            pool.putTable(table);\n        }\n    }\n}",
            "ClassName": "RowResource",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "update",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 36,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(getConfiguration(), tableName);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    org.apache.hadoop.hbase.client.ResultScanner resScan = table.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result res : resScan) {\n        org.apache.hadoop.hbase.client.Delete del = new org.apache.hadoop.hbase.client.Delete(res.getRow());\n        table.delete(del);\n    }\n    return table;\n}",
            "ClassName": "HBaseTestingUtility",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "truncateTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 10,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHTablePool",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testTableWithStringName",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 9,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.client.HTable table = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testGet_NonExistentRow\"), org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, org.apache.hadoop.hbase.client.TestFromClientSide.VALUE);\n    table.put(put);\n    LOG.info(\"Row put\");\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    get.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.apache.hadoop.hbase.client.Result r = table.get(get);\n    org.junit.Assert.assertFalse(r.isEmpty());\n    java.lang.System.out.println(\"Row retrieved successfully\");\n    byte[] missingrow = org.apache.hadoop.hbase.util.Bytes.toBytes(\"missingrow\");\n    get = new org.apache.hadoop.hbase.client.Get(missingrow);\n    get.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    r = table.get(get);\n    org.junit.Assert.assertTrue(r.isEmpty());\n    LOG.info(\"Row missing as it should be\");\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(c, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    org.apache.hadoop.hbase.client.Get get = new org.apache.hadoop.hbase.client.Get(row);\n    get.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.Result res = t.get(get);\n    org.apache.hadoop.hbase.KeyValue[] kvs = res.raw();\n    if (kvs.length <= 0) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    byte[] value = kvs[0].getValue();\n    if (value == null) {\n        throw new java.io.IOException(\"no information for row \" + org.apache.hadoop.hbase.util.Bytes.toString(row));\n    }\n    org.apache.hadoop.hbase.HRegionInfo info = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(value);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    info.setOffline(onlineOffline);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER, org.apache.hadoop.hbase.util.Writables.getBytes(info));\n    t.put(put);\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(row);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.SERVER_QUALIFIER);\n    delete.deleteColumns(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.STARTCODE_QUALIFIER);\n    t.delete(delete);\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "changeOnlineStatus",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.TestZooKeeper.conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testGet_NonExistentRow",
        "NumberOfAsserts": 2,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 18,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    java.util.concurrent.atomic.AtomicBoolean stop = new java.util.concurrent.atomic.AtomicBoolean(false);\n    generateHLogs(-1);\n    fs.initialize(fs.getUri(), conf);\n    java.lang.Thread zombie = new org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.ZombieNewLogWriterRegionServer(stop);\n    try {\n        zombie.start();\n        try {\n            org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n        } catch (java.io.IOException ex) {\n        }\n        int logFilesNumber = fs.listStatus(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir).length;\n        org.junit.Assert.assertEquals(\"Log files should not be archived if there's an extra file after split\", org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.NUM_WRITERS + 1, logFilesNumber);\n    } finally {\n        stop.set(true);\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.removeAll(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions);\n    for (int i = 0; i < 100; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.add(\"region__\" + i);\n    }\n    generateHLogs(1, 100, -1);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    fs.rename(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir);\n    org.apache.hadoop.fs.Path firstSplitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME) + \".first\");\n    org.apache.hadoop.fs.Path splitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME));\n    fs.rename(splitPath, firstSplitPath);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    org.junit.Assert.assertEquals(0, compareHLogSplitDirs(firstSplitPath, splitPath));\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSplittingLargeNumberOfRegionsConsistency",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    for (int i = 0; i < writers; i++) {\n        writer[i] = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + i), conf);\n        for (int j = 0; j < entries; j++) {\n            int prefix = 0;\n            for (java.lang.String region : org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions) {\n                java.lang.String row_key = ((region + (prefix++)) + i) + j;\n                appendEntry(writer[i], org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, region.getBytes(), row_key.getBytes(), org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.FAMILY, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.QUALIFIER, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.VALUE, seq);\n            }\n        }\n        if (i != leaveOpen) {\n            writer[i].close();\n            flushToConsole(\"Closing writer \" + i);\n        }\n    }\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "generateHLogs",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.flushlogentries\", 1);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setStrings(\"hbase.rootdir\", org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir.toString());\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setClass(\"hbase.regionserver.hlog.writer.impl\", org.apache.hadoop.hbase.regionserver.wal.InstrumentedSequenceFileLogWriter.class, org.apache.hadoop.hbase.regionserver.wal.HLog.Writer.class);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.startMiniDFSCluster(2);\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Writer writer = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + suffix), conf);\n    if (closeFile)\n        writer.close();\n\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "injectEmptyFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.shutdownMiniDFSCluster();\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHLogSplit",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testSplitWillNotTouchLogsIfNewHLogGetsCreatedAfterSplitStarted",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 17,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 10,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testCreateTableWithRegions\");\n    byte[][] splitKeys = new byte[][]{ new byte[]{ 1, 1, 1 }, new byte[]{ 2, 2, 2 }, new byte[]{ 3, 3, 3 }, new byte[]{ 4, 4, 4 }, new byte[]{ 5, 5, 5 }, new byte[]{ 6, 6, 6 }, new byte[]{ 7, 7, 7 }, new byte[]{ 8, 8, 8 }, new byte[]{ 9, 9, 9 } };\n    int expectedRegions = splitKeys.length + 1;\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    admin.createTable(desc, splitKeys);\n    org.apache.hadoop.hbase.client.HTable ht = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration(), tableName);\n    java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> regions = ht.getRegionsInfo();\n    org.junit.Assert.assertEquals((((\"Tried to create \" + expectedRegions) + \" regions \") + \"but only found \") + regions.size(), expectedRegions, regions.size());\n    java.lang.System.err.println((\"Found \" + regions.size()) + \" regions\");\n    java.util.Iterator<org.apache.hadoop.hbase.HRegionInfo> hris = regions.keySet().iterator();\n    org.apache.hadoop.hbase.HRegionInfo hri = hris.next();\n    org.junit.Assert.assertTrue((hri.getStartKey() == null) || (hri.getStartKey().length == 0));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), splitKeys[0]));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), splitKeys[0]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), splitKeys[1]));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), splitKeys[1]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), splitKeys[2]));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), splitKeys[2]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), splitKeys[3]));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), splitKeys[3]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), splitKeys[4]));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), splitKeys[4]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), splitKeys[5]));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), splitKeys[5]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), splitKeys[6]));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), splitKeys[6]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), splitKeys[7]));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), splitKeys[7]));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), splitKeys[8]));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), splitKeys[8]));\n    org.junit.Assert.assertTrue((hri.getEndKey() == null) || (hri.getEndKey().length == 0));\n    byte[] startKey = new byte[]{ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 };\n    byte[] endKey = new byte[]{ 9, 9, 9, 9, 9, 9, 9, 9, 9, 9 };\n    expectedRegions = 10;\n    byte[] TABLE_2 = org.apache.hadoop.hbase.util.Bytes.add(tableName, org.apache.hadoop.hbase.util.Bytes.toBytes(\"_2\"));\n    desc = new org.apache.hadoop.hbase.HTableDescriptor(TABLE_2);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration());\n    admin.createTable(desc, startKey, endKey, expectedRegions);\n    ht = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration(), TABLE_2);\n    regions = ht.getRegionsInfo();\n    org.junit.Assert.assertEquals((((\"Tried to create \" + expectedRegions) + \" regions \") + \"but only found \") + regions.size(), expectedRegions, regions.size());\n    java.lang.System.err.println((\"Found \" + regions.size()) + \" regions\");\n    hris = regions.keySet().iterator();\n    hri = hris.next();\n    org.junit.Assert.assertTrue((hri.getStartKey() == null) || (hri.getStartKey().length == 0));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), new byte[]{ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 }));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), new byte[]{ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 }));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), new byte[]{ 2, 2, 2, 2, 2, 2, 2, 2, 2, 2 }));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), new byte[]{ 2, 2, 2, 2, 2, 2, 2, 2, 2, 2 }));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), new byte[]{ 3, 3, 3, 3, 3, 3, 3, 3, 3, 3 }));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), new byte[]{ 3, 3, 3, 3, 3, 3, 3, 3, 3, 3 }));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), new byte[]{ 4, 4, 4, 4, 4, 4, 4, 4, 4, 4 }));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), new byte[]{ 4, 4, 4, 4, 4, 4, 4, 4, 4, 4 }));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), new byte[]{ 5, 5, 5, 5, 5, 5, 5, 5, 5, 5 }));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), new byte[]{ 5, 5, 5, 5, 5, 5, 5, 5, 5, 5 }));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), new byte[]{ 6, 6, 6, 6, 6, 6, 6, 6, 6, 6 }));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), new byte[]{ 6, 6, 6, 6, 6, 6, 6, 6, 6, 6 }));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), new byte[]{ 7, 7, 7, 7, 7, 7, 7, 7, 7, 7 }));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), new byte[]{ 7, 7, 7, 7, 7, 7, 7, 7, 7, 7 }));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), new byte[]{ 8, 8, 8, 8, 8, 8, 8, 8, 8, 8 }));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), new byte[]{ 8, 8, 8, 8, 8, 8, 8, 8, 8, 8 }));\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getEndKey(), new byte[]{ 9, 9, 9, 9, 9, 9, 9, 9, 9, 9 }));\n    hri = hris.next();\n    org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getStartKey(), new byte[]{ 9, 9, 9, 9, 9, 9, 9, 9, 9, 9 }));\n    org.junit.Assert.assertTrue((hri.getEndKey() == null) || (hri.getEndKey().length == 0));\n    startKey = new byte[]{ 0, 0, 0, 0, 0, 0 };\n    endKey = new byte[]{ 1, 0, 0, 0, 0, 0 };\n    expectedRegions = 5;\n    byte[] TABLE_3 = org.apache.hadoop.hbase.util.Bytes.add(tableName, org.apache.hadoop.hbase.util.Bytes.toBytes(\"_3\"));\n    desc = new org.apache.hadoop.hbase.HTableDescriptor(TABLE_3);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration());\n    admin.createTable(desc, startKey, endKey, expectedRegions);\n    ht = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration(), TABLE_3);\n    regions = ht.getRegionsInfo();\n    org.junit.Assert.assertEquals((((\"Tried to create \" + expectedRegions) + \" regions \") + \"but only found \") + regions.size(), expectedRegions, regions.size());\n    java.lang.System.err.println((\"Found \" + regions.size()) + \" regions\");\n    splitKeys = new byte[][]{ new byte[]{ 1, 1, 1 }, new byte[]{ 2, 2, 2 }, new byte[]{ 3, 3, 3 }, new byte[]{ 2, 2, 2 } };\n    byte[] TABLE_4 = org.apache.hadoop.hbase.util.Bytes.add(tableName, org.apache.hadoop.hbase.util.Bytes.toBytes(\"_4\"));\n    desc = new org.apache.hadoop.hbase.HTableDescriptor(TABLE_4);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestAdmin.TEST_UTIL.getConfiguration());\n    try {\n        admin.createTable(desc, splitKeys);\n        org.junit.Assert.assertTrue(\"Should not be able to create this table because of \" + \"duplicate split keys\", false);\n    } catch (java.lang.IllegalArgumentException iae) {\n    }\n}",
        "CUT_1": {
            "Body": "{\n    return org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, hri.getStartKey()) ? org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaa\") : hri.getStartKey();\n}",
            "ClassName": "TestZKBasedCloseRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getStartKey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, hri.getStartKey()) ? org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaa\") : hri.getStartKey();\n}",
            "ClassName": "TestZKBasedReopenRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getStartKey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, hri.getStartKey()) ? org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaa\") : hri.getStartKey();\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getStartKey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    junit.framework.Assert.assertEquals(model.getName(), org.apache.hadoop.hbase.rest.TestTableResource.TABLE);\n    java.util.Iterator<org.apache.hadoop.hbase.rest.model.TableRegionModel> regions = model.getRegions().iterator();\n    junit.framework.Assert.assertTrue(regions.hasNext());\n    while (regions.hasNext()) {\n        org.apache.hadoop.hbase.rest.model.TableRegionModel region = regions.next();\n        boolean found = false;\n        for (java.util.Map.Entry<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> e : org.apache.hadoop.hbase.rest.TestTableResource.regionMap.entrySet()) {\n            org.apache.hadoop.hbase.HRegionInfo hri = e.getKey();\n            java.lang.String hriRegionName = hri.getRegionNameAsString();\n            java.lang.String regionName = region.getName();\n            if (hriRegionName.startsWith(regionName)) {\n                found = true;\n                byte[] startKey = hri.getStartKey();\n                byte[] endKey = hri.getEndKey();\n                java.net.InetSocketAddress sa = e.getValue().getInetSocketAddress();\n                java.lang.String location = (sa.getHostName() + \":\") + java.lang.Integer.valueOf(sa.getPort());\n                junit.framework.Assert.assertEquals(hri.getRegionId(), region.getId());\n                junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(startKey, region.getStartKey()));\n                junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(endKey, region.getEndKey()));\n                junit.framework.Assert.assertEquals(location, region.getLocation());\n                break;\n            }\n        }\n        junit.framework.Assert.assertTrue(found);\n    } \n}",
            "ClassName": "TestTableResource",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "checkTableInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 27,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    final int COL_COUNT = 10;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"tablename\");\n    final byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\");\n    this.conf.setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = null;\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(this.fs, dir, this.oldLogDir, this.conf, null);\n    try {\n        long timestamp = java.lang.System.currentTimeMillis();\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit cols = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        for (int i = 0; i < COL_COUNT; i++) {\n            cols.add(new org.apache.hadoop.hbase.KeyValue(row, org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\"), org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(i)), timestamp, new byte[]{ ((byte) (i + '0')) }));\n        }\n        org.apache.hadoop.hbase.HRegionInfo hri = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(tableName), org.apache.hadoop.hbase.HConstants.EMPTY_START_ROW, org.apache.hadoop.hbase.HConstants.EMPTY_END_ROW);\n        log.append(hri, tableName, cols, java.lang.System.currentTimeMillis());\n        long logSeqId = log.startCacheFlush();\n        log.completeCacheFlush(hri.getRegionName(), tableName, logSeqId, false);\n        log.close();\n        org.apache.hadoop.fs.Path filename = log.computeFilename();\n        log = null;\n        reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, filename, conf);\n        org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = reader.next();\n        junit.framework.Assert.assertEquals(COL_COUNT, entry.getEdit().size());\n        int idx = 0;\n        for (org.apache.hadoop.hbase.KeyValue val : entry.getEdit().getKeyValues()) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getRegionName(), entry.getKey().getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, entry.getKey().getTablename()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(row, val.getRow()));\n            junit.framework.Assert.assertEquals(((byte) (idx + '0')), val.getValue()[0]);\n            java.lang.System.out.println((entry.getKey() + \" \") + val);\n            idx++;\n        }\n        entry = reader.next();\n        junit.framework.Assert.assertEquals(1, entry.getEdit().size());\n        for (org.apache.hadoop.hbase.KeyValue val : entry.getEdit().getKeyValues()) {\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(hri.getRegionName(), entry.getKey().getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, entry.getKey().getTablename()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, val.getRow()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, val.getFamily()));\n            junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH, val.getValue()));\n            java.lang.System.out.println((entry.getKey() + \" \") + val);\n        }\n    } finally {\n        if (log != null) {\n            log.closeAndDelete();\n        }\n        if (reader != null) {\n            reader.close();\n        }\n    }\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testAppend",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 51,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testCreateTableWithRegions",
        "NumberOfAsserts": 44,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 108,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.MiniHBaseCluster.MiniHBaseClusterRegionServer firstServer = ((org.apache.hadoop.hbase.MiniHBaseCluster.MiniHBaseClusterRegionServer) (org.apache.hadoop.hbase.master.TestKillingServersFromMaster.cluster.getRegionServer(0)));\n    firstServer.getHServerInfo().setServerAddress(new org.apache.hadoop.hbase.HServerAddress(\"0.0.0.0\", 60010));\n    org.apache.hadoop.hbase.master.TestKillingServersFromMaster.cluster.waitOnRegionServer(0);\n    org.junit.Assert.assertEquals(1, org.apache.hadoop.hbase.master.TestKillingServersFromMaster.cluster.getLiveRegionServerThreads().size());\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestKillingServersFromMaster.TEST_UTIL.startMiniCluster(2);\n    org.apache.hadoop.hbase.master.TestKillingServersFromMaster.cluster = org.apache.hadoop.hbase.master.TestKillingServersFromMaster.TEST_UTIL.getHBaseCluster();\n}",
            "ClassName": "TestKillingServersFromMaster",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "beforeAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestKillingServersFromMaster.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestKillingServersFromMaster",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "afterAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestKillingServersFromMaster.TEST_UTIL.ensureSomeRegionServersAvailable(2);\n}",
            "ClassName": "TestKillingServersFromMaster",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setup",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    try {\n        hbaseCluster = new org.apache.hadoop.hbase.LocalHBaseCluster(conf, nRegionNodes, org.apache.hadoop.hbase.MiniHBaseCluster.MiniHBaseClusterMaster.class, org.apache.hadoop.hbase.MiniHBaseCluster.MiniHBaseClusterRegionServer.class);\n        hbaseCluster.startup();\n    } catch (java.io.IOException e) {\n        shutdown();\n        throw e;\n    }\n}",
            "ClassName": "MiniHBaseCluster",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "init",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.HBaseConfiguration.create();\n    org.apache.hadoop.hbase.LocalHBaseCluster cluster = new org.apache.hadoop.hbase.LocalHBaseCluster(conf);\n    cluster.startup();\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    org.apache.hadoop.hbase.HTableDescriptor htd = new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.util.Bytes.toBytes(cluster.getClass().getName()));\n    admin.createTable(htd);\n    cluster.shutdown();\n}",
            "ClassName": "LocalHBaseCluster",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "main",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestKillingServersFromMaster",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 1,
        "Label": 0,
        "MethodName": "testRsReportsWrongAddress",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 6,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.master.TestMasterTransitions.LOG.info(\"Running testAddingServerBeforeOldIsDead2413\");\n    org.apache.hadoop.hbase.MiniHBaseCluster cluster = org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getHBaseCluster();\n    int count = org.apache.hadoop.hbase.master.TestMasterTransitions.count();\n    int metaIndex = cluster.getServerWithMeta();\n    org.apache.hadoop.hbase.MiniHBaseCluster.MiniHBaseClusterRegionServer metaHRS = ((org.apache.hadoop.hbase.MiniHBaseCluster.MiniHBaseClusterRegionServer) (cluster.getRegionServer(metaIndex)));\n    int port = metaHRS.getServerInfo().getServerAddress().getPort();\n    org.apache.hadoop.conf.Configuration c = org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration();\n    java.lang.String oldPort = c.get(org.apache.hadoop.hbase.HConstants.REGIONSERVER_PORT, \"0\");\n    try {\n        org.apache.hadoop.hbase.master.TestMasterTransitions.LOG.info(\"KILLED=\" + metaHRS);\n        metaHRS.kill();\n        c.set(org.apache.hadoop.hbase.HConstants.REGIONSERVER_PORT, java.lang.Integer.toString(port));\n        org.apache.hadoop.hbase.regionserver.HRegionServer hrs = null;\n        while (true) {\n            try {\n                hrs = cluster.startRegionServer().getRegionServer();\n                break;\n            } catch (java.io.IOException e) {\n                if ((e.getCause() != null) && (e.getCause() instanceof java.lang.reflect.InvocationTargetException)) {\n                    java.lang.reflect.InvocationTargetException ee = ((java.lang.reflect.InvocationTargetException) (e.getCause()));\n                    if ((ee.getCause() != null) && (ee.getCause() instanceof java.net.BindException)) {\n                        org.apache.hadoop.hbase.master.TestMasterTransitions.LOG.info(\"BindException; retrying: \" + e.toString());\n                    }\n                }\n            }\n        } \n        org.apache.hadoop.hbase.master.TestMasterTransitions.LOG.info(\"STARTED=\" + hrs);\n        while (hrs.getOnlineRegions().size() < 3)\n            org.apache.hadoop.hbase.util.Threads.sleep(100);\n\n        org.apache.hadoop.hbase.master.TestMasterTransitions.LOG.info(((hrs.toString() + \" has \") + hrs.getOnlineRegions().size()) + \" regions\");\n        org.junit.Assert.assertEquals(count, org.apache.hadoop.hbase.master.TestMasterTransitions.count());\n    } finally {\n        c.set(org.apache.hadoop.hbase.HConstants.REGIONSERVER_PORT, oldPort);\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration().setInt(\"hbase.regions.percheckin\", 2);\n    org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.startMiniCluster(2);\n    org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.createTable(org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME), org.apache.hadoop.hbase.master.TestMasterTransitions.FAMILIES);\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    int countOfRegions = org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.createMultiRegions(t, org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily());\n    org.apache.hadoop.hbase.master.TestMasterTransitions.waitUntilAllRegionsAssigned(countOfRegions);\n    org.apache.hadoop.hbase.master.TestMasterTransitions.addToEachStartKey(countOfRegions);\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "beforeAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 10,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.Get g = new org.apache.hadoop.hbase.client.Get(row);\n    org.junit.Assert.assertTrue(t.get(g).size() > 0);\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "assertRegionIsBackOnline",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return org.apache.hadoop.hbase.master.TestMasterTransitions.FAMILIES[0];\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getTestFamily",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HTable t = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.master.TestMasterTransitions.TABLENAME);\n    org.apache.hadoop.hbase.client.HTable meta = new org.apache.hadoop.hbase.client.HTable(org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.getConfiguration(), org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    int rows = 0;\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n    org.apache.hadoop.hbase.client.ResultScanner s = meta.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r = null; (r = s.next()) != null;) {\n        byte[] b = r.getValue(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, org.apache.hadoop.hbase.HConstants.REGIONINFO_QUALIFIER);\n        if ((b == null) || (b.length <= 0))\n            break;\n\n        org.apache.hadoop.hbase.HRegionInfo hri = org.apache.hadoop.hbase.util.Writables.getHRegionInfo(b);\n        byte[] row = org.apache.hadoop.hbase.master.TestMasterTransitions.getStartKey(hri);\n        org.apache.hadoop.hbase.client.Put p = new org.apache.hadoop.hbase.client.Put(row);\n        p.add(org.apache.hadoop.hbase.master.TestMasterTransitions.getTestFamily(), org.apache.hadoop.hbase.master.TestMasterTransitions.getTestQualifier(), row);\n        t.put(p);\n        rows++;\n    }\n    s.close();\n    org.junit.Assert.assertEquals(expected, rows);\n    return rows;\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "addToEachStartKey",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.master.TestMasterTransitions.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestMasterTransitions",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "afterAllTests",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestMasterTransitions",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 4,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 1,
        "Label": 0,
        "MethodName": "testAddingServerBeforeOldIsDead2413",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 37,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[] TABLE = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testJiraTest52\");\n    byte[][] VALUES = makeNAscii(org.apache.hadoop.hbase.client.TestFromClientSide.VALUE, 7);\n    long[] STAMPS = makeStamps(7);\n    org.apache.hadoop.hbase.client.HTable ht = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(TABLE, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, 10);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[0], VALUES[0]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[1], VALUES[1]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[2], VALUES[2]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[3], VALUES[3]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[4], VALUES[4]);\n    put.add(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS[5], VALUES[5]);\n    ht.put(put);\n    getAllVersionsAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 5);\n    scanAllVersionsAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 5);\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.flush();\n    getAllVersionsAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 5);\n    scanAllVersionsAndVerify(ht, org.apache.hadoop.hbase.client.TestFromClientSide.ROW, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.QUALIFIER, STAMPS, VALUES, 0, 5);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    scan.addColumn(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY, org.apache.hadoop.hbase.client.TestFromClientSide.ROW);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 1);\n    scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    org.junit.Assert.assertTrue(scan.getFamilyMap().get(org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY).size() == 0);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanVariableReuse",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.startMiniCluster(3);\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.shutdownMiniCluster();\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.getConfiguration());\n    admin.split(t.getTableName());\n    java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> regions = waitOnSplit(t);\n    org.junit.Assert.assertTrue(regions.size() > 1);\n    return regions;\n}",
            "ClassName": "TestFromClientSide",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "splitTable",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.TestZooKeeper.conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testJiraTest52",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 19,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.conf.Configuration conf = new org.apache.hadoop.conf.Configuration(this.util.getConfiguration());\n    org.apache.hadoop.mapreduce.RecordWriter<org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.KeyValue> writer = null;\n    org.apache.hadoop.mapreduce.TaskAttemptContext context = null;\n    org.apache.hadoop.fs.Path dir = org.apache.hadoop.hbase.HBaseTestingUtility.getTestDir(\"test_LATEST_TIMESTAMP_isReplaced\");\n    try {\n        org.apache.hadoop.mapreduce.Job job = new org.apache.hadoop.mapreduce.Job(conf);\n        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, dir);\n        context = new org.apache.hadoop.mapreduce.TaskAttemptContext(job.getConfiguration(), new org.apache.hadoop.mapreduce.TaskAttemptID());\n        org.apache.hadoop.hbase.mapreduce.HFileOutputFormat hof = new org.apache.hadoop.hbase.mapreduce.HFileOutputFormat();\n        writer = hof.getRecordWriter(context);\n        final byte[] b = org.apache.hadoop.hbase.util.Bytes.toBytes(\"b\");\n        org.apache.hadoop.hbase.KeyValue kv = new org.apache.hadoop.hbase.KeyValue(b, b, b);\n        org.apache.hadoop.hbase.KeyValue original = kv.clone();\n        writer.write(new org.apache.hadoop.hbase.io.ImmutableBytesWritable(), kv);\n        org.junit.Assert.assertFalse(original.equals(kv));\n        org.junit.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(original.getRow(), kv.getRow()));\n        org.junit.Assert.assertTrue(original.matchingColumn(kv.getFamily(), kv.getQualifier()));\n        org.junit.Assert.assertNotSame(original.getTimestamp(), kv.getTimestamp());\n        org.junit.Assert.assertNotSame(org.apache.hadoop.hbase.HConstants.LATEST_TIMESTAMP, kv.getTimestamp());\n        kv = new org.apache.hadoop.hbase.KeyValue(b, b, b, kv.getTimestamp() - 1, b);\n        original = kv.clone();\n        writer.write(new org.apache.hadoop.hbase.io.ImmutableBytesWritable(), kv);\n        org.junit.Assert.assertTrue(original.equals(kv));\n    } finally {\n        if ((writer != null) && (context != null))\n            writer.close(context);\n\n        dir.getFileSystem(conf).delete(dir, true);\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addColumn(org.apache.hadoop.hbase.mapreduce.TestTimeRangeMapRed.FAMILY_NAME, org.apache.hadoop.hbase.mapreduce.TestTimeRangeMapRed.COLUMN_NAME);\n    scan.setMaxVersions(1);\n    org.apache.hadoop.hbase.client.ResultScanner scanner = table.getScanner(scan);\n    for (org.apache.hadoop.hbase.client.Result r : scanner) {\n        for (org.apache.hadoop.hbase.KeyValue kv : r.sorted()) {\n            org.apache.hadoop.hbase.mapreduce.TestTimeRangeMapRed.log.debug((((((((org.apache.hadoop.hbase.util.Bytes.toString(r.getRow()) + \"\\t\") + org.apache.hadoop.hbase.util.Bytes.toString(kv.getFamily())) + \"\\t\") + org.apache.hadoop.hbase.util.Bytes.toString(kv.getQualifier())) + \"\\t\") + kv.getTimestamp()) + \"\\t\") + org.apache.hadoop.hbase.util.Bytes.toBoolean(kv.getValue()));\n            junit.framework.Assert.assertEquals(org.apache.hadoop.hbase.mapreduce.TestTimeRangeMapRed.TIMESTAMP.get(kv.getTimestamp()), ((java.lang.Boolean) (org.apache.hadoop.hbase.util.Bytes.toBoolean(kv.getValue()))));\n        }\n    }\n    scanner.close();\n}",
            "ClassName": "TestTimeRangeMapRed",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "verify",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 13,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(getName());\n    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(getName());\n    byte[] qualifier = org.apache.hadoop.hbase.util.Bytes.toBytes(getName());\n    org.apache.hadoop.hbase.KeyValue original = new org.apache.hadoop.hbase.KeyValue(row, family, qualifier);\n    byte[] bytes = org.apache.hadoop.hbase.util.Writables.getBytes(original);\n    org.apache.hadoop.hbase.KeyValue newone = ((org.apache.hadoop.hbase.KeyValue) (org.apache.hadoop.hbase.util.Writables.getWritable(bytes, new org.apache.hadoop.hbase.KeyValue())));\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compare(original, newone) == 0);\n}",
            "ClassName": "TestSerialization",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testKeyValue",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 9,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.KeyValue kv = new org.apache.hadoop.hbase.KeyValue(row, family, qualifier, timestamp, value);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.compareTo(kv.getRow(), row) == 0);\n    junit.framework.Assert.assertTrue(kv.matchingColumn(family, qualifier));\n    LOG.info(kv.toString());\n}",
            "ClassName": "TestKeyValue",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "check",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.io.hfile.HFile.Writer writer = new org.apache.hadoop.hbase.io.hfile.HFile.Writer(fs, path, org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles.BLOCKSIZE, org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles.COMPRESSION, org.apache.hadoop.hbase.KeyValue.KEY_COMPARATOR);\n    try {\n        for (byte[] key : org.apache.hadoop.hbase.util.Bytes.iterateOnSplits(startKey, endKey, numRows - 2)) {\n            org.apache.hadoop.hbase.KeyValue kv = new org.apache.hadoop.hbase.KeyValue(key, family, qualifier, key);\n            writer.append(kv);\n        }\n    } finally {\n        writer.close();\n    }\n}",
            "ClassName": "TestLoadIncrementalHFiles",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createHFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 11,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.mapred.MiniMRCluster mrCluster = new org.apache.hadoop.mapred.MiniMRCluster(2, fs.getUri().toString(), 1);\n    org.apache.hadoop.mapreduce.Job job = null;\n    try {\n        org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.LOG.info(\"Before map/reduce startup\");\n        job = new org.apache.hadoop.mapreduce.Job(conf, \"process column contents\");\n        job.setNumReduceTasks(1);\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.addFamily(org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.INPUT_FAMILY);\n        org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(org.apache.hadoop.hbase.util.Bytes.toString(table.getTableName()), scan, org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.ProcessContentsMapper.class, org.apache.hadoop.hbase.io.ImmutableBytesWritable.class, org.apache.hadoop.hbase.client.Put.class, job);\n        org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob(org.apache.hadoop.hbase.util.Bytes.toString(table.getTableName()), org.apache.hadoop.hbase.mapreduce.IdentityTableReducer.class, job);\n        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new org.apache.hadoop.fs.Path(\"test\"));\n        org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.LOG.info(\"Started \" + org.apache.hadoop.hbase.util.Bytes.toString(table.getTableName()));\n        job.waitForCompletion(true);\n        org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.LOG.info(\"After map/reduce completion\");\n        verify(org.apache.hadoop.hbase.util.Bytes.toString(table.getTableName()));\n    } finally {\n        mrCluster.shutdown();\n        if (job != null) {\n            org.apache.hadoop.fs.FileUtil.fullyDelete(new java.io.File(job.getConfiguration().get(\"hadoop.tmp.dir\")));\n        }\n    }\n}",
            "ClassName": "TestTableMapReduce",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "runTestOnTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 3,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHFileOutputFormat",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 1,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "test_LATEST_TIMESTAMP_isReplaced",
        "NumberOfAsserts": 6,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 31,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    generateHLogs(5);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    for (java.lang.String region : org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions) {\n        org.apache.hadoop.fs.Path logfile = getLogForRegion(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, region);\n        org.junit.Assert.assertEquals(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.NUM_WRITERS * org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.ENTRIES, countHLog(logfile, fs, conf));\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.removeAll(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions);\n    for (int i = 0; i < 100; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.add(\"region__\" + i);\n    }\n    generateHLogs(1, 100, -1);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    fs.rename(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir);\n    org.apache.hadoop.fs.Path firstSplitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME) + \".first\");\n    org.apache.hadoop.fs.Path splitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME));\n    fs.rename(splitPath, firstSplitPath);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    org.junit.Assert.assertEquals(0, compareHLogSplitDirs(firstSplitPath, splitPath));\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSplittingLargeNumberOfRegionsConsistency",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    for (int i = 0; i < writers; i++) {\n        writer[i] = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + i), conf);\n        for (int j = 0; j < entries; j++) {\n            int prefix = 0;\n            for (java.lang.String region : org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions) {\n                java.lang.String row_key = ((region + (prefix++)) + i) + j;\n                appendEntry(writer[i], org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, region.getBytes(), row_key.getBytes(), org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.FAMILY, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.QUALIFIER, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.VALUE, seq);\n            }\n        }\n        if (i != leaveOpen) {\n            writer[i].close();\n            flushToConsole(\"Closing writer \" + i);\n        }\n    }\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "generateHLogs",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.flushlogentries\", 1);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setStrings(\"hbase.rootdir\", org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir.toString());\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setClass(\"hbase.regionserver.hlog.writer.impl\", org.apache.hadoop.hbase.regionserver.wal.InstrumentedSequenceFileLogWriter.class, org.apache.hadoop.hbase.regionserver.wal.HLog.Writer.class);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.startMiniDFSCluster(2);\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Writer writer = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + suffix), conf);\n    if (closeFile)\n        writer.close();\n\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "injectEmptyFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.shutdownMiniDFSCluster();\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHLogSplit",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 1,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testOpenZeroLengthReportedFileButWithDataGetsSplit",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 9,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.DummyLogActionsListener list = new org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.DummyLogActionsListener();\n    org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.DummyLogActionsListener laterList = new org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.DummyLogActionsListener();\n    org.apache.hadoop.hbase.regionserver.wal.HLog hlog = new org.apache.hadoop.hbase.regionserver.wal.HLog(org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.fs, org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.logDir, org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.oldLogDir, org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.conf, null, list, null);\n    org.apache.hadoop.hbase.HRegionInfo hri = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.SOME_BYTES), org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.SOME_BYTES, org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.SOME_BYTES, false);\n    for (int i = 0; i < 20; i++) {\n        byte[] b = org.apache.hadoop.hbase.util.Bytes.toBytes(i + \"\");\n        org.apache.hadoop.hbase.KeyValue kv = new org.apache.hadoop.hbase.KeyValue(b, b, b);\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit edit = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        edit.add(kv);\n        org.apache.hadoop.hbase.regionserver.wal.HLogKey key = new org.apache.hadoop.hbase.regionserver.wal.HLogKey(b, b, 0, 0);\n        hlog.append(hri, key, edit);\n        if (i == 10) {\n            hlog.addLogActionsListerner(laterList);\n        }\n        if ((i % 2) == 0) {\n            hlog.rollWriter();\n        }\n    }\n    org.junit.Assert.assertEquals(11, list.logRollCounter);\n    org.junit.Assert.assertEquals(5, laterList.logRollCounter);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.conf = org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.TEST_UTIL.getConfiguration();\n    org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.conf.setInt(\"hbase.regionserver.maxlogs\", 5);\n    org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.fs = org.apache.hadoop.fs.FileSystem.get(org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.conf);\n    org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.oldLogDir = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.TEST_UTIL.getTestDir(), org.apache.hadoop.hbase.HConstants.HREGION_OLDLOGDIR_NAME);\n    org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.logDir = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.TEST_UTIL.getTestDir(), org.apache.hadoop.hbase.HConstants.HREGION_LOGDIR_NAME);\n}",
            "ClassName": "TestLogActionsListener",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.fs.delete(org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.logDir, true);\n    org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.fs.delete(org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.oldLogDir, true);\n}",
            "ClassName": "TestLogActionsListener",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUp",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.KeyValue kv = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, null, java.lang.System.currentTimeMillis(), org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH);\n    org.apache.hadoop.hbase.regionserver.wal.WALEdit e = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n    e.add(kv);\n    return e;\n}",
            "ClassName": "HLog",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "completeCacheFlushLogEdit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    final int COL_COUNT = 10;\n    final byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"tablename\");\n    final byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\");\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Reader reader = null;\n    org.apache.hadoop.hbase.regionserver.wal.HLog log = new org.apache.hadoop.hbase.regionserver.wal.HLog(fs, dir, this.oldLogDir, this.conf, null);\n    try {\n        long timestamp = java.lang.System.currentTimeMillis();\n        org.apache.hadoop.hbase.regionserver.wal.WALEdit cols = new org.apache.hadoop.hbase.regionserver.wal.WALEdit();\n        for (int i = 0; i < COL_COUNT; i++) {\n            cols.add(new org.apache.hadoop.hbase.KeyValue(row, org.apache.hadoop.hbase.util.Bytes.toBytes(\"column\"), org.apache.hadoop.hbase.util.Bytes.toBytes(java.lang.Integer.toString(i)), timestamp, new byte[]{ ((byte) (i + '0')) }));\n        }\n        org.apache.hadoop.hbase.HRegionInfo info = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(tableName), row, org.apache.hadoop.hbase.util.Bytes.toBytes(org.apache.hadoop.hbase.util.Bytes.toString(row) + \"1\"), false);\n        final byte[] regionName = info.getRegionName();\n        log.append(info, tableName, cols, java.lang.System.currentTimeMillis());\n        long logSeqId = log.startCacheFlush();\n        log.completeCacheFlush(regionName, tableName, logSeqId, info.isMetaRegion());\n        log.close();\n        org.apache.hadoop.fs.Path filename = log.computeFilename();\n        log = null;\n        reader = org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(fs, filename, conf);\n        for (int i = 0; i < 1; i++) {\n            org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = reader.next(null);\n            if (entry == null)\n                break;\n\n            org.apache.hadoop.hbase.regionserver.wal.HLogKey key = entry.getKey();\n            org.apache.hadoop.hbase.regionserver.wal.WALEdit val = entry.getEdit();\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(regionName, key.getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, key.getTablename()));\n            org.apache.hadoop.hbase.KeyValue kv = val.getKeyValues().get(0);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(row, kv.getRow()));\n            junit.framework.Assert.assertEquals(((byte) (i + '0')), kv.getValue()[0]);\n            java.lang.System.out.println((key + \" \") + val);\n        }\n        org.apache.hadoop.hbase.regionserver.wal.HLog.Entry entry = null;\n        while ((entry = reader.next(null)) != null) {\n            org.apache.hadoop.hbase.regionserver.wal.HLogKey key = entry.getKey();\n            org.apache.hadoop.hbase.regionserver.wal.WALEdit val = entry.getEdit();\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(regionName, key.getRegionName()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(tableName, key.getTablename()));\n            org.apache.hadoop.hbase.KeyValue kv = val.getKeyValues().get(0);\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAROW, kv.getRow()));\n            junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(org.apache.hadoop.hbase.regionserver.wal.HLog.METAFAMILY, kv.getFamily()));\n            junit.framework.Assert.assertEquals(0, org.apache.hadoop.hbase.util.Bytes.compareTo(org.apache.hadoop.hbase.regionserver.wal.HLog.COMPLETE_CACHE_FLUSH, val.getKeyValues().get(0).getValue()));\n            java.lang.System.out.println((key + \" \") + val);\n        } \n    } finally {\n        if (log != null) {\n            log.closeAndDelete();\n        }\n        if (reader != null) {\n            reader.close();\n        }\n    }\n}",
            "ClassName": "TestHLog",
            "CyclomaticComplexity": 6,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testEditAdd",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 56,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Writer writer = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + suffix), conf);\n    if (closeFile)\n        writer.close();\n\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "injectEmptyFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestLogActionsListener",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 3,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testActionListener",
        "NumberOfAsserts": 2,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 22,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    final byte[] CONTENTS_FAMILY = org.apache.hadoop.hbase.util.Bytes.toBytes(\"contents\");\n    final byte[] SMALL_FAMILY = org.apache.hadoop.hbase.util.Bytes.toBytes(\"smallfam\");\n    final byte[] value = org.apache.hadoop.hbase.util.Bytes.toBytes(\"abcd\");\n    final int NB_BATCH_ROWS = 10;\n    org.apache.hadoop.hbase.client.HTable table = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testRowsPutBufferedManyManyFlushes\"), new byte[][]{ CONTENTS_FAMILY, SMALL_FAMILY });\n    table.setAutoFlush(false);\n    table.setWriteBufferSize(10);\n    java.util.ArrayList<org.apache.hadoop.hbase.client.Put> rowsUpdate = new java.util.ArrayList<org.apache.hadoop.hbase.client.Put>();\n    for (int i = 0; i < (NB_BATCH_ROWS * 10); i++) {\n        byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + i);\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n        put.add(CONTENTS_FAMILY, null, value);\n        rowsUpdate.add(put);\n    }\n    table.put(rowsUpdate);\n    table.flushCommits();\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(CONTENTS_FAMILY);\n    org.apache.hadoop.hbase.client.ResultScanner scanner = table.getScanner(scan);\n    int nbRows = 0;\n    for (@java.lang.SuppressWarnings(\"unused\")\n    org.apache.hadoop.hbase.client.Result row : scanner)\n        nbRows++;\n\n    org.junit.Assert.assertEquals(NB_BATCH_ROWS * 10, nbRows);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    this.table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    byte[] row = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row1\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row);\n    put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, org.apache.hadoop.hbase.util.Bytes.toBytes(tableName));\n    table.put(put);\n    return row;\n}",
            "ClassName": "DisabledTestRegionServerExit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createTableAndAddRow",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"test_table\");\n    byte[] family = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testFamily\");\n    initHRegion(tableName, getName(), family);\n    byte[] row1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row111\");\n    byte[] row2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row222\");\n    byte[] row3 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row333\");\n    byte[] row4 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row444\");\n    byte[] row5 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"row555\");\n    byte[] col1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub111\");\n    byte[] col2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"Pub222\");\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(row1);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(10L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row2);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(15L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row3);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(20L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row4);\n    put.add(family, col2, org.apache.hadoop.hbase.util.Bytes.toBytes(30L));\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(row5);\n    put.add(family, col1, org.apache.hadoop.hbase.util.Bytes.toBytes(40L));\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan(row3, row4);\n    scan.setMaxVersions();\n    scan.addColumn(family, col1);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    junit.framework.Assert.assertEquals(false, s.next(results));\n    junit.framework.Assert.assertEquals(0, results.size());\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_StopRow1542",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 34,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.TestZooKeeper.conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    new org.apache.hadoop.hbase.client.HTable(conf, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n    this.server = cluster.getRegionServerThreads().get(0).getRegionServer();\n    this.log = server.getLog();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    desc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(conf);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n    for (int i = 1; i <= 256; i++) {\n        org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"row\" + java.lang.String.format(\"%1$04d\", i)));\n        put.add(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY, null, value);\n        table.put(put);\n        if ((i % 32) == 0) {\n            try {\n                java.lang.Thread.sleep(2000);\n            } catch (java.lang.InterruptedException e) {\n            }\n        }\n    }\n}",
            "ClassName": "TestLogRolling",
            "CyclomaticComplexity": 2,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "startAndWriteData",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 1,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_5": {
            "Body": "{\n    byte[] tableName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"test_table\");\n    byte[] fam1 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"columnA\");\n    byte[] fam2 = org.apache.hadoop.hbase.util.Bytes.toBytes(\"columnB\");\n    initHRegion(tableName, getName(), fam1, fam2);\n    byte[] rowA = org.apache.hadoop.hbase.util.Bytes.toBytes(\"rowA\");\n    byte[] rowB = org.apache.hadoop.hbase.util.Bytes.toBytes(\"rowB\");\n    byte[] value = org.apache.hadoop.hbase.util.Bytes.toBytes(\"value\");\n    org.apache.hadoop.hbase.client.Delete delete = new org.apache.hadoop.hbase.client.Delete(rowA);\n    delete.deleteFamily(fam1);\n    region.delete(delete, null, true);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(rowA);\n    put.add(fam2, null, value);\n    region.put(put);\n    put = new org.apache.hadoop.hbase.client.Put(rowB);\n    put.add(fam1, null, value);\n    put.add(fam2, null, value);\n    region.put(put);\n    org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n    scan.addFamily(fam1).addFamily(fam2);\n    org.apache.hadoop.hbase.regionserver.InternalScanner s = region.getScanner(scan);\n    java.util.List<org.apache.hadoop.hbase.KeyValue> results = new java.util.ArrayList<org.apache.hadoop.hbase.KeyValue>();\n    s.next(results);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(rowA, results.get(0).getRow()));\n    results.clear();\n    s.next(results);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.util.Bytes.equals(rowB, results.get(0).getRow()));\n}",
            "ClassName": "TestHRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testScanner_DeleteOneFamilyNotAnother",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 28,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 2,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testRowsPutBufferedManyManyFlushes",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 27,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    org.apache.hadoop.mapreduce.Job job = new org.apache.hadoop.mapreduce.Job();\n    org.apache.hadoop.hbase.client.HTable table = org.mockito.Mockito.mock(org.apache.hadoop.hbase.client.HTable.class);\n    byte[][] mockKeys = new byte[][]{ org.apache.hadoop.hbase.HConstants.EMPTY_BYTE_ARRAY, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"ggg\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"zzz\") };\n    org.mockito.Mockito.doReturn(mockKeys).when(table).getStartKeys();\n    org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.configureIncrementalLoad(job, table);\n    org.junit.Assert.assertEquals(job.getNumReduceTasks(), 4);\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.mapreduce.Job job = new org.apache.hadoop.mapreduce.Job(conf, \"testLocalMRIncrementalLoad\");\n    setupRandomGeneratorMapper(job);\n    org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.configureIncrementalLoad(job, table);\n    org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, outDir);\n    org.junit.Assert.assertEquals(table.getRegionsInfo().size(), job.getNumReduceTasks());\n    org.junit.Assert.assertTrue(job.waitForCompletion(true));\n}",
            "ClassName": "TestHFileOutputFormat",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "runIncrementalPELoad",
            "NumberOfAsserts": 2,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 8,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    java.lang.String tableName = args[0];\n    org.apache.hadoop.fs.Path inputDir = new org.apache.hadoop.fs.Path(args[1]);\n    org.apache.hadoop.mapreduce.Job job = new org.apache.hadoop.mapreduce.Job(conf, (org.apache.hadoop.hbase.mapreduce.ImportTsv.NAME + \"_\") + tableName);\n    job.setJarByClass(org.apache.hadoop.hbase.mapreduce.ImportTsv.TsvImporter.class);\n    org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(job, inputDir);\n    job.setInputFormatClass(org.apache.hadoop.mapreduce.lib.input.TextInputFormat.class);\n    job.setMapperClass(org.apache.hadoop.hbase.mapreduce.ImportTsv.TsvImporter.class);\n    java.lang.String hfileOutPath = conf.get(org.apache.hadoop.hbase.mapreduce.ImportTsv.BULK_OUTPUT_CONF_KEY);\n    if (hfileOutPath != null) {\n        org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(conf, tableName);\n        job.setReducerClass(org.apache.hadoop.hbase.mapreduce.PutSortReducer.class);\n        org.apache.hadoop.fs.Path outputDir = new org.apache.hadoop.fs.Path(hfileOutPath);\n        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, outputDir);\n        job.setMapOutputKeyClass(org.apache.hadoop.hbase.io.ImmutableBytesWritable.class);\n        job.setMapOutputValueClass(org.apache.hadoop.hbase.client.Put.class);\n        org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.configureIncrementalLoad(job, table);\n    } else {\n        org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob(tableName, null, job);\n        job.setNumReduceTasks(0);\n    }\n    org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.addDependencyJars(job);\n    return job;\n}",
            "ClassName": "ImportTsv",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createSubmittableJob",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 24,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.mapred.MiniMRCluster mrCluster = new org.apache.hadoop.mapred.MiniMRCluster(2, fs.getUri().toString(), 1);\n    org.apache.hadoop.mapreduce.Job job = null;\n    try {\n        org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.LOG.info(\"Before map/reduce startup\");\n        job = new org.apache.hadoop.mapreduce.Job(conf, \"process column contents\");\n        job.setNumReduceTasks(1);\n        org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();\n        scan.addFamily(org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.INPUT_FAMILY);\n        org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(org.apache.hadoop.hbase.util.Bytes.toString(table.getTableName()), scan, org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.ProcessContentsMapper.class, org.apache.hadoop.hbase.io.ImmutableBytesWritable.class, org.apache.hadoop.hbase.client.Put.class, job);\n        org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob(org.apache.hadoop.hbase.util.Bytes.toString(table.getTableName()), org.apache.hadoop.hbase.mapreduce.IdentityTableReducer.class, job);\n        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new org.apache.hadoop.fs.Path(\"test\"));\n        org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.LOG.info(\"Started \" + org.apache.hadoop.hbase.util.Bytes.toString(table.getTableName()));\n        job.waitForCompletion(true);\n        org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.LOG.info(\"After map/reduce completion\");\n        verify(org.apache.hadoop.hbase.util.Bytes.toString(table.getTableName()));\n    } finally {\n        mrCluster.shutdown();\n        if (job != null) {\n            org.apache.hadoop.fs.FileUtil.fullyDelete(new java.io.File(job.getConfiguration().get(\"hadoop.tmp.dir\")));\n        }\n    }\n}",
            "ClassName": "TestTableMapReduce",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "runTestOnTable",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 3,
            "NumberOfLines": 23,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.StoreFile mock = org.mockito.Mockito.mock(org.apache.hadoop.hbase.regionserver.StoreFile.class);\n    org.mockito.Mockito.doReturn(bulkLoad).when(mock).isBulkLoadResult();\n    org.mockito.Mockito.doReturn(bulkTimestamp).when(mock).getBulkLoadTimestamp();\n    if (bulkLoad) {\n        org.mockito.Mockito.doThrow(new java.lang.IllegalAccessError(\"bulk load\")).when(mock).getMaxSequenceId();\n    } else {\n        org.mockito.Mockito.doReturn(seqId).when(mock).getMaxSequenceId();\n    }\n    org.mockito.Mockito.doReturn(new org.apache.hadoop.fs.Path(path)).when(mock).getPath();\n    java.lang.String name = ((((((\"mock storefile, bulkLoad=\" + bulkLoad) + \" bulkTimestamp=\") + bulkTimestamp) + \" seqId=\") + seqId) + \" path=\") + path;\n    org.mockito.Mockito.doReturn(name).when(mock).toString();\n    return mock;\n}",
            "ClassName": "TestStoreFile",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "mockStoreFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 14,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    java.lang.String tableName = args[0];\n    org.apache.hadoop.fs.Path inputDir = new org.apache.hadoop.fs.Path(args[1]);\n    org.apache.hadoop.mapreduce.Job job = new org.apache.hadoop.mapreduce.Job(conf, (org.apache.hadoop.hbase.mapreduce.Import.NAME + \"_\") + tableName);\n    job.setJarByClass(org.apache.hadoop.hbase.mapreduce.Import.Importer.class);\n    org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(job, inputDir);\n    job.setInputFormatClass(org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.class);\n    job.setMapperClass(org.apache.hadoop.hbase.mapreduce.Import.Importer.class);\n    org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableReducerJob(tableName, null, job);\n    job.setNumReduceTasks(0);\n    return job;\n}",
            "ClassName": "Import",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "createSubmittableJob",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHFileOutputFormat",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testJobConfiguration",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 8,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    generateHLogs(4);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.InstrumentedSequenceFileLogWriter.activateFailure = false;\n    appendEntry(writer[4], org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, org.apache.hadoop.hbase.util.Bytes.toBytes(\"break\"), (\"r\" + 999).getBytes(), org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.FAMILY, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.QUALIFIER, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.VALUE, 0);\n    writer[4].close();\n    try {\n        org.apache.hadoop.hbase.regionserver.wal.InstrumentedSequenceFileLogWriter.activateFailure = true;\n        org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    } catch (java.io.IOException e) {\n        org.junit.Assert.assertEquals(\"java.io.IOException: This exception is instrumented and should only be thrown for testing\", e.getMessage());\n        throw e;\n    } finally {\n        org.apache.hadoop.hbase.regionserver.wal.InstrumentedSequenceFileLogWriter.activateFailure = false;\n    }\n}",
        "CUT_1": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.removeAll(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions);\n    for (int i = 0; i < 100; i++) {\n        org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions.add(\"region__\" + i);\n    }\n    generateHLogs(1, 100, -1);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    fs.rename(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir);\n    org.apache.hadoop.fs.Path firstSplitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME) + \".first\");\n    org.apache.hadoop.fs.Path splitPath = new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.util.Bytes.toString(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME));\n    fs.rename(splitPath, firstSplitPath);\n    fs.initialize(fs.getUri(), conf);\n    org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.oldLogDir, fs, conf);\n    org.junit.Assert.assertEquals(0, compareHLogSplitDirs(firstSplitPath, splitPath));\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSplittingLargeNumberOfRegionsConsistency",
            "NumberOfAsserts": 1,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    for (int i = 0; i < writers; i++) {\n        writer[i] = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + i), conf);\n        for (int j = 0; j < entries; j++) {\n            int prefix = 0;\n            for (java.lang.String region : org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.regions) {\n                java.lang.String row_key = ((region + (prefix++)) + i) + j;\n                appendEntry(writer[i], org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TABLE_NAME, region.getBytes(), row_key.getBytes(), org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.FAMILY, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.QUALIFIER, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.VALUE, seq);\n            }\n        }\n        if (i != leaveOpen) {\n            writer[i].close();\n            flushToConsole(\"Closing writer \" + i);\n        }\n    }\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "generateHLogs",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 16,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setInt(\"hbase.regionserver.flushlogentries\", 1);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setBoolean(\"dfs.support.append\", true);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setStrings(\"hbase.rootdir\", org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hbaseDir.toString());\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.getConfiguration().setClass(\"hbase.regionserver.hlog.writer.impl\", org.apache.hadoop.hbase.regionserver.wal.InstrumentedSequenceFileLogWriter.class, org.apache.hadoop.hbase.regionserver.wal.HLog.Writer.class);\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.startMiniDFSCluster(2);\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setUpBeforeClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 7,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.HLog.Writer writer = org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(fs, new org.apache.hadoop.fs.Path(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.hlogDir, org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.HLOG_FILE_PREFIX + suffix), conf);\n    if (closeFile)\n        writer.close();\n\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 1,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "injectEmptyFile",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 6,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.TEST_UTIL.shutdownMiniDFSCluster();\n}",
            "ClassName": "TestHLogSplit",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "tearDownAfterClass",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestHLogSplit",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testSplitWillFailIfWritingToRegionFails",
        "NumberOfAsserts": 1,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 16,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    java.lang.String msg = null;\n    try {\n        this.admin.createTable(org.apache.hadoop.hbase.HTableDescriptor.ROOT_TABLEDESC);\n    } catch (java.lang.IllegalArgumentException e) {\n        msg = e.toString();\n    }\n    org.junit.Assert.assertTrue(\"Unexcepted exception message \" + msg, ((msg != null) && msg.startsWith(java.lang.IllegalArgumentException.class.getName())) && msg.contains(org.apache.hadoop.hbase.HTableDescriptor.ROOT_TABLEDESC.getNameAsString()));\n    msg = null;\n    try {\n        this.admin.createTable(org.apache.hadoop.hbase.HTableDescriptor.META_TABLEDESC);\n    } catch (java.lang.IllegalArgumentException e) {\n        msg = e.toString();\n    }\n    org.junit.Assert.assertTrue(\"Unexcepted exception message \" + msg, ((msg != null) && msg.startsWith(java.lang.IllegalArgumentException.class.getName())) && msg.contains(org.apache.hadoop.hbase.HTableDescriptor.META_TABLEDESC.getNameAsString()));\n    final org.apache.hadoop.hbase.HTableDescriptor threadDesc = new org.apache.hadoop.hbase.HTableDescriptor(\"threaded_testCreateBadTables\");\n    threadDesc.addFamily(new org.apache.hadoop.hbase.HColumnDescriptor(org.apache.hadoop.hbase.HConstants.CATALOG_FAMILY));\n    int count = 10;\n    java.lang.Thread[] threads = new java.lang.Thread[count];\n    final java.util.concurrent.atomic.AtomicInteger successes = new java.util.concurrent.atomic.AtomicInteger(0);\n    final java.util.concurrent.atomic.AtomicInteger failures = new java.util.concurrent.atomic.AtomicInteger(0);\n    final org.apache.hadoop.hbase.client.HBaseAdmin localAdmin = this.admin;\n    for (int i = 0; i < count; i++) {\n        threads[i] = new java.lang.Thread(java.lang.Integer.toString(i)) {\n            @java.lang.Override\n            public void run() {\n                try {\n                    localAdmin.createTable(threadDesc);\n                    successes.incrementAndGet();\n                } catch (org.apache.hadoop.hbase.TableExistsException e) {\n                    failures.incrementAndGet();\n                } catch (java.io.IOException e) {\n                    throw new java.lang.RuntimeException(\"Failed threaded create\" + getName(), e);\n                }\n            }\n        };\n    }\n    for (int i = 0; i < count; i++) {\n        threads[i].start();\n    }\n    for (int i = 0; i < count; i++) {\n        while (threads[i].isAlive()) {\n            try {\n                java.lang.Thread.sleep(1000);\n            } catch (java.lang.InterruptedException e) {\n            }\n        } \n    }\n    org.junit.Assert.assertEquals(1, successes.get());\n    org.junit.Assert.assertEquals(count - 1, failures.get());\n}",
        "CUT_1": {
            "Body": "{\n    return t instanceof java.io.IOException ? ((java.io.IOException) (t)) : (msg == null) || (msg.length() == 0) ? new java.io.IOException(t) : new java.io.IOException(msg, t);\n}",
            "ClassName": "HRegionServer",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "convertThrowableToIOE",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    final java.util.List<java.lang.Thread> threads = new java.util.ArrayList<java.lang.Thread>(N);\n    final int perClientRows = R / N;\n    for (int i = 0; i < N; i++) {\n        java.lang.Thread t = new java.lang.Thread(java.lang.Integer.toString(i)) {\n            @java.lang.Override\n            public void run() {\n                super.run();\n                org.apache.hadoop.hbase.rest.PerformanceEvaluation pe = new org.apache.hadoop.hbase.rest.PerformanceEvaluation(conf);\n                int index = java.lang.Integer.parseInt(getName());\n                try {\n                    long elapsedTime = pe.runOneClient(cmd, index * perClientRows, perClientRows, R, B, new org.apache.hadoop.hbase.rest.PerformanceEvaluation.Status() {\n                        public void setStatus(final java.lang.String msg) throws java.io.IOException {\n                            org.apache.hadoop.hbase.rest.PerformanceEvaluation.LOG.info(((\"client-\" + getName()) + \" \") + msg);\n                        }\n                    });\n                    org.apache.hadoop.hbase.rest.PerformanceEvaluation.LOG.info((((((\"Finished \" + getName()) + \" in \") + elapsedTime) + \"ms writing \") + perClientRows) + \" rows\");\n                } catch (java.io.IOException e) {\n                    throw new java.lang.RuntimeException(e);\n                }\n            }\n        };\n        threads.add(t);\n    }\n    for (java.lang.Thread t : threads) {\n        t.start();\n    }\n    for (java.lang.Thread t : threads) {\n        while (t.isAlive()) {\n            try {\n                t.join();\n            } catch (java.lang.InterruptedException e) {\n                org.apache.hadoop.hbase.rest.PerformanceEvaluation.LOG.debug(\"Interrupted, continuing\" + e.toString());\n            }\n        } \n    }\n}",
            "ClassName": "PerformanceEvaluation",
            "CyclomaticComplexity": 4,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "doMultipleClients",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 37,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 28
        },
        "CUT_3": {
            "Body": "{\n    final int count = 1;\n    long now = java.lang.System.currentTimeMillis();\n    java.util.List<java.lang.Thread> threads = new java.util.ArrayList<java.lang.Thread>(count);\n    for (int i = 0; i < count; i++) {\n        java.lang.Thread t = new java.lang.Thread(r);\n        t.setName(\"\" + i);\n        threads.add(t);\n    }\n    for (java.lang.Thread t : threads) {\n        t.start();\n    }\n    for (java.lang.Thread t : threads) {\n        try {\n            t.join();\n        } catch (java.lang.InterruptedException e) {\n            e.printStackTrace();\n        }\n    }\n    org.apache.hadoop.hbase.PerformanceEvaluationCommons.LOG.info(\"Test took \" + (java.lang.System.currentTimeMillis() - now));\n}",
            "ClassName": "PerformanceEvaluationCommons",
            "CyclomaticComplexity": 3,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "concurrentReads",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 21,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 22
        },
        "CUT_4": {
            "Body": "{\n    if (this.toDo.isEmpty()) {\n        return;\n    }\n    for (org.apache.hadoop.hbase.regionserver.HRegionServer.ToDoEntry e : this.toDo) {\n        if (e == null) {\n            org.apache.hadoop.hbase.regionserver.HRegionServer.LOG.warn(\"toDo gave a null entry during iteration\");\n            break;\n        }\n        org.apache.hadoop.hbase.HMsg msg = e.msg;\n        if (msg != null) {\n            if (msg.isType(org.apache.hadoop.hbase.HMsg.Type.MSG_REGION_OPEN)) {\n                addProcessingMessage(msg.getRegionInfo());\n            }\n        } else {\n            org.apache.hadoop.hbase.regionserver.HRegionServer.LOG.warn(\"Message is empty: \" + e);\n        }\n    }\n}",
            "ClassName": "HRegionServer",
            "CyclomaticComplexity": 5,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "housekeeping",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 19,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 2
        },
        "CUT_5": {
            "Body": "{\n    java.util.Properties properties = new java.util.Properties();\n    try {\n        properties.load(inputStream);\n    } catch (java.io.IOException e) {\n        final java.lang.String msg = \"fail to read properties from \" + org.apache.hadoop.hbase.HConstants.ZOOKEEPER_CONFIG_NAME;\n        org.apache.hadoop.hbase.zookeeper.HQuorumPeer.LOG.fatal(msg);\n        throw new java.io.IOException(msg, e);\n    }\n    for (java.util.Map.Entry<java.lang.Object, java.lang.Object> entry : properties.entrySet()) {\n        java.lang.String value = entry.getValue().toString().trim();\n        java.lang.String key = entry.getKey().toString().trim();\n        java.lang.StringBuilder newValue = new java.lang.StringBuilder();\n        int varStart = value.indexOf(org.apache.hadoop.hbase.zookeeper.HQuorumPeer.VARIABLE_START);\n        int varEnd = 0;\n        while (varStart != (-1)) {\n            varEnd = value.indexOf(org.apache.hadoop.hbase.zookeeper.HQuorumPeer.VARIABLE_END, varStart);\n            if (varEnd == (-1)) {\n                java.lang.String msg = (\"variable at \" + varStart) + \" has no end marker\";\n                org.apache.hadoop.hbase.zookeeper.HQuorumPeer.LOG.fatal(msg);\n                throw new java.io.IOException(msg);\n            }\n            java.lang.String variable = value.substring(varStart + org.apache.hadoop.hbase.zookeeper.HQuorumPeer.VARIABLE_START_LENGTH, varEnd);\n            java.lang.String substituteValue = java.lang.System.getProperty(variable);\n            if (substituteValue == null) {\n                substituteValue = conf.get(variable);\n            }\n            if (substituteValue == null) {\n                java.lang.String msg = ((\"variable \" + variable) + \" not set in system property \") + \"or hbase configs\";\n                org.apache.hadoop.hbase.zookeeper.HQuorumPeer.LOG.fatal(msg);\n                throw new java.io.IOException(msg);\n            }\n            newValue.append(substituteValue);\n            varEnd += org.apache.hadoop.hbase.zookeeper.HQuorumPeer.VARIABLE_END_LENGTH;\n            varStart = value.indexOf(org.apache.hadoop.hbase.zookeeper.HQuorumPeer.VARIABLE_START, varEnd);\n        } \n        if (key.startsWith(\"server.\")) {\n            if (conf.get(org.apache.hadoop.hbase.HConstants.CLUSTER_DISTRIBUTED).equals(org.apache.hadoop.hbase.HConstants.CLUSTER_IS_DISTRIBUTED) && value.startsWith(\"localhost\")) {\n                java.lang.String msg = \"The server in zoo.cfg cannot be set to localhost \" + (\"in a fully-distributed setup because it won't be reachable. \" + \"See \\\"Getting Started\\\" for more information.\");\n                org.apache.hadoop.hbase.zookeeper.HQuorumPeer.LOG.fatal(msg);\n                throw new java.io.IOException(msg);\n            }\n        }\n        newValue.append(value.substring(varEnd));\n        properties.setProperty(key, newValue.toString());\n    }\n    return properties;\n}",
            "ClassName": "HQuorumPeer",
            "CyclomaticComplexity": 7,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "parseZooCfg",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 48,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 4,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testCreateBadTables",
        "NumberOfAsserts": 4,
        "NumberOfAsynchronousWaits": 1,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 51,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 36,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[] name = org.apache.hadoop.hbase.util.Bytes.toBytes(\"testFilterAcrossMutlipleRegions\");\n    org.apache.hadoop.hbase.client.HTable t = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.createTable(name, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    int rowCount = org.apache.hadoop.hbase.client.TestFromClientSide.TEST_UTIL.loadTable(t, org.apache.hadoop.hbase.client.TestFromClientSide.FAMILY);\n    assertRowCount(t, rowCount);\n    java.util.Map<org.apache.hadoop.hbase.HRegionInfo, org.apache.hadoop.hbase.HServerAddress> regions = splitTable(t);\n    assertRowCount(t, rowCount);\n    byte[] endKey = regions.keySet().iterator().next().getEndKey();\n    int endKeyCount = countRows(t, createScanWithRowFilter(endKey));\n    org.junit.Assert.assertTrue(endKeyCount < rowCount);\n    byte[] key = new byte[]{ endKey[0], endKey[1], ((byte) (endKey[2] + 1)) };\n    int plusOneCount = countRows(t, createScanWithRowFilter(key));\n    org.junit.Assert.assertEquals(endKeyCount + 1, plusOneCount);\n    key = new byte[]{ endKey[0], endKey[1], ((byte) (endKey[2] + 2)) };\n    int plusTwoCount = countRows(t, createScanWithRowFilter(key));\n    org.junit.Assert.assertEquals(endKeyCount + 2, plusTwoCount);\n    key = new byte[]{ endKey[0], endKey[1], ((byte) (endKey[2] - 1)) };\n    int minusOneCount = countRows(t, createScanWithRowFilter(key));\n    org.junit.Assert.assertEquals(endKeyCount - 1, minusOneCount);\n    key = new byte[]{ 'a', 'a', 'a' };\n    int countBBB = countRows(t, createScanWithRowFilter(key, null, org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.EQUAL));\n    org.junit.Assert.assertEquals(1, countBBB);\n    int countGreater = countRows(t, createScanWithRowFilter(endKey, null, org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER_OR_EQUAL));\n    org.junit.Assert.assertEquals(0, countGreater);\n    countGreater = countRows(t, createScanWithRowFilter(endKey, endKey, org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.GREATER_OR_EQUAL));\n    org.junit.Assert.assertEquals(rowCount - endKeyCount, countGreater);\n}",
        "CUT_1": {
            "Body": "{\n    this.endKey = endKey;\n}",
            "ClassName": "TableRegionModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setEndKey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return endKey;\n}",
            "ClassName": "HRegionInfo",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getEndKey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return endKey;\n}",
            "ClassName": "TableRegionModel",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getEndKey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    this.endKey = endKey;\n    return this;\n}",
            "ClassName": "TRegionInfo",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "setEndKey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 4,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return this.endKey;\n}",
            "ClassName": "TRegionInfo",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getEndKey",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestFromClientSide",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testFilterAcrossMutlipleRegions",
        "NumberOfAsserts": 7,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 27,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    testScan(null, \"bbb\", \"bba\");\n}",
        "CUT_1": {
            "Body": "{\n    final byte[] a = org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaa\");\n    final byte[] b = org.apache.hadoop.hbase.util.Bytes.toBytes(\"bbb\");\n    final byte[] fam = org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\");\n    final byte[] qf = org.apache.hadoop.hbase.util.Bytes.toBytes(\"umn\");\n    org.apache.hadoop.hbase.KeyValue aaa = new org.apache.hadoop.hbase.KeyValue(a, fam, qf, a);\n    org.apache.hadoop.hbase.KeyValue bbb = new org.apache.hadoop.hbase.KeyValue(b, fam, qf, b);\n    byte[] keyabb = aaa.getKey();\n    byte[] keybbb = bbb.getKey();\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compare(aaa, bbb) < 0);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.KEY_COMPARATOR.compare(keyabb, 0, keyabb.length, keybbb, 0, keybbb.length) < 0);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compare(bbb, aaa) > 0);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.KEY_COMPARATOR.compare(keybbb, 0, keybbb.length, keyabb, 0, keyabb.length) > 0);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compare(bbb, bbb) == 0);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.KEY_COMPARATOR.compare(keybbb, 0, keybbb.length, keybbb, 0, keybbb.length) == 0);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compare(aaa, aaa) == 0);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.KEY_COMPARATOR.compare(keyabb, 0, keyabb.length, keyabb, 0, keyabb.length) == 0);\n    aaa = new org.apache.hadoop.hbase.KeyValue(a, fam, qf, 1, a);\n    bbb = new org.apache.hadoop.hbase.KeyValue(a, fam, qf, 2, a);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compare(aaa, bbb) > 0);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compare(bbb, aaa) < 0);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compare(aaa, aaa) == 0);\n    aaa = new org.apache.hadoop.hbase.KeyValue(a, fam, qf, 1, org.apache.hadoop.hbase.KeyValue.Type.Delete, a);\n    bbb = new org.apache.hadoop.hbase.KeyValue(a, fam, qf, 1, a);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compare(aaa, bbb) < 0);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compare(bbb, aaa) > 0);\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.COMPARATOR.compare(aaa, aaa) == 0);\n}",
            "ClassName": "TestKeyValue",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testPlainCompare",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 28,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return ((x == null) && (y == null)) || ((x != null) && x.equals(y));\n}",
            "ClassName": "Pair",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "equals",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    return null;\n}",
            "ClassName": "HRegion",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "internalPreFlushcacheCommit",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    return null;\n}",
            "ClassName": "WildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    return null;\n}",
            "ClassName": "ScanWildcardColumnTracker",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "getColumnHint",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestTableInputFormatScan",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 0,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testScanEmptyToBBB",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 3,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    },
    {
        "Body": "{\n    byte[][] illegalNames = new byte[][]{ org.apache.hadoop.hbase.util.Bytes.toBytes(\"-bad\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\".bad\"), org.apache.hadoop.hbase.HConstants.ROOT_TABLE_NAME, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME };\n    for (int i = 0; i < illegalNames.length; i++) {\n        try {\n            new org.apache.hadoop.hbase.HTableDescriptor(illegalNames[i]);\n            throw new java.io.IOException((\"Did not detect '\" + org.apache.hadoop.hbase.util.Bytes.toString(illegalNames[i])) + \"' as an illegal user table name\");\n        } catch (java.lang.IllegalArgumentException e) {\n        }\n    }\n    byte[] legalName = org.apache.hadoop.hbase.util.Bytes.toBytes(\"g-oo.d\");\n    try {\n        new org.apache.hadoop.hbase.HTableDescriptor(legalName);\n    } catch (java.lang.IllegalArgumentException e) {\n        throw new java.io.IOException(((\"Legal user table name: '\" + org.apache.hadoop.hbase.util.Bytes.toString(legalName)) + \"' caused IllegalArgumentException: \") + e.getMessage());\n    }\n}",
        "CUT_1": {
            "Body": "{\n    return new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(row), org.apache.hadoop.hbase.util.Bytes.toBytes(family), org.apache.hadoop.hbase.util.Bytes.toBytes(qualifier), timestamp, type, org.apache.hadoop.hbase.util.Bytes.toBytes(value));\n}",
            "ClassName": "KeyValueTestUtil",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "create",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_2": {
            "Body": "{\n    return org.apache.hadoop.hbase.util.Bytes.equals(n, org.apache.hadoop.hbase.HConstants.ROOT_TABLE_NAME) || org.apache.hadoop.hbase.util.Bytes.equals(n, org.apache.hadoop.hbase.HConstants.META_TABLE_NAME);\n}",
            "ClassName": "MetaUtils",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "isMetaTableName",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 3,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_3": {
            "Body": "{\n    org.apache.hadoop.hbase.HRegionInfo a = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(\"a\"), null, null);\n    org.apache.hadoop.hbase.HRegionInfo b = new org.apache.hadoop.hbase.HRegionInfo(new org.apache.hadoop.hbase.HTableDescriptor(\"b\"), null, null);\n    junit.framework.Assert.assertTrue(a.compareTo(b) != 0);\n    org.apache.hadoop.hbase.HTableDescriptor t = new org.apache.hadoop.hbase.HTableDescriptor(\"t\");\n    byte[] midway = org.apache.hadoop.hbase.util.Bytes.toBytes(\"midway\");\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, null, midway);\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, midway, null);\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n    junit.framework.Assert.assertTrue(b.compareTo(a) > 0);\n    junit.framework.Assert.assertEquals(a, a);\n    junit.framework.Assert.assertTrue(a.compareTo(a) == 0);\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"a\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"d\"));\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"e\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"g\"));\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"dddd\"));\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"e\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"g\"));\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n    a = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"dddd\"));\n    b = new org.apache.hadoop.hbase.HRegionInfo(t, org.apache.hadoop.hbase.util.Bytes.toBytes(\"aaaa\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"eeee\"));\n    junit.framework.Assert.assertTrue(a.compareTo(b) < 0);\n}",
            "ClassName": "TestCompare",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testHRegionInfo",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 22,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_4": {
            "Body": "{\n    org.apache.hadoop.hbase.client.HBaseAdmin admin = new org.apache.hadoop.hbase.client.HBaseAdmin(org.apache.hadoop.hbase.TestZooKeeper.conf);\n    java.lang.String tableName = \"test\" + java.lang.System.currentTimeMillis();\n    org.apache.hadoop.hbase.HTableDescriptor desc = new org.apache.hadoop.hbase.HTableDescriptor(tableName);\n    org.apache.hadoop.hbase.HColumnDescriptor family = new org.apache.hadoop.hbase.HColumnDescriptor(\"fam\");\n    desc.addFamily(family);\n    admin.createTable(desc);\n    org.apache.hadoop.hbase.client.HTable table = new org.apache.hadoop.hbase.client.HTable(tableName);\n    org.apache.hadoop.hbase.client.Put put = new org.apache.hadoop.hbase.client.Put(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testrow\"));\n    put.add(org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"col\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"testdata\"));\n    table.put(put);\n}",
            "ClassName": "TestZooKeeper",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testSanity",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 12,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "CUT_5": {
            "Body": "{\n    org.apache.hadoop.hbase.KeyValue rowA = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable,www.hbase.org/,1234\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), java.lang.Long.MAX_VALUE, ((byte[]) (null)));\n    org.apache.hadoop.hbase.KeyValue rowB = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable,www.hbase.org/%20,99999\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), java.lang.Long.MAX_VALUE, ((byte[]) (null)));\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.META_COMPARATOR.compare(rowA, rowB) < 0);\n    rowA = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable,,1234\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), java.lang.Long.MAX_VALUE, ((byte[]) (null)));\n    rowB = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\"testtable,$www.hbase.org/,99999\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), java.lang.Long.MAX_VALUE, ((byte[]) (null)));\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.META_COMPARATOR.compare(rowA, rowB) < 0);\n    rowA = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\".META.,testtable,www.hbase.org/,1234,4321\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), java.lang.Long.MAX_VALUE, ((byte[]) (null)));\n    rowB = new org.apache.hadoop.hbase.KeyValue(org.apache.hadoop.hbase.util.Bytes.toBytes(\".META.,testtable,www.hbase.org/%20,99999,99999\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"fam\"), org.apache.hadoop.hbase.util.Bytes.toBytes(\"\"), java.lang.Long.MAX_VALUE, ((byte[]) (null)));\n    junit.framework.Assert.assertTrue(org.apache.hadoop.hbase.KeyValue.ROOT_COMPARATOR.compare(rowA, rowB) < 0);\n}",
            "ClassName": "TestKeyValue",
            "CyclomaticComplexity": 0,
            "HasTimeoutInAnnotations": 0,
            "MethodName": "testKeyValueBorderCases",
            "NumberOfAsserts": 0,
            "NumberOfAsynchronousWaits": 0,
            "NumberOfDates": 0,
            "NumberOfFiles": 0,
            "NumberOfLines": 11,
            "NumberOfRandoms": 0,
            "NumberOfThreads": 0
        },
        "ClassName": "TestAdmin",
        "Commit": "d59d054fc9abeab776d90709f64bb5bb59d1b673",
        "CyclomaticComplexity": 1,
        "Date": "Mon, 21 Jun 2010 20:52:31 +0000",
        "HasTimeoutInAnnotations": 0,
        "Label": 0,
        "MethodName": "testTableNames",
        "NumberOfAsserts": 0,
        "NumberOfAsynchronousWaits": 0,
        "NumberOfDates": 0,
        "NumberOfFiles": 0,
        "NumberOfLines": 16,
        "NumberOfRandoms": 0,
        "NumberOfThreads": 0,
        "ProjectName": "hbase",
        "Timestamp": "1277153551"
    }
]
